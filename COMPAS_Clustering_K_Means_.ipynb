{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DM4tlj2rbuqA",
        "Pt9lViq7bQMy",
        "IXF8Nbx6Yyie"
      ],
      "authorship_tag": "ABX9TyNxu6yDIw8unE/NiJ46Yt6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering_K_Means_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-OEcsJQOo6Ai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "DM4tlj2rbuqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Shap_error_data.csv')"
      ],
      "metadata": {
        "id": "FHXmgXkkW0pn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = data_shaperr.drop(['Error_Type', 'errors', 'predicted_class', 'true_class'], axis =1)\n",
        "#features.info()"
      ],
      "metadata": {
        "id": "AJqXXZ25YrTg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into tp/fn and tn/fp"
      ],
      "metadata": {
        "id": "Pt9lViq7bQMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Drop rows where both TP and FN are '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP_data = drop_zero_TN_FP(data_shaperr)"
      ],
      "metadata": {
        "id": "SZkXQMwKbNYR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "IXF8Nbx6Yyie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Pca on scaled features'''\n",
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop certain columns\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type' 'clusters', 'new_clusters',\n",
        "                              'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'\n",
        "                              ], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                           'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'\n",
        "                           ]]\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "'''Initialization of dataset to scale the features and errors, which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True):\n",
        "    new_data = data.copy(deep=True)\n",
        "\n",
        "    if with_errors:\n",
        "        scaling_factor = 0.8\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP']\n",
        "        new_data[error_columns] = new_data[error_columns] * scaling_factor\n",
        "\n",
        "    if just_features:\n",
        "        new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type',\n",
        "                                  'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American',\n",
        "                                  'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other']\n",
        "                                  , axis=1)\n",
        "\n",
        "    if scale_features:\n",
        "        numeric_columns = new_data.select_dtypes(include=['number']).columns\n",
        "        scaler = StandardScaler()\n",
        "        new_data[numeric_columns] = scaler.fit_transform(new_data[numeric_columns])\n",
        "\n",
        "    if with_classes:\n",
        "      for col in ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type',\n",
        "                  'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American',\n",
        "                  'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'\n",
        "                  ]:\n",
        "            new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data\n",
        "\n",
        "'''Calculate accuracy on error'''\n",
        "\n",
        "def accuracy_error (results, error =None):\n",
        "  if len(results) == 0:\n",
        "    print (\"you are calculating the accuracy on an empty cluster\")\n",
        "  correct = results.loc[results[error] == 0]\n",
        "  acc = len(correct)/ len(results)\n",
        "  return acc\n",
        "\n",
        "'''Calculate bias based on accuracy_error. The accuracy of a selected cluster - accruacy of the remaining clusters\n",
        " Colster col: name of the DF column where the cluster assignments are'''\n",
        "\n",
        "def bias_w_error (data, cluster_id, cluster_col):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  if len(cluster_x) ==0:\n",
        "    print(\"this is an empty cluster\", cluster_id)\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "  if len(remaining_clusters) ==0:\n",
        "    print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "  return accuracy_error(cluster_x) - accuracy_error(remaining_clusters)\n",
        "\n",
        "'''Returns a value for max negative and positive bias. returns a value'''\n",
        "\n",
        "def get_max_bias(data, bias_type = 'negative', function = bias_w_error):\n",
        "  max_bias = float('inf') if bias_type == 'negative' else -float('inf') #initializes max_bias with either positive of negative infinity (special floating point value) based on bias_type param\n",
        "  for cluster_number in data['new_clusters'].unique():\n",
        "    if cluster_number == -1:#outliers in dbscan\n",
        "      continue\n",
        "    current_bias = function(data, cluster_number, 'new_clusters') #for each cluster the bias_w_error function is calculated\n",
        "    if (bias_type == 'negative' and current_bias < max_bias) or (bias_type == 'positive' and current_bias > max_bias):\n",
        "      max_bias = current_bias\n",
        "  print(f'Maximum {bias_type} bias is:', max_bias)\n",
        "  return max_bias\n",
        "\n",
        "\n",
        "'''Returns a cluster for max neg bias (for newly added clusters)'''\n",
        "\n",
        "def get_cluster_max_bias(data, function = bias_w_error):\n",
        "  max_pos_bias = 100 #max_abs bias selma code\n",
        "  max_bias_cluster = -2\n",
        "  for cluster_number in data['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    current_bias = (function(data, cluster_number, 'clusters')) #pos function to find the highest bias\n",
        "    print(f\"{cluster_number} has bias {current_bias}\")\n",
        "    if current_bias < max_pos_bias:\n",
        "      max_pos_bias = current_bias\n",
        "      max_bias_cluster = cluster_number\n",
        "  print ('cluster with the highest discriminating bias:', max_bias_cluster)\n",
        "  return max_bias_cluster\n",
        "\n",
        "'''Select a new cluster to split on based on the smallest absolute difference from the overall error rate of 0.5\n",
        "Function requires a df.columns named 'clusters' and an error column (fp or fn)'''\n",
        "\n",
        "def select_new_cluster(data, error_column=None, overall_error_rate=0.5):\n",
        "    smallest_diff = 1\n",
        "    selected_cluster = None\n",
        "\n",
        "    if error_column is None:\n",
        "        error_column = 'FP'  # Default to 'FP' if error_column is not specified\n",
        "\n",
        "    for cluster_number in data['clusters'].unique():\n",
        "        if cluster_number == -1:\n",
        "            continue\n",
        "        cluster_data = data[data['clusters'] == cluster_number]\n",
        "        cluster_error_rate = cluster_data[error_column].mean()  # Use specified error column\n",
        "        abs_diff = abs(overall_error_rate - cluster_error_rate)\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_number\n",
        "\n",
        "    return selected_cluster\n",
        "\n",
        "'''Calculate variance based on error'''\n",
        "\n",
        "def calculate_variance(data):\n",
        "  variance_list_local = []\n",
        "  for j in data['clusters'].unique():\n",
        "    average_acc = accuracy_error(data)\n",
        "    bias_clus = bias_w_error(data, j, 'clusters')\n",
        "    variance_list_local.append(bias_clus)\n",
        "  variance = np.variance(variance_list_local)\n",
        "  return variance\n",
        "\n",
        "'''Calculate bias_acc_global'''\n",
        "\n",
        "def calculate_bias_global_average(data, cluster_id, cluster_col, ave_acc):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  return accuracy_error(cluster_x) - ave_acc\n",
        "\n",
        "'''Get min splittable cluster size - returns size of smallest new cluster'''\n",
        "def min_split_cluster_size(data):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1:\n",
        "      continue\n",
        "    size = len(data.loc[data['new_clusters']==i])\n",
        "    if size < min_cluster_size:\n",
        "      min_cluster_size = size\n",
        "  return min_cluster_size\n",
        "\n",
        "\n",
        "'''Select a random cluster from provided list of clusters that is not -1'''\n",
        "def get_random_cluster(clusters):\n",
        "  result = -1\n",
        "  while (result == -1):\n",
        "    result = random.randint(0, len(clusters.unique()))\n",
        "  print('This is the random cluster we picked:', result)\n",
        "  return result\n",
        "\n",
        "'''Plot cluster '''\n",
        "def plot_clusters(data):\n",
        "  scatterplot = sns.scatterplot(data=data, x='1st', y='2nd', hue=\"clusters\", size = 'errors', sizes=(100, 20), palette = \"tab10\")\n",
        "  plt.show()\n",
        "\n",
        "'''Tsne plot'''\n",
        "def tsne_plot(data, title, perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5):\n",
        "    tsne_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                              'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other' ], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                           'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other']]\n",
        "\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KR-dg62TY-42"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fJsvgYjud9xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering with K-means"
      ],
      "metadata": {
        "id": "coAQK2yyd_wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clus_model_kwargs = { \"n_clusters\": 2,\n",
        "    \"init\": \"k-means++\", #method used to initialize the initial cluster centroids.\n",
        "    \"n_init\": 10, #nr of times K-means  will be run with different centroid seeds\n",
        "    \"max_iter\": 300,\n",
        "}"
      ],
      "metadata": {
        "id": "hYjyaNzujnHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bias_with_error_rate_kmeans(data = TPFN_data, plot_clusters = True):\n",
        "\n",
        "    # Initialize the dataset\n",
        "    data = initialize_dataset(data)\n",
        "    variance_list = []\n",
        "    acc = accuracy_error(data, error = 'FN') #Calculating accuracy based on FN error\n",
        "\n",
        "    #Loop for clustering iterations\n",
        "    for i in range(1, max_iter):\n",
        "        print('This is the current cluster: ', x) #for each iteration the current cluster is printed\n",
        "        eps = eps - 0.001 #the eps value is incrementedly decreased to identify dense areas\n",
        "        if len(data['clusters'].unique()) != 1:\n",
        "            variance_list.append(calculate_variance(data)) #variance calculation is performed if the nr of unique clusters is not equal to 1\n",
        "\n",
        "        data['new_clusters'] = -2\n",
        "        candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "        if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "            x = get_random_cluster(data['clusters'])\n",
        "            continue"
      ],
      "metadata": {
        "id": "xEUPGU5ZeFk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}