{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKZVwmVmEYFM9tYsYLxNYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/Kmeans_Bafv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "1PdnCtzI9weo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um3esJVD9GFd",
        "outputId": "d5486c62-6415-45ce-f657-76419469478e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! cd '/content/drive/MyDrive/Mirthe_Supervision /Paper#2/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import f_oneway\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9Z4rfAWL90G_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "tTn_RgT8-DRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set of Features (aka data columns)"
      ],
      "metadata": {
        "id": "dahEhY_--FiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Master dataset\n",
        "#FEATURES\n",
        "META = ['clusters', 'new_clusters']\n",
        "ERROR = ['errors', 'TP', 'TN', 'FN', 'FP']\n",
        "REG = ['bank_branch_count_8w',\n",
        "       #'credit_risk_score',\n",
        "       'device_os','month', 'session_length_in_minutes', 'email_is_free',\n",
        "       'proposed_credit_limit',\n",
        "       'name_email_similarity',\n",
        "      'zip_count_4w', 'date_of_birth_distinct_emails_4w', 'phone_mobile_valid', 'has_other_cards', 'foreign_request']\n",
        "SEN = ['customer_age', 'income'] #protected attributes in BAF paper\n",
        "DUMMY = ['source_INTERNET', 'Source_TELEAPP', 'device_os_other', 'device_os_macintosh','device_os_linux','device_os_windows', 'device_os_x11']\n",
        "\n",
        "#FEATURES SCALED\n",
        "ERROR_scaled = ['errors_scaled']\n",
        "REG_scaled = ['bank_branch_count_8w_scaled',\n",
        "              #'credit_risk_score_scaled',\n",
        "              'device_os_scaled','month_scaled', 'session_length_in_minutes_scaled',\n",
        "              'email_is_free_scaled',\n",
        "              'proposed_credit_limit_scaled',\n",
        "              'name_email_similarity_scaled',\n",
        "              'zip_count_4w_scaled',\n",
        "              'date_of_birth_distinct_emails_4w_scaled', 'phone_home_valid_scaled','has_other_cards_scaled', 'foreign_request_scaled']\n",
        "SEN_scaled = ['customer_age_scaled', 'income_scaled']\n",
        "DUMMY_scaled = ['source_INTERNET_scaled', 'Source_TELEAPP_scaled', 'device_os_other_scaled', 'device_os_macintosh_scaled', 'device_os_linux_scaled', 'device_os_windows_scaled', 'device_os_x11_scaled']\n",
        "\n",
        "\n",
        "#SHAP FEATURES\n",
        "SHAP_REG = ['bank_branch_count_8w_shap',\n",
        "            #'credit_risk_score_shap',\n",
        "            'device_os_shap', 'month_shap', 'session_length_in_minutes_shap', 'email_is_free_shap', 'proposed_credit_limit_shap', 'name_email_similarity_shap',\n",
        " 'zip_count_4w_shap', 'date_of_birth_distinct_emails_4w_shap', 'phone_mobile_valid_shap', 'has_other_cards_shap', 'foreign_request_shap']\n",
        "SHAP_SEN = ['customer_age_shap', 'income_shap']\n",
        "SHAP_DUMMY = ['source_INTERNET_shap', 'Source_TELEAPP_shap', 'device_os_other_shap', 'device_os_macintosh_shap','device_os_linux_shap','device_os_windows_shap', 'device_os_x11_shap']\n",
        "\n",
        "#SHAP FEATURES SCALED\n",
        "SHAP_REG_scaled = ['bank_branch_count_8w_shap_scaled',\n",
        "                   #'credit_risk_score_shap_scaled',\n",
        "                   'device_os_shap_scaled', 'month_shap_scaled', 'session_length_in_minutes_shap_scaled',\n",
        " 'email_is_free_shap_scaled', 'proposed_credit_limit_shap_scaled', 'name_email_similarity_shap_scaled', 'zip_count_4w_shap_scaled',\n",
        " 'date_of_birth_distinct_emails_4w_shap_scaled', 'phone_mobile_valid_shap_scaled', 'has_other_cards_shap_scaled', 'foreign_request_shap_scaled']\n",
        "SHAP_SEN_scaled = ['customer_age_shap_scaled', 'income_shap_scaled']\n",
        "SHAP_DUMMY_scaled = ['source_INTERNET_shap_scaled', 'Source_TELEAPP_shap_scaled', 'device_os_other_shap_scaled', 'device_os_macintosh_shap_scaled',\n",
        " 'device_os_linux_shap_scaled', 'device_os_windows_shap_scaled', 'device_os_x11_shap_scaled']\n",
        "\n",
        "'''removed features = 'device_fraud_count', 'intended_balcon_amount', 'payment_type', 'days_since_request',\n",
        "'velocity_6h', 'velocity_24h', 'velocity_4w', 'keep_alive_session', 'prev_address_months_count', 'current_address_months_count', 'phone_home_valid', 'bank_months_count', 'device_distinct_emails_8w', 'housing_status', 'employment_status' '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "bBZBBmk8-JQ2",
        "outputId": "781e87f7-45e5-442d-d1d2-8a7f301e0a0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"removed features = 'device_fraud_count', 'intended_balcon_amount', 'payment_type', 'days_since_request',\\n'velocity_6h', 'velocity_24h', 'velocity_4w', 'keep_alive_session', 'prev_address_months_count', 'current_address_months_count', 'phone_home_valid', 'bank_months_count', 'device_distinct_emails_8w', 'housing_status', 'employment_status' \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for Data prep"
      ],
      "metadata": {
        "id": "MrCFVWz4CMiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate TPFN & TNFP dataset\n",
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def subset_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def subset_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "'''undo Dummy for DUMMY_source or DUMMY_device'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)\n",
        "\n"
      ],
      "metadata": {
        "id": "duYEXRK8COmm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for Clustering"
      ],
      "metadata": {
        "id": "FTFOkvQ1Cp8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average (unscaled) Error rate by counting the amount of max values (1) and dividing them by the total nr of rows\n",
        "# Does not work on scaled (binary) error features\n",
        "def get_error_rate(data, column='errors'):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty set')\n",
        "    return\n",
        "  max_value = data[column].max()\n",
        "  count_max_value = (data[column] == max_value).sum()\n",
        "  average_error_rate = count_max_value / len(data)\n",
        "  return average_error_rate\n",
        "\n",
        "\n",
        "def get_next_cluster(data, cluster_col, min_size, all_cluster_ids, banned_clusters):\n",
        "  if(len(banned_clusters) != 0):\n",
        "    filter_tf = np.isin(all_cluster_ids, banned_clusters, invert=True)\n",
        "    all_cluster_ids = all_cluster_ids[filter_tf]\n",
        "\n",
        "  for candidate_cluster_id in all_cluster_ids:\n",
        "    if candidate_cluster_id == -1:\n",
        "      continue\n",
        "\n",
        "    #print ('This is the next cluster:', candidate_cluster_id)\n",
        "\n",
        "    candidate_cluster = data.loc[data[cluster_col] == candidate_cluster_id]\n",
        "\n",
        "    if len(candidate_cluster) < min_size:\n",
        "      #print('...it is too small:', len(candidate_cluster))\n",
        "      continue\n",
        "    else:\n",
        "      return(candidate_cluster_id)\n",
        "\n",
        "  #print('No suitable clusters were found!')\n",
        "  return(-1)\n",
        ""
      ],
      "metadata": {
        "id": "yx91KNqYCnXD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for Results"
      ],
      "metadata": {
        "id": "1Q4IY7w8DXMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make recap"
      ],
      "metadata": {
        "id": "_CnlAGCQDb16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_recap(data_result, feature_set):\n",
        "  # MAKE RECAP of cluster info\n",
        "  # ...with error rates\n",
        "  res = data_result[['clusters', 'errors']]\n",
        "\n",
        "  # ...with cluster size\n",
        "  temp = data_result[['clusters']]\n",
        "  temp['count'] = 1\n",
        "  recap = temp.groupby(['clusters'], as_index=False).sum()\n",
        "\n",
        "  # ...with number of error\n",
        "  recap['n_error'] = res.groupby(['clusters']).sum().astype(int)\n",
        "\n",
        "  # ...with 1-vs-All error diff\n",
        "  recap['error_rate'] = res.groupby(['clusters']).mean()\n",
        "  # recap['std'] = (recap['error_rate'] * (1-recap['error_rate']))/recap['count']\n",
        "  # recap['std'] = recap['std'].apply(np.sqrt)\n",
        "\n",
        "  # Prepare Quality metrics\n",
        "  diff_vs_rest = []\n",
        "  # diff_std = []\n",
        "  diff_p =[]\n",
        "\n",
        "  race_aa_prop = []\n",
        "  race_aa_diff = []\n",
        "  race_aa_p = []\n",
        "\n",
        "  race_c_prop = []\n",
        "  race_c_diff = []\n",
        "  race_c_p = []\n",
        "\n",
        "  female_prop = []\n",
        "  female_diff = []\n",
        "  female_p = []\n",
        "\n",
        "  silhouette = []\n",
        "\n",
        "  # Get individual silhouette scores\n",
        "  clusters = data_result['clusters']\n",
        "  if(len(recap['clusters'].unique()) > 1):\n",
        "    silhouette_val = silhouette_samples(data_result[feature_set], clusters)\n",
        "\n",
        "  for c in recap['clusters']:\n",
        "    # Get in-cluster data\n",
        "    c_data = data_result.loc[data_result['clusters'] == c]\n",
        "    c_count = recap['count'][c]\n",
        "\n",
        "    # Get out-of-cluster data\n",
        "    rest_data = data_result.loc[data_result['clusters'] != c]\n",
        "    # Check if no other cluster\n",
        "    if(len(rest_data) == 0):\n",
        "      diff_vs_rest.append(np.nan)\n",
        "      # diff_std.append(np.nan)\n",
        "      diff_p.append(np.nan)\n",
        "\n",
        "      race_aa_prop.append(np.nan)\n",
        "      race_aa_diff.append(np.nan)\n",
        "      race_aa_p.append(np.nan)\n",
        "\n",
        "      race_c_prop.append(np.nan)\n",
        "      race_c_diff.append(np.nan)\n",
        "      race_c_p.append(np.nan)\n",
        "\n",
        "      female_prop.append(np.nan)\n",
        "      female_diff.append(np.nan)\n",
        "      female_p.append(np.nan)\n",
        "\n",
        "      silhouette.append(np.nan)\n",
        "      break\n",
        "\n",
        "    # Add silhouette score\n",
        "    silhouette.append(silhouette_val[clusters == c].mean())\n",
        "\n",
        "    rest_recap = recap.loc[recap['clusters'] != c]\n",
        "    rest_count = rest_recap['count'].sum()\n",
        "\n",
        "    #### Quick test: differences in error rates\n",
        "    # Get error rate difference 1-vs-rest\n",
        "    rest_n_error = rest_recap['n_error'].sum()\n",
        "    rest_rate = rest_n_error / rest_count\n",
        "    diff_vs_rest.append(recap['error_rate'][c] - rest_rate)\n",
        "\n",
        "    # ...with std deviation of error differences\n",
        "    # std_rest = (rest_rate * (1-rest_rate))/rest_count\n",
        "    # std_rest = np.sqrt(std_rest)\n",
        "    # diff_std.append(recap['std'][c] + std_rest)\n",
        "\n",
        "    # ...with Poisson stat testprint('Zero!')\n",
        "    # Deal with splits with 0 error (by using either number of errors (FN or FP), or number of correct classifications (TP or TN))\n",
        "    if((recap['n_error'][c] < 1) | (recap['count'][c] < 1) | (rest_n_error < 1) | (rest_count < 1)):\n",
        "      res = stats.poisson_means_test(recap['count'][c] - recap['n_error'][c], recap['count'][c], rest_count - rest_n_error, rest_count)\n",
        "      diff_p.append(round(res.pvalue, 3))\n",
        "    else:\n",
        "      res = stats.poisson_means_test(recap['n_error'][c], recap['count'][c], rest_n_error, rest_count)\n",
        "      diff_p.append(round(res.pvalue, 3))\n",
        "\n",
        "    ##### Sensitive features (gender, race) -- ['sex_Female', 'race_African-American', 'race_Caucasian']]\n",
        "    ### Race African-American (AA)\n",
        "    rest_n_aa = rest_data['race_African-American'].sum()\n",
        "    rest_prop_aa = rest_n_aa / rest_count\n",
        "\n",
        "    c_n_aa = c_data['race_African-American'].sum()\n",
        "    c_prop_aa = c_n_aa / c_count\n",
        "\n",
        "    race_aa_prop.append(c_prop_aa)\n",
        "    race_aa_diff.append(c_prop_aa - rest_prop_aa)\n",
        "\n",
        "    # Deal with splits with 0 African-American (by using either number of AA, or number of non-AA)\n",
        "    if((c_n_aa < 1) | (c_count < 1) | (rest_n_aa < 1) | (rest_count < 1)):\n",
        "      res = stats.poisson_means_test(c_count - c_n_aa, c_count, rest_count - rest_n_aa, rest_count)\n",
        "      race_aa_p.append(round(res.pvalue, 3))\n",
        "    else:\n",
        "      res = stats.poisson_means_test(c_n_aa, c_count, rest_n_aa, rest_count)\n",
        "      race_aa_p.append(round(res.pvalue, 3))\n",
        "\n",
        "    ### Race Caucasian\n",
        "    rest_n_c = rest_data['race_Caucasian'].sum()\n",
        "    rest_prop_c = rest_n_c / rest_count\n",
        "\n",
        "    c_n_c = c_data['race_Caucasian'].sum()\n",
        "    c_prop_c = c_n_c / c_count\n",
        "\n",
        "    race_c_prop.append(c_prop_c)\n",
        "    race_c_diff.append(c_prop_c - rest_prop_c)\n",
        "\n",
        "    # Deal with splits with 0 African-American (by using either number of AA, or number of non-AA)\n",
        "    if((c_n_c < 1) | (c_count < 1) | (rest_n_c < 1) | (rest_count < 1)):\n",
        "      res = stats.poisson_means_test(c_count - c_n_c, c_count, rest_count - rest_n_c, rest_count)\n",
        "      race_c_p.append(round(res.pvalue, 3))\n",
        "    else:\n",
        "      res = stats.poisson_means_test(c_n_c, c_count, rest_n_c, rest_count)\n",
        "      race_c_p.append(round(res.pvalue, 3))\n",
        "\n",
        "    ##### Gender\n",
        "    rest_n_female = rest_data['sex_Female'].sum()\n",
        "    rest_prop_female = rest_n_female/ rest_count\n",
        "\n",
        "    c_n_female = c_data['sex_Female'].sum()\n",
        "    c_prop_female = c_n_female / c_count\n",
        "\n",
        "    female_prop.append(c_prop_female)\n",
        "    female_diff.append(c_prop_female - rest_prop_female)\n",
        "\n",
        "    # Deal with splits with 0 females(by using either number of females, or number of males)\n",
        "    if((c_n_female < 1) | (c_count < 1) | (rest_n_female < 1) | (rest_count < 1)):\n",
        "      res = stats.poisson_means_test(c_count - c_n_female, c_count, rest_count - rest_n_female, rest_count)\n",
        "      female_p.append(round(res.pvalue, 3))\n",
        "    else:\n",
        "      res = stats.poisson_means_test(c_n_female, c_count, rest_n_female, rest_count)\n",
        "      female_p.append(round(res.pvalue, 3))\n",
        "\n",
        "  recap['diff_vs_rest'] = np.around(diff_vs_rest, 3)\n",
        "  # recap['diff_std'] = np.around(diff_std, 3)\n",
        "  recap['diff_p'] = diff_p\n",
        "\n",
        "  recap['race_aa_prop'] = np.around(race_aa_prop, 3)\n",
        "  recap['race_aa_diff'] = np.around(race_aa_diff, 3)\n",
        "  recap['race_aa_p'] = race_aa_p\n",
        "\n",
        "  recap['race_c_prop'] = np.around(race_c_prop, 3)\n",
        "  recap['race_c_diff'] = np.around(race_c_diff, 3)\n",
        "  recap['race_c_p'] = race_c_p\n",
        "\n",
        "  recap['female_prop'] = np.around(female_prop, 3)\n",
        "  recap['female_diff'] = np.around(female_diff, 3)\n",
        "  recap['female_p'] = female_p\n",
        "\n",
        "  recap['silhouette'] = silhouette\n",
        "\n",
        "  recap['error_rate'] = np.around(recap['error_rate'] , 3)\n",
        "  # recap['std'] = np.around(recap['std'] , 3)\n",
        "\n",
        "  recap.rename(columns={'clusters':'c'}, inplace=True)\n",
        "  #print(recap.sort_values(by=['diff_p']))\n",
        "\n",
        "  return(recap)"
      ],
      "metadata": {
        "id": "fUzX1WJpDZx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}