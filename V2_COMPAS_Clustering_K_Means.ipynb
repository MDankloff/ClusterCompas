{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnN7ypqqtBrlrlZKsLNSO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V2_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Shap_error_data.csv')"
      ],
      "metadata": {
        "id": "WMyJfrpwc6ys"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "META_COL = ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters']\n",
        "SHAP_COL = ['Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male',\n",
        "            'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic',\n",
        "            'Shap_race_Native American', 'Shap_race_Other']\n",
        "BASIC_COL = ['age', 'priors_count', 'sex_Female', 'sex_Male', 'race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other', 'error_scaled' ]\n",
        "META_COL_VIZ = ['predicted_class', 'true_class', 'TP', 'TN', 'FN', 'FP', 'error_scaled', 'Error_Type', 'new_clusters']\n",
        "ONE_HOT_RACE = ['race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other']\n",
        "SHORT_LABEL_RACE = ['Afr.Am.', 'Asian', 'Cauc.', 'Hisp.', 'Native', 'Other']\n",
        "ONE_HOT_GENDER = ['sex_Female', 'sex_Male']\n",
        "SHORT_LABEL_GENDER = ['Female', 'Male']"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP_data = drop_zero_TN_FP(data_shaperr)\n",
        "\n",
        "#TNFP_data.head()\n",
        "#TPFN_data.info()"
      ],
      "metadata": {
        "id": "8A9b1ah7dR22"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True):\n",
        "    new_data = data.copy(deep=True)\n",
        "    #features = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1)\n",
        "    features = new_data.drop(META_COL, axis=1)\n",
        "\n",
        "    if with_errors:\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP', 'errors']\n",
        "        new_data[error_columns] *= 0.8 #scaling factor\n",
        "\n",
        "    if just_features:\n",
        "        #drop_columns = ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type']\n",
        "        new_data = new_data.drop(META_COL, axis = 1)\n",
        "\n",
        "    if scale_features:\n",
        "        to_scale = features.columns\n",
        "        #to_scale = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1).columns\n",
        "        new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "        #new_data[to_scale] = MinMaxScaler().fit_transform(features[to_scale])\n",
        "        '''try using minmax and standardscaler'''\n",
        "\n",
        "    if with_classes:\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "      #['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type',\n",
        "                  #'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American',\n",
        "                  #'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'\n",
        "                  #]:new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    }
  ]
}