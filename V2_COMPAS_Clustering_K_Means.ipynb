{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZhhyszq1P0gr0XYTIJntz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V2_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Shap_error_data.csv')"
      ],
      "metadata": {
        "id": "WMyJfrpwc6ys"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "META_COL = ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters']\n",
        "SHAP_COL = ['Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male',\n",
        "            'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic',\n",
        "            'Shap_race_Native American', 'Shap_race_Other']\n",
        "BASIC_COL = ['age', 'priors_count', 'sex_Female', 'sex_Male', 'race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other', 'error_scaled' ]\n",
        "META_COL_VIZ = ['predicted_class', 'true_class', 'TP', 'TN', 'FN', 'FP', 'error_scaled', 'Error_Type', 'new_clusters']\n",
        "DUMMY_RACE = ['race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other']\n",
        "SHORT_LABEL_RACE = ['Afr.Am.', 'Asian', 'Cauc.', 'Hisp.', 'Native', 'Other']\n",
        "DUMMY_GENDER = ['sex_Female', 'sex_Male']\n",
        "SHORT_LABEL_GENDER = ['Female', 'Male']"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP_data = drop_zero_TN_FP(data_shaperr)\n",
        "\n",
        "#TNFP_data.head()\n",
        "#TPFN_data.info()"
      ],
      "metadata": {
        "id": "8A9b1ah7dR22"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\n",
        "\n",
        "    new_data = data.copy(deep=True)\n",
        "    features = new_data.drop(META_COL, axis=1)\n",
        "\n",
        "    if with_errors:\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP', 'errors']\n",
        "        new_data[error_columns] *= 0.8 #scaling factor\n",
        "\n",
        "    if just_features:\n",
        "        new_data = new_data.drop(META_COL, axis = 1)\n",
        "\n",
        "    if scale_features:\n",
        "        to_scale = features.columns\n",
        "        new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "        #new_data[to_scale] = MinMaxScaler().fit_transform(features[to_scale])\n",
        "        '''try using minmax and standardscaler'''\n",
        "\n",
        "    if with_Dummy:\n",
        "      for col in DUMMY_RACE + DUMMY_GENDER:\n",
        "        if col in data.columns:\n",
        "          one_hot = pd.get_dummies(data[col], prefix=col)\n",
        "          new_data = data.drop(col, axis=1)\n",
        "          new_data = pd.concat([data, one_hot], axis=1)\n",
        "\n",
        "    if with_classes:\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''undo Dummy for DUMMY_RACE or DUMMY_GENDER'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)"
      ],
      "metadata": {
        "id": "bYnrCZPlw8Of"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data_shaperr.shape)\n",
        "#data_shaperr.head()\n",
        "#data_shaperr.info()"
      ],
      "metadata": {
        "id": "4pFakXccwJYR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for BIAS in ERROR DIFFERENCE"
      ],
      "metadata": {
        "id": "l3ZumSo56l9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate Error rate based on mean - replacing old accuracy_error()\n",
        "def get_error_rate(data):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty cluster')\n",
        "    return\n",
        "  return data.loc[:, 'errors'].mean()\n",
        "\n",
        "'''Calculate BIAS in terms of Error Difference\n",
        "bias_type can be 'negative', 'positive' or 'absolute'\n",
        "baseline can be 'all' which is the overall error rate, or 'other' or 'best' '''\n",
        "\n",
        "def get_error_diff(data, cluster_id, cluster_col, bias_type = 'negative', baseline= 'all', full_info=False):\n",
        "  cluster_x= data.loc[data[cluster_col] == cluster_id]\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "\n",
        "  if len(cluster_x) == 0:\n",
        "    print ('calculating error difference on an empty cluster')\n",
        "    return\n",
        "\n",
        "  if baseline == 'all':\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(data)\n",
        "\n",
        "  elif baseline == 'other':\n",
        "    if len(remaining_clusters) == 0:\n",
        "      print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "      return\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(remaining_clusters)\n",
        "\n",
        "  #elif baseline == 'best':\n",
        "    #best_cluster = get_cluster_w_min_bias(data, cluster_col, bias_type, baseline)\n",
        "    #error_diff = get_error_rate(cluster_x) - best_cluster[1]\n",
        "\n",
        "  else:\n",
        "    print ('unknown baseline')\n",
        "    return\n",
        "\n",
        "  if full_info:\n",
        "    return [error_diff, get_error_rate(cluster_x), get_error_rate(remaining_clusters)]\n",
        "\n",
        "  if bias_type == 'negative':\n",
        "    pass #no change needed\n",
        "\n",
        "  elif bias_type == 'positive':\n",
        "    error_diff = -error_diff\n",
        "\n",
        "  elif bias_type == 'absolute':\n",
        "    error_diff = np.absolute(error_diff)\n",
        "\n",
        "  else:\n",
        "    print(\"unknown bias type\")\n",
        "    return\n",
        "\n",
        "  return error_diff"
      ],
      "metadata": {
        "id": "PhGil8f663vU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    }
  ]
}