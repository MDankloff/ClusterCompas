{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w1MYjWK6bkSL"
      ],
      "authorship_tag": "ABX9TyPEp4RNdWEn31VSQMap+cL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V2_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Shap_error_data.csv')"
      ],
      "metadata": {
        "id": "WMyJfrpwc6ys"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "META_COL = ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters']\n",
        "SHAP_COL = ['Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male',\n",
        "            'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic',\n",
        "            'Shap_race_Native American', 'Shap_race_Other']\n",
        "BASIC_COL = ['age', 'priors_count', 'sex_Female', 'sex_Male', 'race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other', 'error_scaled' ]\n",
        "META_COL_VIZ = ['predicted_class', 'true_class', 'TP', 'TN', 'FN', 'FP', 'error_scaled', 'Error_Type', 'new_clusters']\n",
        "DUMMY_RACE = ['race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other']\n",
        "SHORT_LABEL_RACE = ['Afr.Am.', 'Asian', 'Cauc.', 'Hisp.', 'Native', 'Other']\n",
        "DUMMY_GENDER = ['sex_Female', 'sex_Male']\n",
        "SHORT_LABEL_GENDER = ['Female', 'Male']"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP_data = drop_zero_TN_FP(data_shaperr)\n",
        "\n",
        "TNFP_data.head()\n",
        "TPFN_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A9b1ah7dR22",
        "outputId": "0048d9c8-d139-452b-8407-9a33ff7fe7e6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1579 entries, 1 to 5048\n",
            "Data columns (total 28 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         1579 non-null   float64\n",
            " 1   priors_count                1579 non-null   float64\n",
            " 2   sex_Female                  1579 non-null   float64\n",
            " 3   sex_Male                    1579 non-null   float64\n",
            " 4   race_African-American       1579 non-null   float64\n",
            " 5   race_Asian                  1579 non-null   float64\n",
            " 6   race_Caucasian              1579 non-null   float64\n",
            " 7   race_Hispanic               1579 non-null   float64\n",
            " 8   race_Native American        1579 non-null   float64\n",
            " 9   race_Other                  1579 non-null   float64\n",
            " 10  Shap_age                    1579 non-null   float64\n",
            " 11  Shap_priors_count           1579 non-null   float64\n",
            " 12  Shap_sex_Female             1579 non-null   float64\n",
            " 13  Shap_sex_Male               1579 non-null   float64\n",
            " 14  Shap_race_African-American  1579 non-null   float64\n",
            " 15  Shap_race_Asian             1579 non-null   float64\n",
            " 16  Shap_race_Caucasian         1579 non-null   float64\n",
            " 17  Shap_race_Hispanic          1579 non-null   float64\n",
            " 18  Shap_race_Native American   1579 non-null   float64\n",
            " 19  Shap_race_Other             1579 non-null   float64\n",
            " 20  predicted_class             1579 non-null   float64\n",
            " 21  true_class                  1579 non-null   float64\n",
            " 22  errors                      1579 non-null   float64\n",
            " 23  TP                          1579 non-null   float64\n",
            " 24  TN                          1579 non-null   float64\n",
            " 25  FN                          1579 non-null   float64\n",
            " 26  FP                          1579 non-null   float64\n",
            " 27  Error_Type                  1579 non-null   object \n",
            "dtypes: float64(27), object(1)\n",
            "memory usage: 357.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\n",
        "\n",
        "    new_data = data.copy(deep=True)\n",
        "\n",
        "    #check if META_COL exists before dropping\n",
        "    if all(col in new_data.columns for col in META_COL):\n",
        "      features = new_data.drop(META_COL, axis=1)\n",
        "    else:\n",
        "      features = new_data\n",
        "\n",
        "    if with_errors:\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP', 'errors']\n",
        "        new_data[error_columns] *= 0.8 #scaling factor\n",
        "\n",
        "    if just_features:\n",
        "      new_data = new_data.drop(META_COL, axis = 1)\n",
        "      if 'clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('clusters', axis = 1)\n",
        "      if 'new_clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('new_clusters', axis = 1)\n",
        "\n",
        "    if scale_features:\n",
        "        to_scale = features.columns\n",
        "        new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "        #new_data[to_scale] = MinMaxScaler().fit_transform(features[to_scale])\n",
        "        '''try using minmax and standardscaler'''\n",
        "\n",
        "    if with_Dummy:\n",
        "      for col in DUMMY_RACE + DUMMY_GENDER:\n",
        "        if col in data.columns:\n",
        "          one_hot = pd.get_dummies(data[col], prefix=col)\n",
        "          new_data = data.drop(col, axis=1)\n",
        "          new_data = pd.concat([data, one_hot], axis=1)\n",
        "\n",
        "    if with_classes:\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''undo Dummy for DUMMY_RACE or DUMMY_GENDER'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)"
      ],
      "metadata": {
        "id": "bYnrCZPlw8Of"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data_shaperr.shape)\n",
        "#data_shaperr.head()\n",
        "#data_shaperr.info()"
      ],
      "metadata": {
        "id": "4pFakXccwJYR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for BIAS in ERROR DIFFERENCE"
      ],
      "metadata": {
        "id": "l3ZumSo56l9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate Error rate based on mean - replacing old accuracy_error()\n",
        "def get_error_rate(data):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty cluster')\n",
        "    return\n",
        "  return data.loc[:, 'errors'].mean()\n",
        "\n",
        "'''Calculate BIAS in terms of Error Difference\n",
        "bias_type can be 'negative', 'positive' or 'absolute'\n",
        "baseline can be 'all' which is the overall error rate, or 'other' or 'best' '''\n",
        "\n",
        "def get_error_diff(data, cluster_id, cluster_col, bias_type = 'negative', baseline= 'all', full_info=False, function = get_error_rate):\n",
        "  cluster_x= data.loc[data[cluster_col] == cluster_id]\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "\n",
        "  if len(cluster_x) == 0:\n",
        "    print ('calculating error difference on an empty cluster')\n",
        "    return\n",
        "\n",
        "  if baseline == 'all':\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(data)\n",
        "\n",
        "  elif baseline == 'other':\n",
        "    if len(remaining_clusters) == 0:\n",
        "      print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "      return\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(remaining_clusters)\n",
        "\n",
        "  #elif baseline == 'best':\n",
        "    #best_cluster = get_cluster_w_min_bias(data, cluster_col, bias_type, baseline)\n",
        "    #error_diff = get_error_rate(cluster_x) - best_cluster[1]\n",
        "  else:\n",
        "    print ('unknown baseline')\n",
        "    return\n",
        "\n",
        "  if full_info:\n",
        "    return [error_diff, get_error_rate(cluster_x), get_error_rate(remaining_clusters)]\n",
        "\n",
        "  if bias_type == 'negative':\n",
        "    pass #no change needed\n",
        "  elif bias_type == 'positive':\n",
        "    error_diff = -error_diff\n",
        "  elif bias_type == 'absolute':\n",
        "    error_diff = np.absolute(error_diff)\n",
        "  else:\n",
        "    print(\"unknown bias type\")\n",
        "    return\n",
        "\n",
        "  return error_diff"
      ],
      "metadata": {
        "id": "PhGil8f663vU"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for VISUALS"
      ],
      "metadata": {
        "id": "w1MYjWK6bkSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop Meta_colums\n",
        "    pca_features = data.drop(META_COL, axis=1)\n",
        "    other_features = data(META_COL)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['PC1', 'PC2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='PC1', y='PC2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def tsne_plot(data, title, perplexity, learning_rate, n_iter, alpha = 0.5):\n",
        "    # Extract features for TSNE and drop Meta_colums\n",
        "    tsne_features = data.drop(META_COL, axis=1)\n",
        "    other_features = data(META_COL)\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity= 30, learning_rate= 200, n_iter= 1000)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = tsne_features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "HoSLp22kbykA"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS FOR CLUSTERING"
      ],
      "metadata": {
        "id": "URIWoUpSfBLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get cluster with max error difference\n",
        "def get_max_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all', function = get_error_diff):\n",
        "  max_bias = 0\n",
        "  max_bias_cluster = -2\n",
        "  full_info = [0,0,0]\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline, full_info) #pos function to find highest bias\n",
        "    if current_bias > max_bias:\n",
        "      max_bias = current_bias\n",
        "      max_bias_cluster = cluster_id\n",
        "\n",
        "      full_info = get_error_diff(data,cluster_id,cluster_col, bias_type, baseline, full_info= True)\n",
        "      return(max_bias_cluster, full_info[0])\n",
        "\n",
        "\n",
        "#get cluster with min error difference\n",
        "def get_min_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all'):\n",
        "  min_bias = 1\n",
        "  min_bias_cluster = -2\n",
        "  full_info = [0,0,0]\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline, full_info) #\n",
        "    if current_bias < min_bias:\n",
        "      min_bias = current_bias\n",
        "      min_bias_cluster = cluster_id\n",
        "\n",
        "  full_info = get_error_diff(data,cluster_id,cluster_col, bias_type, baseline, full_info= True)\n",
        "  return(min_bias_cluster, full_info[0])\n",
        "\n",
        "#get size of the smallest cluster\n",
        "def get_min_cluster_size(data, cluster_col = 'new_clusters'):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1: #exclude the -1 clusters as they may present outliers (in dbscan?)\n",
        "      continue\n",
        "      size = len(data.loc[data['new_clusters'] == i])\n",
        "      if size < min_cluster_size: #update if new cluster size is smaller\n",
        "        min_cluster_size = size\n",
        "  return(min_cluster_size)\n",
        "\n",
        "def get_random_cluster(data, cluster_col, min_splittable_cluster_size, previous_cluster, all_cluster_ids):\n",
        "  for candidate_cluster_id in all_cluster_ids:\n",
        "    if candidate_cluster_id == -1 or candidate_cluster_id == previous_cluster:\n",
        "      continue\n",
        "      print ('This is the random cluster we picked:', candidate_cluster_id)\n",
        "\n",
        "      candidate_cluster = data.loc[data[cluster_col] == candidate_cluster_id]\n",
        "      if len(candidate_cluster) >= min_splittable_cluster_size:\n",
        "        print('it is too small:', len(candidate_cluster))\n",
        "        continue\n",
        "      else:\n",
        "        return candidate_cluster_id\n",
        "\n",
        "def select_new_cluster(data, cluster_col='clusters', error_column='errors', overall_error_rate=0.5, bias_type='negative', baseline='all'):\n",
        "    smallest_diff = float('inf')\n",
        "    selected_cluster = None\n",
        "\n",
        "    for cluster_id in data[cluster_col].unique():\n",
        "        if cluster_id == -1: #skip outlier\n",
        "            continue\n",
        "\n",
        "        error_diff = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline) #calculate the error_diff for each cluster\n",
        "\n",
        "        if error_diff is None:\n",
        "            continue\n",
        "\n",
        "        abs_diff = abs(overall_error_rate - (get_error_rate(data[data[cluster_col] == cluster_id]))) #get cluster with the smallest absolute difference with the overall error rate (0.5)\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_id\n",
        "    return selected_cluster\n",
        "\n",
        "def exit_clustering(data, msg='', bias_type='', iter=''):\n",
        "  print('Iteration ', iter, ': ', msg)\n",
        "  print('Overall error rate: ', get_error_rate(data))\n",
        "  for c in np.sort(data['clusters'].unique()):\n",
        "    print('Cluster: ', c, '\\tSize: ', len(data.loc[data['clusters'] == c]), '\\tError rate: ', get_error_rate(data.loc[data['clusters'] == c]))\n",
        "  pca_plot(data,'HBAC-DBSCAN on COMPAS - ' + bias_type + ' bias', hue='clusters', s=15, alpha=0.8)\n",
        "  return data\n",
        ""
      ],
      "metadata": {
        "id": "zApFFYGVfERC"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-MEANS CLUSTERING"
      ],
      "metadata": {
        "id": "NAD_kfGCfFN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clus_model_kwargs = { \"n_clusters\": 2, #split in two clusters\n",
        "    \"init\": \"k-means++\", # method for initializing k-means++: first centroid is chosen randomly and subsequent centriods are selected based on max distance from the nearest centriod\n",
        "    \"n_init\": 10, #K-means is sensitive to the initial placement of cluster centers - running it 10 times with different initial seeds\n",
        "    \"max_iter\": 300,} #max mr of iterations for k-means in a single run. If convergence is not achieved within 300 the algorithm stops"
      ],
      "metadata": {
        "id": "KrVYKp3WfKCo"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TPFN_data.dtypes)\n",
        "#TPFN = TPFN_data.drop('Error_Type')\n",
        "#print(TPFN.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s69zlq1xFClC",
        "outputId": "9aeb0437-89ea-4023-c293-c773dd239b08"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                           float64\n",
            "priors_count                  float64\n",
            "sex_Female                    float64\n",
            "sex_Male                      float64\n",
            "race_African-American         float64\n",
            "race_Asian                    float64\n",
            "race_Caucasian                float64\n",
            "race_Hispanic                 float64\n",
            "race_Native American          float64\n",
            "race_Other                    float64\n",
            "Shap_age                      float64\n",
            "Shap_priors_count             float64\n",
            "Shap_sex_Female               float64\n",
            "Shap_sex_Male                 float64\n",
            "Shap_race_African-American    float64\n",
            "Shap_race_Asian               float64\n",
            "Shap_race_Caucasian           float64\n",
            "Shap_race_Hispanic            float64\n",
            "Shap_race_Native American     float64\n",
            "Shap_race_Other               float64\n",
            "predicted_class               float64\n",
            "true_class                    float64\n",
            "errors                        float64\n",
            "TP                            float64\n",
            "TN                            float64\n",
            "FN                            float64\n",
            "FP                            float64\n",
            "Error_Type                     object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP_FN = initialize_dataset(TPFN_data)\n",
        "pca_plot(TP_FN, 'Compas', 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "9k8xXxvJAMPN",
        "outputId": "c2a148a3-6b0d-408b-a5f2-c8ee6fae3ebe"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['clusters', 'new_clusters'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-16bca1ed47db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTP_FN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPFN_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpca_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTP_FN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Compas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-656d4a103458>\u001b[0m in \u001b[0;36minitialize_dataset\u001b[0;34m(data, with_errors, just_features, scale_features, with_classes, with_Dummy)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjust_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETA_COL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'clusters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clusters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5256\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m         \"\"\"\n\u001b[0;32m-> 5258\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5259\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5260\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4590\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4591\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['clusters', 'new_clusters'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOVA SIGNIFICANCE TESTING"
      ],
      "metadata": {
        "id": "FK_n1GA3fKgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XhKl7z5mfODS"
      }
    }
  ]
}