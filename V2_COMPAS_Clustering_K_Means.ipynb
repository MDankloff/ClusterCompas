{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "amdTO-ejctkD",
        "WoAA5JWHc_qW",
        "w1MYjWK6bkSL"
      ],
      "authorship_tag": "ABX9TyOuJ4o5zttCNRsxjD+GO9SU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V2_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Shap_error_data.csv')"
      ],
      "metadata": {
        "id": "WMyJfrpwc6ys"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "META_COL = ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters']\n",
        "SHAP_COL = ['Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male',\n",
        "            'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic',\n",
        "            'Shap_race_Native American', 'Shap_race_Other']\n",
        "BASIC_COL = ['age', 'priors_count', 'sex_Female', 'sex_Male', 'race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other', 'error_scaled' ]\n",
        "META_COL_VIZ = ['predicted_class', 'true_class', 'TP', 'TN', 'FN', 'FP', 'error_scaled', 'Error_Type', 'new_clusters']\n",
        "DUMMY_RACE = ['race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other']\n",
        "SHORT_LABEL_RACE = ['Afr.Am.', 'Asian', 'Cauc.', 'Hisp.', 'Native', 'Other']\n",
        "DUMMY_GENDER = ['sex_Female', 'sex_Male']\n",
        "SHORT_LABEL_GENDER = ['Female', 'Male']"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP_data = drop_zero_TN_FP(data_shaperr)\n",
        "\n",
        "#TNFP_data.head()\n",
        "#TPFN_data.info()"
      ],
      "metadata": {
        "id": "8A9b1ah7dR22"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\n",
        "\n",
        "    new_data = data.copy(deep=True)\n",
        "    features = new_data.drop(META_COL, axis=1)\n",
        "\n",
        "    if with_errors:\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP', 'errors']\n",
        "        new_data[error_columns] *= 0.8 #scaling factor\n",
        "\n",
        "    if just_features:\n",
        "        new_data = new_data.drop(META_COL, axis = 1)\n",
        "\n",
        "    if scale_features:\n",
        "        to_scale = features.columns\n",
        "        new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "        #new_data[to_scale] = MinMaxScaler().fit_transform(features[to_scale])\n",
        "        '''try using minmax and standardscaler'''\n",
        "\n",
        "    if with_Dummy:\n",
        "      for col in DUMMY_RACE + DUMMY_GENDER:\n",
        "        if col in data.columns:\n",
        "          one_hot = pd.get_dummies(data[col], prefix=col)\n",
        "          new_data = data.drop(col, axis=1)\n",
        "          new_data = pd.concat([data, one_hot], axis=1)\n",
        "\n",
        "    if with_classes:\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''undo Dummy for DUMMY_RACE or DUMMY_GENDER'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)"
      ],
      "metadata": {
        "id": "bYnrCZPlw8Of"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data_shaperr.shape)\n",
        "#data_shaperr.head()\n",
        "#data_shaperr.info()"
      ],
      "metadata": {
        "id": "4pFakXccwJYR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for BIAS in ERROR DIFFERENCE"
      ],
      "metadata": {
        "id": "l3ZumSo56l9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate Error rate based on mean - replacing old accuracy_error()\n",
        "def get_error_rate(data):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty cluster')\n",
        "    return\n",
        "  return data.loc[:, 'errors'].mean()\n",
        "\n",
        "'''Calculate BIAS in terms of Error Difference\n",
        "bias_type can be 'negative', 'positive' or 'absolute'\n",
        "baseline can be 'all' which is the overall error rate, or 'other' or 'best' '''\n",
        "\n",
        "def get_error_diff(data, cluster_id, cluster_col, bias_type = 'negative', baseline= 'all', full_info=False, function = get_error_rate):\n",
        "  cluster_x= data.loc[data[cluster_col] == cluster_id]\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "\n",
        "  if len(cluster_x) == 0:\n",
        "    print ('calculating error difference on an empty cluster')\n",
        "    return\n",
        "\n",
        "  if baseline == 'all':\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(data)\n",
        "\n",
        "  elif baseline == 'other':\n",
        "    if len(remaining_clusters) == 0:\n",
        "      print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "      return\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(remaining_clusters)\n",
        "\n",
        "  #elif baseline == 'best':\n",
        "    #best_cluster = get_cluster_w_min_bias(data, cluster_col, bias_type, baseline)\n",
        "    #error_diff = get_error_rate(cluster_x) - best_cluster[1]\n",
        "  else:\n",
        "    print ('unknown baseline')\n",
        "    return\n",
        "\n",
        "  if full_info:\n",
        "    return [error_diff, get_error_rate(cluster_x), get_error_rate(remaining_clusters)]\n",
        "\n",
        "  if bias_type == 'negative':\n",
        "    pass #no change needed\n",
        "  elif bias_type == 'positive':\n",
        "    error_diff = -error_diff\n",
        "  elif bias_type == 'absolute':\n",
        "    error_diff = np.absolute(error_diff)\n",
        "  else:\n",
        "    print(\"unknown bias type\")\n",
        "    return\n",
        "\n",
        "  return error_diff"
      ],
      "metadata": {
        "id": "PhGil8f663vU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for VISUALS"
      ],
      "metadata": {
        "id": "w1MYjWK6bkSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop Meta_colums\n",
        "    pca_features = data.drop(META_COL, axis=1)\n",
        "    other_features = data(META_COL)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['PC1', 'PC2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='PC1', y='PC2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def tsne_plot(data, title, perplexity, learning_rate, n_iter, alpha = 0.5):\n",
        "    # Extract features for TSNE and drop Meta_colums\n",
        "    tsne_features = data.drop(META_COL, axis=1)\n",
        "    other_features = data(META_COL)\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity= 30, learning_rate= 200, n_iter= 1000)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = tsne_features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "HoSLp22kbykA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS FOR CLUSTERING"
      ],
      "metadata": {
        "id": "URIWoUpSfBLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get cluster with max error difference\n",
        "def get_max_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all', function = get_error_diff):\n",
        "  max_bias = 0\n",
        "  max_bias_cluster = -2\n",
        "  full_info = [0,0,0]\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline, full_info) #pos function to find highest bias\n",
        "    if current_bias > max_bias:\n",
        "      max_bias = current_bias\n",
        "      max_bias_cluster = cluster_id\n",
        "\n",
        "      full_info = get_error_diff(data,cluster_id,cluster_col, bias_type, baseline, full_info= True)\n",
        "      return(max_bias_cluster, full_info[0])\n",
        "\n",
        "\n",
        "#get cluster with min error difference\n",
        "def get_min_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all'):\n",
        "  min_bias = 1\n",
        "  min_bias_cluster = -2\n",
        "  full_info = [0,0,0]\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline, full_info) #\n",
        "    if current_bias < min_bias:\n",
        "      min_bias = current_bias\n",
        "      min_bias_cluster = cluster_id\n",
        "\n",
        "  full_info = get_error_diff(data,cluster_id,cluster_col, bias_type, baseline, full_info= True)\n",
        "  return(min_bias_cluster, full_info[0])\n",
        "\n",
        "#get size of the smallest cluster\n",
        "def get_min_cluster_size(data, cluster_col = 'new_clusters'):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1: #exclude the -1 clusters as they may present outliers (in dbscan?)\n",
        "      continue\n",
        "      size = len(data.loc[data['new_clusters'] == i])\n",
        "      if size < min_cluster_size: #update if new cluster size is smaller\n",
        "        min_cluster_size = size\n",
        "  return(min_cluster_size)\n",
        "\n",
        "def get_random_cluster(data, cluster_col, min_splittable_cluster_size, previous_cluster, all_cluster_ids):\n",
        "  for candidate_cluster_id in all_cluster_ids:\n",
        "    if candidate_cluster_id == -1 or candidate_cluster_id == previous_cluster:\n",
        "      continue\n",
        "      print ('This is the random cluster we picked:', candidate_cluster_id)\n",
        "\n",
        "#get cluster with the smallest absolute difference with the overall error rate (0.5)\n",
        "def select_new_cluster(data, cluster_col='clusters', error_column='errors', overall_error_rate=0.5, bias_type='negative', baseline='all'):\n",
        "    smallest_diff = float('inf')\n",
        "    selected_cluster = None\n",
        "\n",
        "    for cluster_id in data[cluster_col].unique():\n",
        "        if cluster_id == -1: #skip outlier\n",
        "            continue\n",
        "\n",
        "        error_diff = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline) #calculate the error_diff for each cluster\n",
        "\n",
        "        if error_diff is None:\n",
        "            continue\n",
        "\n",
        "        abs_diff = abs(overall_error_rate - (get_error_rate(data[data[cluster_col] == cluster_id])))\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_id\n",
        "\n",
        "    return selected_cluster\n"
      ],
      "metadata": {
        "id": "zApFFYGVfERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-MEANS CLUSTERING"
      ],
      "metadata": {
        "id": "NAD_kfGCfFN5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KrVYKp3WfKCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOVA SIGNIFICANCE TESTING"
      ],
      "metadata": {
        "id": "FK_n1GA3fKgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XhKl7z5mfODS"
      }
    }
  ]
}