{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODesjDFD1DOuWgCsAuscP/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V3_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Compas_w_error_shap.csv')\n",
        "data_shaperr.head()\n",
        "data_shaperr.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lncfm9EwEifF",
        "outputId": "e0c960d7-e8d7-40bc-ecd6-550130875758"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3545 entries, 0 to 3544\n",
            "Data columns (total 43 columns):\n",
            " #   Column                                  Non-Null Count  Dtype  \n",
            "---  ------                                  --------------  -----  \n",
            " 0   age_unscaled                            3545 non-null   float64\n",
            " 1   decile_score_unscaled                   3545 non-null   float64\n",
            " 2   priors_count_unscaled                   3545 non-null   float64\n",
            " 3   sex_Female_unscaled                     3545 non-null   float64\n",
            " 4   sex_Male_unscaled                       3545 non-null   float64\n",
            " 5   race_African-American_unscaled          3545 non-null   float64\n",
            " 6   race_Asian_unscaled                     3545 non-null   float64\n",
            " 7   race_Caucasian_unscaled                 3545 non-null   float64\n",
            " 8   race_Hispanic_unscaled                  3545 non-null   float64\n",
            " 9   race_Native American_unscaled           3545 non-null   float64\n",
            " 10  race_Other_unscaled                     3545 non-null   float64\n",
            " 11  age_scaled                              3545 non-null   float64\n",
            " 12  decile_score_scaled                     3545 non-null   float64\n",
            " 13  priors_count_scaled                     3545 non-null   float64\n",
            " 14  sex_Female_scaled                       3545 non-null   float64\n",
            " 15  sex_Male_scaled                         3545 non-null   float64\n",
            " 16  race_African-American_scaled            3545 non-null   float64\n",
            " 17  race_Asian_scaled                       3545 non-null   float64\n",
            " 18  race_Caucasian_scaled                   3545 non-null   float64\n",
            " 19  race_Hispanic_scaled                    3545 non-null   float64\n",
            " 20  race_Native American_scaled             3545 non-null   float64\n",
            " 21  race_Other_scaled                       3545 non-null   float64\n",
            " 22  predicted_class                         3545 non-null   float64\n",
            " 23  true_class                              3545 non-null   float64\n",
            " 24  errors                                  3545 non-null   float64\n",
            " 25  TP                                      3545 non-null   float64\n",
            " 26  TN                                      3545 non-null   float64\n",
            " 27  FN                                      3545 non-null   float64\n",
            " 28  FP                                      3545 non-null   float64\n",
            " 29  Error_Type                              3545 non-null   object \n",
            " 30  sex                                     3545 non-null   object \n",
            " 31  race                                    3545 non-null   object \n",
            " 32  Shap_age_basicscaled                    3545 non-null   float64\n",
            " 33  Shap_decile_score_basicscaled           3545 non-null   float64\n",
            " 34  Shap_priors_count_basicscaled           3545 non-null   float64\n",
            " 35  Shap_sex_Female_basicscaled             3545 non-null   float64\n",
            " 36  Shap_sex_Male_basicscaled               3545 non-null   float64\n",
            " 37  Shap_race_African-American_basicscaled  3545 non-null   float64\n",
            " 38  Shap_race_Asian_basicscaled             3545 non-null   float64\n",
            " 39  Shap_race_Caucasian_basicscaled         3545 non-null   float64\n",
            " 40  Shap_race_Hispanic_basicscaled          3545 non-null   float64\n",
            " 41  Shap_race_Native American_basicscaled   3545 non-null   float64\n",
            " 42  Shap_race_Other_basicscaled             3545 non-null   float64\n",
            "dtypes: float64(40), object(3)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "META_COL = ['clusters', 'new_clusters', 'predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP']  #'Error_scaled']\n",
        "BASIC_COL_unscaled = ['age_unscaled', 'decile_score_unscaled', 'priors_count_unscaled']\n",
        "DUMMY_GENDER_unscaled = ['sex_Female_unscaled', 'sex_Male_unscaled']\n",
        "DUMMY_RACE_unscaled = ['race_African-American_unscaled', 'race_Asian_unscaled', 'race_Caucasian_unscaled', 'race_Hispanic_unscaled',\n",
        "                      'race_Native American_unscaled', 'race_Other_unscaled']\n",
        "\n",
        "BASIC_COL_scaled = ['age_scaled', 'decile_score_scaled', ' priors_count_scaled','sex_Female_scaled', 'sex_Male_scaled', 'race_Native American_scaled','race_Other_scaled',\n",
        "                      'race_African-American_scaled', 'race_Asian_scaled', 'race_Caucasian_scaled', 'race_Hispanic_scaled']\n",
        "#ERROR_COL = ['errors', 'TP', 'TN', 'FN', 'FP']\n",
        "#DUMMY_RACE = ['race_African-American', 'race_Asian', 'race_Caucasian',\n",
        "             'race_Hispanic', 'race_Native American', 'race_Other']\n",
        "#DUMMY_GENDER = ['sex_Female', 'sex_Male']\n",
        "SHAP_BASIC = ['Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male',\n",
        "            'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic',\n",
        "            'Shap_race_Native American', 'Shap_race_Other', 'Shap_sex', 'Shap_race']\n",
        "SHAP_ERROR = ['SHAP_error']\n",
        "SHAP_DUMMY_GENDER = ['Shap_dummy_Female', 'Shap_dummy_Male']\n",
        "SHAP_DUMMY_RACE = ['Shap_dummy_AA', 'Shap_dummy_Asian', 'Shap_dummy_Caucasian', 'Shap_dummy_Hispanic', 'Shap_dummy_NativeA', 'Shap_dummy_Other']\n",
        "META_COL_VIZ = ['predicted_class', 'true_class', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'new_clusters']\n",
        "\n",
        "'''\n",
        "BASIC_SCALED = ['age_scaled', 'priors_count_scaled', 'sex_scaled', 'race_scaled']\n",
        "ERROR_SCALED = ['errors_scaled']\n",
        "DUMMY_GENDER_SCALED = ['sex_female_scaled', 'sex_male_scaled]\n",
        "DUMMY_RACE_SCALED = ['race_AA_scaled', 'race_Asian_scaled', 'race_caucasian_scaled', 'race_hispanic_scaled', 'race_NativaA_scaled', 'race_Other_scaled']\n",
        "SHAP_BA_SCALED = ['shap_age_scaled', 'shap_priors_count_scaled', 'shap_sex_scaled', 'shap_race_scaled']\n",
        "SHAP_ER_SCALED = ['shap_errors_scaled']\n",
        "SHAP_DUM_GEN_SCALED = ['shap_female_scaled', 'shap_male_scaled']\n",
        "SHAP_DUM_RACE_SCALED = ['shap_AA_scaled', 'shap_Asian_scaled', 'shap_caucasian_scaled', 'shap_hispanic_scaled', 'shap_NativaA_scaled', 'shap_Other_scaled']\n",
        "\n",
        "META_COL = ['predicted_class', 'true_class','Error_Type', 'sex', 'race']\n",
        "BASIC_COL_unscaled = ['age_unscaled', 'decile_score_unscaled', 'priors_count_unscaled','sex_Female_unscaled', 'sex_Male_unscaled',\n",
        "                      'race_African-American_unscaled', 'race_Asian_unscaled', 'race_Caucasian_unscaled', 'race_Hispanic_unscaled',\n",
        "                      'race_Native American_unscaled', 'race_Other_unscaled']\n",
        "BASIC_COL_scaled = ['age_scaled', 'decile_score_scaled', ' priors_count_scaled','sex_Female_scaled', 'sex_Male_scaled', 'race_Native American_scaled','race_Other_scaled',\n",
        "                      'race_African-American_scaled', 'race_Asian_scaled', 'race_Caucasian_scaled', 'race_Hispanic_scaled']\n",
        "ERROR_COL = ['errors', 'TP', 'TN', 'FN', 'FP']\n",
        "SHAP_COL_Basic_scaled = ['Shap_age_basicscaled', 'Shap_priors_count_basicscaled', 'Shap_sex_Female_basicscaled', 'Shap_sex_Male_basicscaled',\n",
        "                         'Shap_race_African-American_basicscaled', 'Shap_race_Asian_basicscaled', 'Shap_race_Caucasian_basicscaled',\n",
        "                         'Shap_race_Hispanic_basicscaled', 'Shap_race_Native American_basicscaled', 'Shap_race_Other_basicscaled']\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "41ec54b8-cc24-4547-d13f-6dfc23f48603"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBASIC_SCALED = ['age_scaled', 'priors_count_scaled', 'sex_scaled', 'race_scaled']\\nERROR_SCALED = ['errors_scaled']\\nDUMMY_GENDER_SCALED = ['sex_female_scaled', 'sex_male_scaled]\\nDUMMY_RACE_SCALED = ['race_AA_scaled', 'race_Asian_scaled', 'race_caucasian_scaled', 'race_hispanic_scaled', 'race_NativaA_scaled', 'race_Other_scaled']\\nSHAP_BA_SCALED = ['shap_age_scaled', 'shap_priors_count_scaled', 'shap_sex_scaled', 'shap_race_scaled']\\nSHAP_ER_SCALED = ['shap_errors_scaled']\\nSHAP_DUM_GEN_SCALED = ['shap_female_scaled', 'shap_male_scaled']\\nSHAP_DUM_RACE_SCALED = ['shap_AA_scaled', 'shap_Asian_scaled', 'shap_caucasian_scaled', 'shap_hispanic_scaled', 'shap_NativaA_scaled', 'shap_Other_scaled']\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP UTILS"
      ],
      "metadata": {
        "id": "3_mds7crD9Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate TPFN & TNFP dataset\n",
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN = drop_zero_TP_FN(data_shaperr)\n",
        "TNFP = drop_zero_TN_FP(data_shaperr)\n",
        "\n",
        "#TNFP.head()\n",
        "TPFN.info()"
      ],
      "metadata": {
        "id": "8A9b1ah7dR22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa5b6de-b8e9-4e9b-f09c-6eacaa59ae8a"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1579 entries, 1 to 5048\n",
            "Data columns (total 28 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         1579 non-null   float64\n",
            " 1   priors_count                1579 non-null   float64\n",
            " 2   sex_Female                  1579 non-null   float64\n",
            " 3   sex_Male                    1579 non-null   float64\n",
            " 4   race_African-American       1579 non-null   float64\n",
            " 5   race_Asian                  1579 non-null   float64\n",
            " 6   race_Caucasian              1579 non-null   float64\n",
            " 7   race_Hispanic               1579 non-null   float64\n",
            " 8   race_Native American        1579 non-null   float64\n",
            " 9   race_Other                  1579 non-null   float64\n",
            " 10  Shap_age                    1579 non-null   float64\n",
            " 11  Shap_priors_count           1579 non-null   float64\n",
            " 12  Shap_sex_Female             1579 non-null   float64\n",
            " 13  Shap_sex_Male               1579 non-null   float64\n",
            " 14  Shap_race_African-American  1579 non-null   float64\n",
            " 15  Shap_race_Asian             1579 non-null   float64\n",
            " 16  Shap_race_Caucasian         1579 non-null   float64\n",
            " 17  Shap_race_Hispanic          1579 non-null   float64\n",
            " 18  Shap_race_Native American   1579 non-null   float64\n",
            " 19  Shap_race_Other             1579 non-null   float64\n",
            " 20  predicted_class             1579 non-null   float64\n",
            " 21  true_class                  1579 non-null   float64\n",
            " 22  errors                      1579 non-null   float64\n",
            " 23  TP                          1579 non-null   float64\n",
            " 24  TN                          1579 non-null   float64\n",
            " 25  FN                          1579 non-null   float64\n",
            " 26  FP                          1579 non-null   float64\n",
            " 27  Error_Type                  1579 non-null   object \n",
            "dtypes: float64(27), object(1)\n",
            "memory usage: 357.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"New Initialization for Dataset\"\n",
        "\n",
        "'''1. Handle missing values\n",
        "2. One-hot-encode Dummy variables\n",
        "3. Standard cale BASIC_COL, SHAP_COL, DUMMY_COL and ERROR_COL\n",
        "4. add META_COLUMN\n",
        "5. Return dataset\n",
        "\n",
        "def INIT_data(data, with_dummy)'''"
      ],
      "metadata": {
        "id": "hv-wMxciCkAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC '''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\n",
        "\n",
        "    new_data = data.copy(deep=True).dropna() #remove rows with NaN values\n",
        "\n",
        "    if just_features: #Check if the columns exist before dropping\n",
        "      new_data = new_data.drop(columns=META_COL, errors = 'ignore')\n",
        "      if 'clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('clusters', axis=1)\n",
        "      if 'new_clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('new_clusters', axis=1)\n",
        "\n",
        "    if with_Dummy:\n",
        "      for col in DUMMY_RACE + DUMMY_GENDER:\n",
        "        if col in new_data.columns:\n",
        "          one_hot = pd.get_dummies(new_data[col], prefix=col)\n",
        "          new_data = new_data.drop(col, axis=1) #drop original dummy columns\n",
        "          new_data = pd.concat([new_data, one_hot], axis=1)\n",
        "\n",
        "    #Seperate features before scaling - drop metacols from features for scaling\n",
        "    features = new_data.drop(META_COL, errors='ignore')\n",
        "\n",
        "    if scale_features:\n",
        "      '''Ensure only numeric columns are scaled + try using minmax and standard scaler'''\n",
        "      numeric_cols = features.select_dtypes(include=['number']).columns\n",
        "      new_data[numeric_cols] = StandardScaler().fit_transform(features[numeric_cols])\n",
        "      #new_data[numeric_cols] = MinMaxScaler().fit_transform(features[numeric_cols])\n",
        "\n",
        "    if with_errors:\n",
        "        if all(col in new_data.columns for col in ERROR_COL):\n",
        "          new_data[ERROR_COL] *= 0.8 #scaling factor\n",
        "\n",
        "    if with_classes: #making sure that the class columns are retained in new dataset\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data\n",
        "\n",
        "TPFN = initialize_dataset(TPFN)\n",
        "TPFN.head()\n"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "16029256-4f62-4480-c47f-08f82ebbd2aa"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         age  priors_count  Shap_age  Shap_priors_count  Shap_sex_Female  \\\n",
              "1   0.440419      0.114094  1.506631           0.042098        -1.279802   \n",
              "6   0.186677      4.208502  0.132716          -2.360761        -0.350571   \n",
              "11  1.455387     -0.704788  0.024451           0.384648        -1.895364   \n",
              "14 -0.828290      0.318814 -0.589300          -2.098692        -0.517833   \n",
              "18 -0.236226     -0.090627  0.639837          -0.608329         0.104511   \n",
              "\n",
              "    Shap_sex_Male  Shap_race_African-American  Shap_race_Asian  \\\n",
              "1       -1.433271                    0.053966         0.247509   \n",
              "6       -0.445772                   -1.242829         0.006217   \n",
              "11      -1.132092                    0.066667        -0.063304   \n",
              "14      -0.508983                   -0.677776        -0.088368   \n",
              "18       0.078545                   -0.412613         0.090742   \n",
              "\n",
              "    Shap_race_Caucasian  Shap_race_Hispanic  ...  \\\n",
              "1              0.483832            2.190319  ...   \n",
              "6              0.011907           -0.764602  ...   \n",
              "11            -1.381890           -0.280775  ...   \n",
              "14             0.157041           -0.058714  ...   \n",
              "18             0.035370            0.149455  ...   \n",
              "\n",
              "    race_Other_-0.2399825473584469  race_Other_4.166969685951214  \\\n",
              "1                             True                         False   \n",
              "6                             True                         False   \n",
              "11                            True                         False   \n",
              "14                            True                         False   \n",
              "18                            True                         False   \n",
              "\n",
              "    sex_Female_-0.4947344131066229  sex_Female_2.0212865196108454  \\\n",
              "1                            False                           True   \n",
              "6                             True                          False   \n",
              "11                           False                           True   \n",
              "14                            True                          False   \n",
              "18                            True                          False   \n",
              "\n",
              "    sex_Male_-2.0212865196108454  sex_Male_0.4947344131066228  \\\n",
              "1                           True                        False   \n",
              "6                          False                         True   \n",
              "11                          True                        False   \n",
              "14                         False                         True   \n",
              "18                         False                         True   \n",
              "\n",
              "    predicted_class true_class  clusters  new_clusters  \n",
              "1               0.0        1.0         0            -1  \n",
              "6               0.0        1.0         0            -1  \n",
              "11              1.0        1.0         0            -1  \n",
              "14              0.0        1.0         0            -1  \n",
              "18              1.0        1.0         0            -1  \n",
              "\n",
              "[5 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f29f260-16e9-44cd-b781-fb19aa565972\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>priors_count</th>\n",
              "      <th>Shap_age</th>\n",
              "      <th>Shap_priors_count</th>\n",
              "      <th>Shap_sex_Female</th>\n",
              "      <th>Shap_sex_Male</th>\n",
              "      <th>Shap_race_African-American</th>\n",
              "      <th>Shap_race_Asian</th>\n",
              "      <th>Shap_race_Caucasian</th>\n",
              "      <th>Shap_race_Hispanic</th>\n",
              "      <th>...</th>\n",
              "      <th>race_Other_-0.2399825473584469</th>\n",
              "      <th>race_Other_4.166969685951214</th>\n",
              "      <th>sex_Female_-0.4947344131066229</th>\n",
              "      <th>sex_Female_2.0212865196108454</th>\n",
              "      <th>sex_Male_-2.0212865196108454</th>\n",
              "      <th>sex_Male_0.4947344131066228</th>\n",
              "      <th>predicted_class</th>\n",
              "      <th>true_class</th>\n",
              "      <th>clusters</th>\n",
              "      <th>new_clusters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.440419</td>\n",
              "      <td>0.114094</td>\n",
              "      <td>1.506631</td>\n",
              "      <td>0.042098</td>\n",
              "      <td>-1.279802</td>\n",
              "      <td>-1.433271</td>\n",
              "      <td>0.053966</td>\n",
              "      <td>0.247509</td>\n",
              "      <td>0.483832</td>\n",
              "      <td>2.190319</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.186677</td>\n",
              "      <td>4.208502</td>\n",
              "      <td>0.132716</td>\n",
              "      <td>-2.360761</td>\n",
              "      <td>-0.350571</td>\n",
              "      <td>-0.445772</td>\n",
              "      <td>-1.242829</td>\n",
              "      <td>0.006217</td>\n",
              "      <td>0.011907</td>\n",
              "      <td>-0.764602</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.455387</td>\n",
              "      <td>-0.704788</td>\n",
              "      <td>0.024451</td>\n",
              "      <td>0.384648</td>\n",
              "      <td>-1.895364</td>\n",
              "      <td>-1.132092</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>-0.063304</td>\n",
              "      <td>-1.381890</td>\n",
              "      <td>-0.280775</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.828290</td>\n",
              "      <td>0.318814</td>\n",
              "      <td>-0.589300</td>\n",
              "      <td>-2.098692</td>\n",
              "      <td>-0.517833</td>\n",
              "      <td>-0.508983</td>\n",
              "      <td>-0.677776</td>\n",
              "      <td>-0.088368</td>\n",
              "      <td>0.157041</td>\n",
              "      <td>-0.058714</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.236226</td>\n",
              "      <td>-0.090627</td>\n",
              "      <td>0.639837</td>\n",
              "      <td>-0.608329</td>\n",
              "      <td>0.104511</td>\n",
              "      <td>0.078545</td>\n",
              "      <td>-0.412613</td>\n",
              "      <td>0.090742</td>\n",
              "      <td>0.035370</td>\n",
              "      <td>0.149455</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f29f260-16e9-44cd-b781-fb19aa565972')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f29f260-16e9-44cd-b781-fb19aa565972 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f29f260-16e9-44cd-b781-fb19aa565972');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c3241461-d8b6-4699-ba6d-5b11f7ff48d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3241461-d8b6-4699-ba6d-5b11f7ff48d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c3241461-d8b6-4699-ba6d-5b11f7ff48d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "TPFN"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''undo Dummy for DUMMY_RACE or DUMMY_GENDER'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)"
      ],
      "metadata": {
        "id": "bYnrCZPlw8Of"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for BIAS in ERROR DIFFERENCE"
      ],
      "metadata": {
        "id": "l3ZumSo56l9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate average Error rate based on unscaled error rate by counting the amount of max values (1) and dividing them by the total nr of rows - replacing old accuracy_error()\n",
        "def get_error_rate(data, column = 'errors'):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty cluster')\n",
        "    return\n",
        "  max_value = data[column].max()\n",
        "  count_max_value = (data[column] == max_value).sum()\n",
        "  average_error_rate = count_max_value / len(data)\n",
        "  return average_error_rate\n",
        "\n",
        "'''Calculate BIAS in terms of Error Difference\n",
        "bias_type can be 'negative', 'positive' or 'absolute'\n",
        "baseline can be 'all' which is the overall error rate, or 'other' or 'best' '''\n",
        "\n",
        "def get_error_diff(data, cluster_id, cluster_col, bias_type = 'negative', baseline= 'all', #function= get_error_rate\n",
        "                   ):\n",
        "  cluster_x= data.loc[data[cluster_col] == cluster_id]\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "\n",
        "  if len(cluster_x) == 0:\n",
        "    print ('calculating error difference on an empty cluster')\n",
        "    return\n",
        "\n",
        "  if baseline == 'all':\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(data)\n",
        "\n",
        "  elif baseline == 'other':\n",
        "    if len(remaining_clusters) == 0:\n",
        "      print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "      return\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(remaining_clusters)\n",
        "\n",
        "  elif baseline == 'best':\n",
        "    best_cluster = get_min_bias_cluster(data, cluster_col, bias_type, baseline)\n",
        "    error_diff = get_error_rate(cluster_x) - best_cluster[1]\n",
        "\n",
        "  else:\n",
        "    print ('unknown baseline')\n",
        "    return\n",
        "\n",
        "  #if full_info:\n",
        "    #return [error_diff, function(cluster_x), function(remaining_clusters)]\n",
        "\n",
        "  if bias_type == 'negative':\n",
        "    pass #no change needed\n",
        "  elif bias_type == 'positive':\n",
        "    error_diff = -error_diff\n",
        "  elif bias_type == 'absolute':\n",
        "    error_diff = np.absolute(error_diff)\n",
        "  else:\n",
        "    print(\"unknown bias type\")\n",
        "    return\n",
        "\n",
        "  return error_diff"
      ],
      "metadata": {
        "id": "PhGil8f663vU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for VISUALS"
      ],
      "metadata": {
        "id": "w1MYjWK6bkSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop Meta_colums\n",
        "    pca_features = data.drop(META_COL + ERROR_COL, axis=1)\n",
        "    other_features = data[META_COL + ERROR_COL]\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['PC1', 'PC2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='PC1', y='PC2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def tsne_plot(data, title, perplexity, learning_rate, n_iter, alpha = 0.5):\n",
        "    # Extract features for TSNE and drop Meta_colums\n",
        "    tsne_features = data.drop(META_COL + ERROR_COL, axis=1)\n",
        "    other_features = data[META_COL + ERROR_COL]\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity= 30, learning_rate= 200, n_iter= 1000)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = tsne_features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HoSLp22kbykA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FSE_tpfn = initialize_dataset(TPFN_data)\n",
        "#pca_plot(TP_FN, 'Compas', 0.6)\n",
        "tsne_plot(FSE_tpfn, 'Compas', 30, 200, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BhU6II4-2mWM",
        "outputId": "6b2b0ab3-4851-4d21-bff5-3d01f24b6512"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['TP', 'TN', 'FN', 'FP'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bfe2a9e1da42>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFSE_tpfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPFN_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pca_plot(TP_FN, 'Compas', 0.6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtsne_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFSE_tpfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Compas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-33acdddcb74a>\u001b[0m in \u001b[0;36mtsne_plot\u001b[0;34m(data, title, perplexity, learning_rate, n_iter, alpha)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtsne_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Extract features for TSNE and drop Meta_colums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtsne_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETA_COL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mERRORS_COL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mother_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMETA_COL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mERRORS_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5256\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m         \"\"\"\n\u001b[0;32m-> 5258\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5259\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5260\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4590\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4591\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['TP', 'TN', 'FN', 'FP'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FSE_tnfp = initialize_dataset(TNFP_data)\n",
        "#pca_plot(TN_FP, 'Compas', 0.6)\n",
        "tsne_plot(FSE_tnfp, 'Compas', 30, 200, 1000)"
      ],
      "metadata": {
        "id": "JngeycNY2rrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS FOR CLUSTERING"
      ],
      "metadata": {
        "id": "URIWoUpSfBLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get cluster with max error difference\n",
        "def get_max_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all', function = get_error_diff):\n",
        "  max_bias = 0 #min possible bias\n",
        "  max_bias_cluster = -2\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "\n",
        "    current_bias = function(data, cluster_id, cluster_col, bias_type, baseline) #pos function to find highest bias\n",
        "\n",
        "    #if isinstance(current_bias, list):\n",
        "     # current_bias = current_bias[0]\n",
        "\n",
        "    if current_bias > max_bias:\n",
        "      max_bias = current_bias\n",
        "      max_bias_cluster = cluster_id\n",
        "\n",
        "    return(max_bias_cluster, max_bias)\n",
        "\n",
        "#get cluster with min error difference\n",
        "def get_min_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all'):\n",
        "  min_bias = 1 #max possible bias and look for smt smaller\n",
        "  min_bias_cluster = -2\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline)\n",
        "    if current_bias < min_bias:\n",
        "      min_bias = current_bias\n",
        "      min_bias_cluster = cluster_id\n",
        "  return(min_bias_cluster, min_bias)\n",
        "\n",
        "#get size of the smallest cluster\n",
        "def get_min_cluster_size(data, cluster_col = 'new_clusters'):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1: #exclude the -1 clusters as they may present outliers (in dbscan?)\n",
        "      continue\n",
        "      size = len(data.loc[data['new_clusters'] == i])\n",
        "      if size < min_cluster_size: #update if new cluster size is smaller\n",
        "        min_cluster_size = size\n",
        "  return(min_cluster_size)\n",
        "\n",
        "def get_random_cluster(data, cluster_col, min_splittable_cluster_size, previous_cluster, all_cluster_ids):\n",
        "  for candidate_cluster_id in all_cluster_ids:\n",
        "    if candidate_cluster_id == -1 or candidate_cluster_id == previous_cluster:\n",
        "      continue\n",
        "      print ('This is the random cluster we picked:', candidate_cluster_id)\n",
        "\n",
        "      candidate_cluster = data.loc[data[cluster_col] == candidate_cluster_id]\n",
        "      if len(candidate_cluster) >= min_splittable_cluster_size:\n",
        "        print('it is too small:', len(candidate_cluster))\n",
        "        continue\n",
        "      else:\n",
        "        return candidate_cluster_id\n",
        "\n",
        "def select_new_cluster(data, cluster_col='clusters', error_column='errors', overall_error_rate=0.5, bias_type='negative', baseline='all'):\n",
        "    smallest_diff = float('inf')\n",
        "    selected_cluster = None\n",
        "\n",
        "    for cluster_id in data[cluster_col].unique():\n",
        "        if cluster_id == -1: #skip outlier\n",
        "            continue\n",
        "\n",
        "        error_diff = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline) #calculate the error_diff for each cluster\n",
        "\n",
        "        if error_diff is None:\n",
        "            continue\n",
        "\n",
        "        abs_diff = abs(overall_error_rate - (get_error_rate(data[data[cluster_col] == cluster_id]))) #get cluster with the smallest absolute difference with the overall error rate (0.5)\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_id\n",
        "    return selected_cluster\n",
        "\n",
        "def exit_clustering(data, msg='', bias_type='', iter=''):\n",
        "  print('Iteration ', iter, ': ', msg)\n",
        "  print('Overall error rate: ', get_error_rate(data))\n",
        "  for c in np.sort(data['clusters'].unique()):\n",
        "    print('Cluster: ', c, '\\tSize: ', len(data.loc[data['clusters'] == c]), '\\tError rate: ', get_error_rate(data.loc[data['clusters'] == c]))\n",
        "  pca_plot(data,'HBAC-DBSCAN on COMPAS - ' + bias_type + ' bias', hue='clusters', s=15, alpha=0.8)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "zApFFYGVfERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-MEANS CLUSTERING"
      ],
      "metadata": {
        "id": "NAD_kfGCfFN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TP FN DATA"
      ],
      "metadata": {
        "id": "4bYXg1id6kTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FSE_tpfn\n",
        "def hbac_kmeans(data = FSE_tpfn, max_iter = 300, show_plot= True):\n",
        "  clus_model_kwargs = { \"n_clusters\": 2, #split in two clusters\n",
        "    \"init\": \"k-means++\", # method for initializing k-means++: first centroid is chosen randomly and subsequent centriods are selected based on max distance from the nearest centriod\n",
        "    \"n_init\": 10, #K-means is sensitive to the initial placement of cluster centers - running it 10 times with different initial seeds\n",
        "    \"max_iter\": max_iter,} #max mr of iterations for k-means in a single run. If convergence is not achieved within 300 the algorithm stops\n",
        "\n",
        "  x = 0 #initial cluster nr\n",
        "  initial_bias = 0\n",
        "\n",
        "  error_list = []\n",
        "  error_rate = get_error_rate(data, column = 'FN') #Calculating mean error rate on FN column\n",
        "\n",
        "  min_splittable_cluster_size = round(0.05 * len(data))\n",
        "  min_acceptable_cluster_size = round(0.03 * len(data))\n",
        "  print(\"error rate:\", error_rate)\n",
        "\n",
        "  #Loop for clustering iterations\n",
        "  for i in range(1, max_iter):\n",
        "    if len(data['clusters'].unique()) != 1:\n",
        "      error_list.append(get_error_rate(data)) #the error rate is calculated if the nr of unique clusters is not equal to 1\n",
        "      data['new_clusters'] = -1\n",
        "    candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "    if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "      x = get_random_cluster(data, 'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "      continue\n",
        "\n",
        "    kmeans = KMeans(**clus_model_kwargs).fit(candidate_cluster.drop(META_COL,axis=1))\n",
        "\n",
        "    candidate_cluster['new_clusters'] = pd.DataFrame(kmeans.predict(candidate_cluster.drop(META_COL, axis=1)), index=candidate_cluster.index)\n",
        "    data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "    discr_bias = get_error_diff(data, x, 'clusters', bias_type = 'negative', baseline= 'all')\n",
        "    print('discriminative bias:', discr_bias)\n",
        "\n",
        "    min_cluster_size = get_min_cluster_size(data)\n",
        "    print('Smallest cluster size:', min_cluster_size)\n",
        "\n",
        "    if (discr_bias >= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "      print (\"adding a new cluster\")\n",
        "      n_cluster = max(data['clusters'])\n",
        "      data['clusters'][data['new_clusters'] == 1] = n_cluster + 1\n",
        "\n",
        "      if show_plot:\n",
        "        #pca_plot(data, 'K-means for False Negatives', 0.6)\n",
        "        tsne_plot(data, 'K-means for False Negatives', perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5)\n",
        "        plt.show()\n",
        "\n",
        "      x = select_new_cluster(data, error_column = 'FN')\n",
        "      initial_bias = discr_bias\n",
        "\n",
        "    else:\n",
        "      x = get_random_cluster(data,'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "\n",
        "  print('MAX_ITER')\n",
        "  print(error_list)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "uZpJmkcq1u2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = FSE_tpfn, max_iter = 300, show_plot= False) #True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4L496pDztCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c= get_max_bias_cluster(FSE_tpfn)\n",
        "#highest_bias_cluster = TP_FN[TP_FN['clusters']==c]\n",
        "#len(highest_bias_cluster)\n",
        "\n",
        "Mean_error_rate_TPFN = get_error_rate(FSE_tpfn)\n",
        "\n",
        "print ('Mean error rate of full TPFN data set:', Mean_error_rate_TPFN)\n",
        "\n",
        "print (f\" cluster {c} has the highest discrimination bias for TPFN data\")\n"
      ],
      "metadata": {
        "id": "WyZlLl7Ku6iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TN FP DATA"
      ],
      "metadata": {
        "id": "gR-UqBi_6g-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''TN FP DATA'''\n",
        "def hbac_kmeans(data = FSE_tnfp, max_iter = 300, show_plot= True):\n",
        "  clus_model_kwargs = { \"n_clusters\": 2, \"init\": \"k-means++\", \"n_init\": 10, \"max_iter\": max_iter,}\n",
        "\n",
        "  x = 0 #initial cluster nr\n",
        "  initial_bias = 0\n",
        "\n",
        "  error_list = []\n",
        "  error_rate = get_error_rate(data, column = 'FP') #Calculating mean error rate on FP column\n",
        "\n",
        "  min_splittable_cluster_size = round(0.05 * len(data))\n",
        "  min_acceptable_cluster_size = round(0.03 * len(data))\n",
        "  print(\"error rate:\", error_rate)\n",
        "\n",
        "  #Loop for clustering iterations\n",
        "  for i in range(1, max_iter):\n",
        "    if len(data['clusters'].unique()) != 1:\n",
        "      error_list.append(get_error_rate(data)) #the error rate is calculated if the nr of unique clusters is not equal to 1\n",
        "      data['new_clusters'] = -1\n",
        "    candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "    if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "      x = get_random_cluster(data, 'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "      continue\n",
        "\n",
        "    kmeans = KMeans(**clus_model_kwargs).fit(candidate_cluster.drop(META_COL,axis=1))\n",
        "\n",
        "    candidate_cluster['new_clusters'] = pd.DataFrame(kmeans.predict(candidate_cluster.drop(META_COL, axis=1)), index=candidate_cluster.index)\n",
        "    data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "    discr_bias = get_error_diff(data, x, 'clusters', bias_type = 'negative', baseline= 'all')\n",
        "    print('discriminative bias:', discr_bias)\n",
        "\n",
        "    min_cluster_size = get_min_cluster_size(data)\n",
        "    print('Smallest cluster size:', min_cluster_size)\n",
        "\n",
        "    if (discr_bias >= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "      print (\"adding a new cluster\")\n",
        "      n_cluster = max(data['clusters'])\n",
        "      data['clusters'][data['new_clusters'] == 1] = n_cluster + 1\n",
        "\n",
        "      if show_plot:\n",
        "        #pca_plot(data, 'K-means for False Positives', 0.6)\n",
        "        tsne_plot(data, 'K-means for False Negatives', perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5)\n",
        "        plt.show()\n",
        "\n",
        "      x = select_new_cluster(data, error_column = 'FP')\n",
        "      initial_bias = discr_bias\n",
        "\n",
        "    else:\n",
        "      x = get_random_cluster(data,'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "\n",
        "  print('MAX_ITER')\n",
        "  print(error_list)\n",
        "  return data"
      ],
      "metadata": {
        "id": "FA_cQ1j06QdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = TN_FP, max_iter = 300, show_plot= False) #True)"
      ],
      "metadata": {
        "id": "H8aEiMm87QRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c= get_max_bias_cluster(TN_FP)\n",
        "\n",
        "Mean_error_rate_TNFP = get_error_rate(TN_FP)\n",
        "\n",
        "print ('Mean error rate of full TNFP data set:', Mean_error_rate_TNFP)\n",
        "\n",
        "print (f\" cluster {c} has the highest discrimination bias for TNFP data\")\n"
      ],
      "metadata": {
        "id": "niZ4E-xj8JS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOVA SIGNIFICANCE TESTING"
      ],
      "metadata": {
        "id": "FK_n1GA3fKgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP, ERROR & BASIC COLUMNS FOR TPFN ANOVA'''\n",
        "groupstpfn = TP_FN.groupby('clusters')['FN'].apply(list)\n",
        "anovatpfn = [np.array(groupstpfn) for groupstpfn in groupstpfn]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovatpfn)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "H5eq8K21umRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP, ERROR & BASIC COLUMNS FOR TNFP ANOVA'''\n",
        "groupstnfp = TN_FP.groupby('clusters')['FP'].apply(list)\n",
        "anovatnfp = [np.array(groupstnfp) for groupstnfp in groupstnfp]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovatnfp)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "KPAFNlzT_v8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XhKl7z5mfODS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP AND ERROR NO FEATURES"
      ],
      "metadata": {
        "id": "j65th8V0pKoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**RQ1: is there a relationshop between shap & risk of error?**\n",
        "\n",
        "TP_FN & TN_FP = *all columns* (above)\n",
        "\n",
        "SE = shap and error columns\n",
        "\n",
        "FE = all columns except shap (selma)\n",
        "\n",
        "\n",
        "--------------------------------------------------\n",
        "**RQ2: does shap help in finding disc clusters?**\n",
        "\n",
        "FS = all columns except error\n",
        "\n",
        "S = only SHAP_COL\n",
        "\n",
        "F = only BASIC_COL\n"
      ],
      "metadata": {
        "id": "YxxgBsfJ69iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SE DF: Shap and Error '''\n",
        "SE_tpfn_ = TPFN_data[META_COL + SHAP_COL]\n",
        "SE_tnfp_ = TNFP_data[META_COL + SHAP_COL]\n",
        "#SE_tnfp_.drop('Error_Type', axis = 1)\n",
        "\n",
        "SE_tpfn = initialize_dataset(SE_tpfn_)\n",
        "SE_tnfp = initialize_dataset(SE_tnfp_)\n",
        "\n",
        "SE_tpfn.info()\n",
        "SE_tnfp.info()"
      ],
      "metadata": {
        "id": "USVopU1opVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = SE_tnfp, max_iter = 300, show_plot= False)\n",
        "hbac_kmeans(data = SE_tpfn, max_iter = 300, show_plot= False)"
      ],
      "metadata": {
        "id": "OeIa2jdkqu52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP & ERROR COLUMNS FOR TPFN ANOVA'''\n",
        "groupsSE_tpfn = SE_tpfn.groupby('clusters')['FN'].apply(list)\n",
        "anovaSE_tpfn = [np.array(groupsSE_tpfn) for groupsSE_tpfn in groupsSE_tpfn]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovaSE_tpfn)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "UQVR_YQhrTXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP & ERROR COLUMNS FOR TNFP ANOVA'''\n",
        "groupsSE_tnfp = SE_tnfp.groupby('clusters')['FP'].apply(list)\n",
        "anovaSE_tnfp = [np.array(groupsSE_tnfp) for groupsSE_tnfp in groupsSE_tnfp]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovaSE_tnfp)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "1MO0-G5LTl74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ERROR AND FEATURES NO SHAP"
      ],
      "metadata": {
        "id": "mw7ZAR6tyVy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''FE_df: FEATURES AND ERROR NO SHAP (Selma) '''\n",
        "\n",
        "FE_tpfn_ = TPFN_data.drop(SHAP_COL, axis = 1) #[BASIC_COL + META_COL + DUMMY_RACE + DUMMY_GENDER]\n",
        "FE_tnfp_ = TNFP_data.drop(SHAP_COL, axis = 1) #[BASIC_COL + META_COL + DUMMY_RACE + DUMMY_GENDER]\n",
        "\n",
        "FE_tpfn = initialize_dataset(FE_tpfn_)\n",
        "FE_tnfp = initialize_dataset(FE_tnfp_)\n",
        "\n",
        "#FE_tpfn_.info()\n",
        "FE_tpfn.info()\n",
        "#FE_tnfp.info()\n",
        "#FE_tnfp_.info()"
      ],
      "metadata": {
        "id": "AsgNawceyLEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}