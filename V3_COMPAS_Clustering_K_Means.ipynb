{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4QdpbPCBiVJmI5XvDopXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/V3_COMPAS_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "amdTO-ejctkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6p03LXU_cl1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "WoAA5JWHc_qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shaperr = pd.read_csv('/content/Compas_w_error_shap.csv')\n",
        "data_shaper = data_shaperr.drop(['sex', 'race', 'predicted_class', 'true_class'], axis=1)\n",
        "#data_shaperr.head()\n",
        "#data_shaperr.info()\n",
        "data_shaper.iloc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lncfm9EwEifF",
        "outputId": "42c6d09a-76b1-4608-cef8-997abc99eff2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age_unscaled                                  31.0\n",
              "decile_score_unscaled                          7.0\n",
              "priors_count_unscaled                          0.0\n",
              "sex_Female_unscaled                            0.0\n",
              "sex_Male_unscaled                              1.0\n",
              "race_African-American_unscaled                 1.0\n",
              "race_Asian_unscaled                            0.0\n",
              "race_Caucasian_unscaled                        0.0\n",
              "race_Hispanic_unscaled                         0.0\n",
              "race_Native American_unscaled                  0.0\n",
              "race_Other_unscaled                            0.0\n",
              "age_scaled                               -0.321161\n",
              "decile_score_scaled                       0.871941\n",
              "priors_count_scaled                       -0.71124\n",
              "sex_Female_scaled                        -0.489624\n",
              "sex_Male_scaled                           0.489624\n",
              "race_African-American_scaled              0.975623\n",
              "race_Asian_scaled                         -0.06675\n",
              "race_Caucasian_scaled                    -0.718015\n",
              "race_Hispanic_scaled                     -0.311212\n",
              "race_Native American_scaled              -0.050014\n",
              "race_Other_scaled                        -0.234822\n",
              "errors                                         0.0\n",
              "TP                                             0.0\n",
              "TN                                             1.0\n",
              "FN                                             0.0\n",
              "FP                                             0.0\n",
              "Error_Type                                      TN\n",
              "Shap_age_basicscaled                     -0.069741\n",
              "Shap_decile_score_basicscaled            -0.146266\n",
              "Shap_priors_count_basicscaled            -0.043785\n",
              "Shap_sex_Female_basicscaled               0.006644\n",
              "Shap_sex_Male_basicscaled                 0.002615\n",
              "Shap_race_African-American_basicscaled     -0.0362\n",
              "Shap_race_Asian_basicscaled                    0.0\n",
              "Shap_race_Caucasian_basicscaled          -0.070371\n",
              "Shap_race_Hispanic_basicscaled            0.000804\n",
              "Shap_race_Native American_basicscaled     0.000928\n",
              "Shap_race_Other_basicscaled              -0.001136\n",
              "Name: 3, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Master Dataset\n",
        "META_COL = ['clusters', 'new_clusters']\n",
        "ERROR_COL = ['errors', 'TP', 'TN', 'FN', 'FP']\n",
        "BASIC_COL_unscaled = ['age_unscaled', 'decile_score_unscaled', 'priors_count_unscaled']\n",
        "DUMMY_unscaled = ['sex_Female_unscaled', 'sex_Male_unscaled','race_African-American_unscaled', 'race_Asian_unscaled', 'race_Caucasian_unscaled', 'race_Hispanic_unscaled',\n",
        "                      'race_Native American_unscaled', 'race_Other_unscaled']\n",
        "\n",
        "BASIC_COL_scaled = ['age_scaled', 'decile_score_scaled', 'priors_count_scaled']\n",
        "DUMMY_scaled = ['sex_Female_scaled', 'sex_Male_scaled', 'race_Native American_scaled','race_Other_scaled',\n",
        "                'race_African-American_scaled', 'race_Asian_scaled', 'race_Caucasian_scaled', 'race_Hispanic_scaled']\n",
        "\n",
        "SHAP_COL_Basic_scaled = ['Shap_age_basicscaled', 'Shap_decile_score_basicscaled', 'Shap_priors_count_basicscaled']\n",
        "SHAP_COL_Dummy_scaled = ['Shap_sex_Female_basicscaled', 'Shap_sex_Male_basicscaled',\n",
        "                         'Shap_race_African-American_basicscaled', 'Shap_race_Asian_basicscaled', 'Shap_race_Caucasian_basicscaled',\n",
        "                         'Shap_race_Hispanic_basicscaled', 'Shap_race_Native American_basicscaled', 'Shap_race_Other_basicscaled']\n",
        "#SHAP_META =['clusters', 'new_clusters', 'predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP']\n",
        "\n",
        "#SHAP_COL_Basic_unscaled = ['Shap_age_basicunscaled', 'Shap_decile_score_basicunscaled', 'Shap_priors_count_basicunscaled']\n",
        "#SHAP_COL_Dummy_unscaled = ['Shap_sex_Female_basicunscaled', 'Shap_sex_Male_basicunscaled','Shap_race_African-American_basicunscaled',\n",
        "#'Shap_race_Asian_basicunscaled', 'Shap_race_Caucasian_basicunscaled','Shap_race_Hispanic_basicunscaled', 'Shap_race_Native American_basicunscaled', 'Shap_race_Other_basicunscaled']\n",
        "\n",
        "META_COL_VIZ = ['Error_Type']"
      ],
      "metadata": {
        "id": "Ot8djK6adHLL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP UTILS"
      ],
      "metadata": {
        "id": "3_mds7crD9Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate TPFN & TNFP dataset\n",
        "'''Drop rows where both TP and FN are 0 '''\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "'''Drop rows where both TN and FP are 0'''\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TPFN_all = drop_zero_TP_FN(data_shaper)\n",
        "TNFP_all = drop_zero_TN_FP(data_shaper)\n",
        "\n",
        "#scaled and unscaled version\n",
        "TPFN_scaled = TPFN_all.drop(BASIC_COL_unscaled + DUMMY_unscaled, axis=1)\n",
        "TPFN_unscaled = TPFN_all.drop(BASIC_COL_scaled + DUMMY_scaled, axis=1)\n",
        "TNFP_scaled = TNFP_all.drop(BASIC_COL_unscaled + DUMMY_unscaled, axis=1)\n",
        "TNFP_unscaled = TNFP_all.drop(BASIC_COL_scaled + DUMMY_scaled, axis=1)\n",
        "\n",
        "#TNFP.head()\n",
        "#TPFN_scaled.iloc[3]\n",
        "TNFP_unscaled.iloc[3]"
      ],
      "metadata": {
        "id": "8A9b1ah7dR22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7df5a80-9ef0-4991-aa53-0c87f8fff236"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age_unscaled                                  37.0\n",
              "decile_score_unscaled                          1.0\n",
              "priors_count_unscaled                          0.0\n",
              "sex_Female_unscaled                            1.0\n",
              "sex_Male_unscaled                              0.0\n",
              "race_African-American_unscaled                 1.0\n",
              "race_Asian_unscaled                            0.0\n",
              "race_Caucasian_unscaled                        0.0\n",
              "race_Hispanic_unscaled                         0.0\n",
              "race_Native American_unscaled                  0.0\n",
              "race_Other_unscaled                            0.0\n",
              "errors                                         0.0\n",
              "TP                                             0.0\n",
              "TN                                             1.0\n",
              "FN                                             0.0\n",
              "FP                                             0.0\n",
              "Error_Type                                      TN\n",
              "Shap_age_basicscaled                     -0.057228\n",
              "Shap_decile_score_basicscaled            -0.176166\n",
              "Shap_priors_count_basicscaled            -0.167965\n",
              "Shap_sex_Female_basicscaled               0.004481\n",
              "Shap_sex_Male_basicscaled                 0.003341\n",
              "Shap_race_African-American_basicscaled   -0.007474\n",
              "Shap_race_Asian_basicscaled                    0.0\n",
              "Shap_race_Caucasian_basicscaled          -0.002647\n",
              "Shap_race_Hispanic_basicscaled           -0.000644\n",
              "Shap_race_Native American_basicscaled     0.000713\n",
              "Shap_race_Other_basicscaled               0.002914\n",
              "Name: 5, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop Na's\n",
        "TPFN_scaled = TPFN_scaled.dropna()\n",
        "TPFN_unscaled = TPFN_unscaled.dropna()\n",
        "TNFP_scaled = TNFP_scaled.dropna()\n",
        "TNFP_unscaled = TNFP_unscaled.dropna()"
      ],
      "metadata": {
        "id": "-BLx4V_hwx0F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''New Initialization Function for Dataset. Returns a dataset with scaled features and new columns for clusters = 0 and new_clusters = -1 which is required for HBAC'''\n",
        "\n",
        "def initialize_dataset(data, unscaled =True, meta_col =True, with_classes =True):\n",
        "\n",
        "#1. Make deep copy\n",
        "  new_data = data.copy(deep=True)\n",
        "\n",
        "#2. Ignore cluster and new cluster for scaling and add them back in step 4\n",
        "  if meta_col:\n",
        "    new_data = new_data.drop(columns =META_COL + ['clusters', 'new_clusters'], errors = 'ignore')\n",
        "\n",
        "#3. Scale 1) All features or 2) only SHAP_COL, and ERROR_COL\n",
        "  if unscaled:\n",
        "    features_to_scale = SHAP_COL_Dummy_scaled + SHAP_COL_Basic_scaled + ERROR_COL + BASIC_COL_unscaled + DUMMY_unscaled\n",
        "  else:\n",
        "    features_to_scale = SHAP_COL_Dummy_scaled + SHAP_COL_Basic_scaled + ERROR_COL\n",
        "\n",
        "  new_data[features_to_scale] = StandardScaler().fit_transform(new_data[features_to_scale])\n",
        "\n",
        "#4. add back META_COLUMN\n",
        "  if with_classes: #making sure that the class columns are retained in new dataset\n",
        "    for col in META_COL:\n",
        "      if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "  new_data['clusters'] = 0\n",
        "  new_data['new_clusters'] = -1\n",
        "\n",
        "#5. Return dataset\n",
        "  return new_data\n",
        "\n",
        "#example usage\n",
        "TPFN_init = initialize_dataset(TPFN_unscaled, unscaled = True, meta_col = True, with_classes = True)\n",
        "TNFP_init = initialize_dataset(TNFP_unscaled, unscaled = True, meta_col = True, with_classes = True)\n",
        "#TPFN_scaled2 = initialize_dataset(TPFN_scaled, unscaled = False, meta_col = True, with_classes = True) #will give the same dataset\n",
        "#TNFP_scaled2 = initialize_dataset(TNFP_scaled, unsclaed = False, meta_col = True, with_classes = True) #will give the same dataset\n"
      ],
      "metadata": {
        "id": "hv-wMxciCkAV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TPFN_init.iloc[3]\n",
        "#TNFP_init.iloc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bDAPOW54Mh4",
        "outputId": "49faf880-a8ec-426b-f698-c610504caac3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age_unscaled                              3.291936\n",
              "decile_score_unscaled                    -1.641946\n",
              "priors_count_unscaled                    -0.514283\n",
              "sex_Female_unscaled                      -0.409297\n",
              "sex_Male_unscaled                         0.409297\n",
              "race_African-American_unscaled           -1.177237\n",
              "race_Asian_unscaled                      -0.056183\n",
              "race_Caucasian_unscaled                   1.513227\n",
              "race_Hispanic_unscaled                   -0.267351\n",
              "race_Native American_unscaled            -0.050236\n",
              "race_Other_unscaled                      -0.211441\n",
              "errors                                   -0.856052\n",
              "TP                                        0.856052\n",
              "TN                                             0.0\n",
              "FN                                       -0.856052\n",
              "FP                                             0.0\n",
              "Error_Type                                      TP\n",
              "Shap_age_basicscaled                      0.337445\n",
              "Shap_decile_score_basicscaled             0.561073\n",
              "Shap_priors_count_basicscaled             0.928931\n",
              "Shap_sex_Female_basicscaled               0.425445\n",
              "Shap_sex_Male_basicscaled                 0.700353\n",
              "Shap_race_African-American_basicscaled    0.735779\n",
              "Shap_race_Asian_basicscaled               0.026874\n",
              "Shap_race_Caucasian_basicscaled           1.020727\n",
              "Shap_race_Hispanic_basicscaled            0.060804\n",
              "Shap_race_Native American_basicscaled     0.106724\n",
              "Shap_race_Other_basicscaled              -0.101747\n",
              "clusters                                         0\n",
              "new_clusters                                    -1\n",
              "Name: 9, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize dataset to scale the features and errors which can be in/excluded for clustering.\n",
        "Returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\n",
        "\n",
        "    new_data = data.copy(deep=True).dropna() #remove rows with NaN values\n",
        "\n",
        "    if just_features: #Check if the columns exist before dropping\n",
        "      new_data = new_data.drop(columns=META_COL, errors = 'ignore')\n",
        "      if 'clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('clusters', axis=1)\n",
        "      if 'new_clusters' in new_data.columns:\n",
        "        new_data = new_data.drop('new_clusters', axis=1)\n",
        "\n",
        "    if with_Dummy:\n",
        "      for col in DUMMY_RACE + DUMMY_GENDER:\n",
        "        if col in new_data.columns:\n",
        "          one_hot = pd.get_dummies(new_data[col], prefix=col)\n",
        "          new_data = new_data.drop(col, axis=1) #drop original dummy columns\n",
        "          new_data = pd.concat([new_data, one_hot], axis=1)\n",
        "\n",
        "    #Seperate features before scaling - drop metacols from features for scaling\n",
        "    features = new_data.drop(META_COL, errors='ignore') #? already happened\n",
        "\n",
        "    if scale_features:\n",
        "    #Ensure only numeric columns are scaled + try using minmax and standard scaler\n",
        "      numeric_cols = features.select_dtypes(include=['number']).columns\n",
        "      new_data[numeric_cols] = StandardScaler().fit_transform(features[numeric_cols])\n",
        "      #new_data[numeric_cols] = MinMaxScaler().fit_transform(features[numeric_cols])\n",
        "\n",
        "    if with_errors:\n",
        "        if all(col in new_data.columns for col in ERROR_COL):\n",
        "          new_data[ERROR_COL] *= 0.8 #scaling factor\n",
        "\n",
        "    if with_classes: #making sure that the class columns are retained in new dataset\n",
        "      for col in META_COL:\n",
        "        if col in data.columns:\n",
        "          new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data\n",
        "\n",
        "TPFN = initialize_dataset(TPFN)\n",
        "TPFN.head()\n",
        "'''"
      ],
      "metadata": {
        "id": "Lak8FevYdjnW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "fd992370-00a6-48e7-e415-fd7aae54b008"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Initialize dataset to scale the features and errors which can be in/excluded for clustering.\\nReturns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC \\n\\ndef initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True, with_Dummy= True):\\n\\n    new_data = data.copy(deep=True).dropna() #remove rows with NaN values\\n\\n    if just_features: #Check if the columns exist before dropping\\n      new_data = new_data.drop(columns=META_COL, errors = \\'ignore\\')\\n      if \\'clusters\\' in new_data.columns:\\n        new_data = new_data.drop(\\'clusters\\', axis=1)\\n      if \\'new_clusters\\' in new_data.columns:\\n        new_data = new_data.drop(\\'new_clusters\\', axis=1)\\n\\n    if with_Dummy:\\n      for col in DUMMY_RACE + DUMMY_GENDER:\\n        if col in new_data.columns:\\n          one_hot = pd.get_dummies(new_data[col], prefix=col)\\n          new_data = new_data.drop(col, axis=1) #drop original dummy columns\\n          new_data = pd.concat([new_data, one_hot], axis=1)\\n\\n    #Seperate features before scaling - drop metacols from features for scaling\\n    features = new_data.drop(META_COL, errors=\\'ignore\\') #? already happened\\n\\n    if scale_features:\\n    #Ensure only numeric columns are scaled + try using minmax and standard scaler\\n      numeric_cols = features.select_dtypes(include=[\\'number\\']).columns\\n      new_data[numeric_cols] = StandardScaler().fit_transform(features[numeric_cols])\\n      #new_data[numeric_cols] = MinMaxScaler().fit_transform(features[numeric_cols])\\n\\n    if with_errors:\\n        if all(col in new_data.columns for col in ERROR_COL):\\n          new_data[ERROR_COL] *= 0.8 #scaling factor\\n\\n    if with_classes: #making sure that the class columns are retained in new dataset\\n      for col in META_COL:\\n        if col in data.columns:\\n          new_data[col] = data[col]\\n\\n    new_data[\\'clusters\\'] = 0\\n    new_data[\\'new_clusters\\'] = -1\\n\\n    return new_data\\n\\nTPFN = initialize_dataset(TPFN)\\nTPFN.head()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''undo Dummy for DUMMY_RACE or DUMMY_GENDER'''\n",
        "def undo_dummy(data, with_Dummy, col_label, numeric_values=True, short_label=None):\n",
        "  data[col_label] = ''\n",
        "  for i, c in enumerate(with_Dummy):\n",
        "    values = np.sort(data[c].unique())\n",
        "    if numeric_values:\n",
        "      data.loc[data[c] == values[1], col_label] = i\n",
        "    else:\n",
        "      if short_label is None:\n",
        "        raise ValueError(\"short label must be provided if numeric_values is False\")\n",
        "        data.loc[data[c] == values[1], col_label] = short_label[i]\n",
        "    data = data.drop(c, axis=1)\n",
        "  return(data)\n",
        "\n",
        "#data = undo_dummy(data, DUMMY_RACE, col_label='race', numeric_values=False, short_label=SHORT_LABEL_RACE)\n",
        "#data = undo_dummy(data, DUMMY_GENDER, col_label='gender', numeric_values=False, short_label=SHORT_LABEL_GENDER)"
      ],
      "metadata": {
        "id": "bYnrCZPlw8Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for BIAS in ERROR DIFFERENCE"
      ],
      "metadata": {
        "id": "l3ZumSo56l9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate average Error rate based on unscaled error rate by counting the amount of max values (1) and dividing them by the total nr of rows - replacing old accuracy_error()\n",
        "def get_error_rate(data, column = 'errors'):\n",
        "  if len(data) == 0:\n",
        "    print ('calculating error rate on an empty cluster')\n",
        "    return\n",
        "  max_value = data[column].max()\n",
        "  count_max_value = (data[column] == max_value).sum()\n",
        "  average_error_rate = count_max_value / len(data)\n",
        "  return average_error_rate\n",
        "\n",
        "'''Calculate BIAS in terms of Error Difference\n",
        "bias_type can be 'negative', 'positive' or 'absolute'\n",
        "baseline can be 'all' which is the overall error rate, or 'other' or 'best' '''\n",
        "\n",
        "def get_error_diff(data, cluster_id, cluster_col, bias_type = 'negative', baseline= 'all', #function= get_error_rate\n",
        "                   ):\n",
        "  cluster_x= data.loc[data[cluster_col] == cluster_id]\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "\n",
        "  if len(cluster_x) == 0:\n",
        "    print ('calculating error difference on an empty cluster')\n",
        "    return\n",
        "\n",
        "  if baseline == 'all':\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(data)\n",
        "\n",
        "  elif baseline == 'other':\n",
        "    if len(remaining_clusters) == 0:\n",
        "      print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "      return\n",
        "    error_diff = get_error_rate(cluster_x) - get_error_rate(remaining_clusters)\n",
        "\n",
        "  elif baseline == 'best':\n",
        "    best_cluster = get_min_bias_cluster(data, cluster_col, bias_type, baseline)\n",
        "    error_diff = get_error_rate(cluster_x) - best_cluster[1]\n",
        "\n",
        "  else:\n",
        "    print ('unknown baseline')\n",
        "    return\n",
        "\n",
        "  #if full_info:\n",
        "    #return [error_diff, function(cluster_x), function(remaining_clusters)]\n",
        "\n",
        "  if bias_type == 'negative':\n",
        "    pass #no change needed\n",
        "  elif bias_type == 'positive':\n",
        "    error_diff = -error_diff\n",
        "  elif bias_type == 'absolute':\n",
        "    error_diff = np.absolute(error_diff)\n",
        "  else:\n",
        "    print(\"unknown bias type\")\n",
        "    return\n",
        "\n",
        "  return error_diff"
      ],
      "metadata": {
        "id": "PhGil8f663vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS for VISUALS"
      ],
      "metadata": {
        "id": "w1MYjWK6bkSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop Meta_colums\n",
        "    pca_features = data.drop(META_COL + ERROR_COL, axis=1)\n",
        "    other_features = data[META_COL + ERROR_COL]\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['PC1', 'PC2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='PC1', y='PC2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def tsne_plot(data, title, perplexity, learning_rate, n_iter, alpha = 0.5):\n",
        "    # Extract features for TSNE and drop Meta_colums\n",
        "    tsne_features = data.drop(META_COL + ERROR_COL, axis=1)\n",
        "    other_features = data[META_COL + ERROR_COL]\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity= 30, learning_rate= 200, n_iter= 1000)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = tsne_features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HoSLp22kbykA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bj-kyT-ldUwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FSE_tpfn = initialize_dataset(TPFN_data)\n",
        "#pca_plot(TP_FN, 'Compas', 0.6)\n",
        "tsne_plot(FSE_tpfn, 'Compas', 30, 200, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BhU6II4-2mWM",
        "outputId": "6b2b0ab3-4851-4d21-bff5-3d01f24b6512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['TP', 'TN', 'FN', 'FP'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bfe2a9e1da42>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFSE_tpfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPFN_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pca_plot(TP_FN, 'Compas', 0.6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtsne_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFSE_tpfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Compas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-33acdddcb74a>\u001b[0m in \u001b[0;36mtsne_plot\u001b[0;34m(data, title, perplexity, learning_rate, n_iter, alpha)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtsne_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Extract features for TSNE and drop Meta_colums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtsne_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETA_COL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mERRORS_COL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mother_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMETA_COL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mERRORS_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5256\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m         \"\"\"\n\u001b[0;32m-> 5258\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5259\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5260\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4590\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4591\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['TP', 'TN', 'FN', 'FP'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FSE_tnfp = initialize_dataset(TNFP_data)\n",
        "#pca_plot(TN_FP, 'Compas', 0.6)\n",
        "tsne_plot(FSE_tnfp, 'Compas', 30, 200, 1000)"
      ],
      "metadata": {
        "id": "JngeycNY2rrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS FOR CLUSTERING"
      ],
      "metadata": {
        "id": "URIWoUpSfBLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get cluster with max error difference\n",
        "def get_max_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all', function = get_error_diff):\n",
        "  max_bias = 0 #min possible bias\n",
        "  max_bias_cluster = -2\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "\n",
        "    current_bias = function(data, cluster_id, cluster_col, bias_type, baseline) #pos function to find highest bias\n",
        "\n",
        "    #if isinstance(current_bias, list):\n",
        "     # current_bias = current_bias[0]\n",
        "\n",
        "    if current_bias > max_bias:\n",
        "      max_bias = current_bias\n",
        "      max_bias_cluster = cluster_id\n",
        "\n",
        "    return(max_bias_cluster, max_bias)\n",
        "\n",
        "#get cluster with min error difference\n",
        "def get_min_bias_cluster(data, cluster_col= 'clusters', bias_type = 'negative', baseline = 'all'):\n",
        "  min_bias = 1 #max possible bias and look for smt smaller\n",
        "  min_bias_cluster = -2\n",
        "\n",
        "  for cluster_id in data[cluster_col].unique():\n",
        "    if cluster_id == -1: #outliers in dbscan\n",
        "      continue\n",
        "    current_bias = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline)\n",
        "    if current_bias < min_bias:\n",
        "      min_bias = current_bias\n",
        "      min_bias_cluster = cluster_id\n",
        "  return(min_bias_cluster, min_bias)\n",
        "\n",
        "#get size of the smallest cluster\n",
        "def get_min_cluster_size(data, cluster_col = 'new_clusters'):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1: #exclude the -1 clusters as they may present outliers (in dbscan?)\n",
        "      continue\n",
        "      size = len(data.loc[data['new_clusters'] == i])\n",
        "      if size < min_cluster_size: #update if new cluster size is smaller\n",
        "        min_cluster_size = size\n",
        "  return(min_cluster_size)\n",
        "\n",
        "def get_random_cluster(data, cluster_col, min_splittable_cluster_size, previous_cluster, all_cluster_ids):\n",
        "  for candidate_cluster_id in all_cluster_ids:\n",
        "    if candidate_cluster_id == -1 or candidate_cluster_id == previous_cluster:\n",
        "      continue\n",
        "      print ('This is the random cluster we picked:', candidate_cluster_id)\n",
        "\n",
        "      candidate_cluster = data.loc[data[cluster_col] == candidate_cluster_id]\n",
        "      if len(candidate_cluster) >= min_splittable_cluster_size:\n",
        "        print('it is too small:', len(candidate_cluster))\n",
        "        continue\n",
        "      else:\n",
        "        return candidate_cluster_id\n",
        "\n",
        "def select_new_cluster(data, cluster_col='clusters', error_column='errors', overall_error_rate=0.5, bias_type='negative', baseline='all'):\n",
        "    smallest_diff = float('inf')\n",
        "    selected_cluster = None\n",
        "\n",
        "    for cluster_id in data[cluster_col].unique():\n",
        "        if cluster_id == -1: #skip outlier\n",
        "            continue\n",
        "\n",
        "        error_diff = get_error_diff(data, cluster_id, cluster_col, bias_type, baseline) #calculate the error_diff for each cluster\n",
        "\n",
        "        if error_diff is None:\n",
        "            continue\n",
        "\n",
        "        abs_diff = abs(overall_error_rate - (get_error_rate(data[data[cluster_col] == cluster_id]))) #get cluster with the smallest absolute difference with the overall error rate (0.5)\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_id\n",
        "    return selected_cluster\n",
        "\n",
        "def exit_clustering(data, msg='', bias_type='', iter=''):\n",
        "  print('Iteration ', iter, ': ', msg)\n",
        "  print('Overall error rate: ', get_error_rate(data))\n",
        "  for c in np.sort(data['clusters'].unique()):\n",
        "    print('Cluster: ', c, '\\tSize: ', len(data.loc[data['clusters'] == c]), '\\tError rate: ', get_error_rate(data.loc[data['clusters'] == c]))\n",
        "  pca_plot(data,'HBAC-DBSCAN on COMPAS - ' + bias_type + ' bias', hue='clusters', s=15, alpha=0.8)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "zApFFYGVfERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-MEANS CLUSTERING"
      ],
      "metadata": {
        "id": "NAD_kfGCfFN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TP FN DATA"
      ],
      "metadata": {
        "id": "4bYXg1id6kTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FSE_tpfn\n",
        "def hbac_kmeans(data = FSE_tpfn, max_iter = 300, show_plot= True):\n",
        "  clus_model_kwargs = { \"n_clusters\": 2, #split in two clusters\n",
        "    \"init\": \"k-means++\", # method for initializing k-means++: first centroid is chosen randomly and subsequent centriods are selected based on max distance from the nearest centriod\n",
        "    \"n_init\": 10, #K-means is sensitive to the initial placement of cluster centers - running it 10 times with different initial seeds\n",
        "    \"max_iter\": max_iter,} #max mr of iterations for k-means in a single run. If convergence is not achieved within 300 the algorithm stops\n",
        "\n",
        "  x = 0 #initial cluster nr\n",
        "  initial_bias = 0\n",
        "\n",
        "  error_list = []\n",
        "  error_rate = get_error_rate(data, column = 'FN') #Calculating mean error rate on FN column\n",
        "\n",
        "  min_splittable_cluster_size = round(0.05 * len(data))\n",
        "  min_acceptable_cluster_size = round(0.03 * len(data))\n",
        "  print(\"error rate:\", error_rate)\n",
        "\n",
        "  #Loop for clustering iterations\n",
        "  for i in range(1, max_iter):\n",
        "    if len(data['clusters'].unique()) != 1:\n",
        "      error_list.append(get_error_rate(data)) #the error rate is calculated if the nr of unique clusters is not equal to 1\n",
        "      data['new_clusters'] = -1\n",
        "    candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "    if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "      x = get_random_cluster(data, 'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "      continue\n",
        "\n",
        "    kmeans = KMeans(**clus_model_kwargs).fit(candidate_cluster.drop(META_COL,axis=1))\n",
        "\n",
        "    candidate_cluster['new_clusters'] = pd.DataFrame(kmeans.predict(candidate_cluster.drop(META_COL, axis=1)), index=candidate_cluster.index)\n",
        "    data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "    discr_bias = get_error_diff(data, x, 'clusters', bias_type = 'negative', baseline= 'all')\n",
        "    print('discriminative bias:', discr_bias)\n",
        "\n",
        "    min_cluster_size = get_min_cluster_size(data)\n",
        "    print('Smallest cluster size:', min_cluster_size)\n",
        "\n",
        "    if (discr_bias >= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "      print (\"adding a new cluster\")\n",
        "      n_cluster = max(data['clusters'])\n",
        "      data['clusters'][data['new_clusters'] == 1] = n_cluster + 1\n",
        "\n",
        "      if show_plot:\n",
        "        #pca_plot(data, 'K-means for False Negatives', 0.6)\n",
        "        tsne_plot(data, 'K-means for False Negatives', perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5)\n",
        "        plt.show()\n",
        "\n",
        "      x = select_new_cluster(data, error_column = 'FN')\n",
        "      initial_bias = discr_bias\n",
        "\n",
        "    else:\n",
        "      x = get_random_cluster(data,'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "\n",
        "  print('MAX_ITER')\n",
        "  print(error_list)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "uZpJmkcq1u2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = FSE_tpfn, max_iter = 300, show_plot= False) #True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4L496pDztCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c= get_max_bias_cluster(FSE_tpfn)\n",
        "#highest_bias_cluster = TP_FN[TP_FN['clusters']==c]\n",
        "#len(highest_bias_cluster)\n",
        "\n",
        "Mean_error_rate_TPFN = get_error_rate(FSE_tpfn)\n",
        "\n",
        "print ('Mean error rate of full TPFN data set:', Mean_error_rate_TPFN)\n",
        "\n",
        "print (f\" cluster {c} has the highest discrimination bias for TPFN data\")\n"
      ],
      "metadata": {
        "id": "WyZlLl7Ku6iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TN FP DATA"
      ],
      "metadata": {
        "id": "gR-UqBi_6g-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''TN FP DATA'''\n",
        "def hbac_kmeans(data = FSE_tnfp, max_iter = 300, show_plot= True):\n",
        "  clus_model_kwargs = { \"n_clusters\": 2, \"init\": \"k-means++\", \"n_init\": 10, \"max_iter\": max_iter,}\n",
        "\n",
        "  x = 0 #initial cluster nr\n",
        "  initial_bias = 0\n",
        "\n",
        "  error_list = []\n",
        "  error_rate = get_error_rate(data, column = 'FP') #Calculating mean error rate on FP column\n",
        "\n",
        "  min_splittable_cluster_size = round(0.05 * len(data))\n",
        "  min_acceptable_cluster_size = round(0.03 * len(data))\n",
        "  print(\"error rate:\", error_rate)\n",
        "\n",
        "  #Loop for clustering iterations\n",
        "  for i in range(1, max_iter):\n",
        "    if len(data['clusters'].unique()) != 1:\n",
        "      error_list.append(get_error_rate(data)) #the error rate is calculated if the nr of unique clusters is not equal to 1\n",
        "      data['new_clusters'] = -1\n",
        "    candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "    if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "      x = get_random_cluster(data, 'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "      continue\n",
        "\n",
        "    kmeans = KMeans(**clus_model_kwargs).fit(candidate_cluster.drop(META_COL,axis=1))\n",
        "\n",
        "    candidate_cluster['new_clusters'] = pd.DataFrame(kmeans.predict(candidate_cluster.drop(META_COL, axis=1)), index=candidate_cluster.index)\n",
        "    data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "    discr_bias = get_error_diff(data, x, 'clusters', bias_type = 'negative', baseline= 'all')\n",
        "    print('discriminative bias:', discr_bias)\n",
        "\n",
        "    min_cluster_size = get_min_cluster_size(data)\n",
        "    print('Smallest cluster size:', min_cluster_size)\n",
        "\n",
        "    if (discr_bias >= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "      print (\"adding a new cluster\")\n",
        "      n_cluster = max(data['clusters'])\n",
        "      data['clusters'][data['new_clusters'] == 1] = n_cluster + 1\n",
        "\n",
        "      if show_plot:\n",
        "        #pca_plot(data, 'K-means for False Positives', 0.6)\n",
        "        tsne_plot(data, 'K-means for False Negatives', perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5)\n",
        "        plt.show()\n",
        "\n",
        "      x = select_new_cluster(data, error_column = 'FP')\n",
        "      initial_bias = discr_bias\n",
        "\n",
        "    else:\n",
        "      x = get_random_cluster(data,'clusters', min_splittable_cluster_size, x, data['clusters'].unique())\n",
        "\n",
        "  print('MAX_ITER')\n",
        "  print(error_list)\n",
        "  return data"
      ],
      "metadata": {
        "id": "FA_cQ1j06QdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = TN_FP, max_iter = 300, show_plot= False) #True)"
      ],
      "metadata": {
        "id": "H8aEiMm87QRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c= get_max_bias_cluster(TN_FP)\n",
        "\n",
        "Mean_error_rate_TNFP = get_error_rate(TN_FP)\n",
        "\n",
        "print ('Mean error rate of full TNFP data set:', Mean_error_rate_TNFP)\n",
        "\n",
        "print (f\" cluster {c} has the highest discrimination bias for TNFP data\")\n"
      ],
      "metadata": {
        "id": "niZ4E-xj8JS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOVA SIGNIFICANCE TESTING"
      ],
      "metadata": {
        "id": "FK_n1GA3fKgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP, ERROR & BASIC COLUMNS FOR TPFN ANOVA'''\n",
        "groupstpfn = TP_FN.groupby('clusters')['FN'].apply(list)\n",
        "anovatpfn = [np.array(groupstpfn) for groupstpfn in groupstpfn]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovatpfn)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "H5eq8K21umRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP, ERROR & BASIC COLUMNS FOR TNFP ANOVA'''\n",
        "groupstnfp = TN_FP.groupby('clusters')['FP'].apply(list)\n",
        "anovatnfp = [np.array(groupstnfp) for groupstnfp in groupstnfp]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovatnfp)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "KPAFNlzT_v8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XhKl7z5mfODS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP AND ERROR NO FEATURES"
      ],
      "metadata": {
        "id": "j65th8V0pKoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**RQ1: is there a relationshop between shap & risk of error?**\n",
        "\n",
        "TP_FN & TN_FP = *all columns* (above)\n",
        "\n",
        "SE = shap and error columns\n",
        "\n",
        "FE = all columns except shap (selma)\n",
        "\n",
        "\n",
        "--------------------------------------------------\n",
        "**RQ2: does shap help in finding disc clusters?**\n",
        "\n",
        "FS = all columns except error\n",
        "\n",
        "S = only SHAP_COL\n",
        "\n",
        "F = only BASIC_COL\n"
      ],
      "metadata": {
        "id": "YxxgBsfJ69iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SE DF: Shap and Error '''\n",
        "SE_tpfn_ = TPFN_data[META_COL + SHAP_COL]\n",
        "SE_tnfp_ = TNFP_data[META_COL + SHAP_COL]\n",
        "#SE_tnfp_.drop('Error_Type', axis = 1)\n",
        "\n",
        "SE_tpfn = initialize_dataset(SE_tpfn_)\n",
        "SE_tnfp = initialize_dataset(SE_tnfp_)\n",
        "\n",
        "SE_tpfn.info()\n",
        "SE_tnfp.info()"
      ],
      "metadata": {
        "id": "USVopU1opVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hbac_kmeans(data = SE_tnfp, max_iter = 300, show_plot= False)\n",
        "hbac_kmeans(data = SE_tpfn, max_iter = 300, show_plot= False)"
      ],
      "metadata": {
        "id": "OeIa2jdkqu52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP & ERROR COLUMNS FOR TPFN ANOVA'''\n",
        "groupsSE_tpfn = SE_tpfn.groupby('clusters')['FN'].apply(list)\n",
        "anovaSE_tpfn = [np.array(groupsSE_tpfn) for groupsSE_tpfn in groupsSE_tpfn]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovaSE_tpfn)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "UQVR_YQhrTXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SHAP & ERROR COLUMNS FOR TNFP ANOVA'''\n",
        "groupsSE_tnfp = SE_tnfp.groupby('clusters')['FP'].apply(list)\n",
        "anovaSE_tnfp = [np.array(groupsSE_tnfp) for groupsSE_tnfp in groupsSE_tnfp]\n",
        "\n",
        "f_stat, p_val = f_oneway(*anovaSE_tnfp)\n",
        "\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "  print(\"there are statistically significant differences between the clusters.\")\n",
        "else:\n",
        "  print(\"there are no statistically significant differences between the clusters\")"
      ],
      "metadata": {
        "id": "1MO0-G5LTl74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ERROR AND FEATURES NO SHAP"
      ],
      "metadata": {
        "id": "mw7ZAR6tyVy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''FE_df: FEATURES AND ERROR NO SHAP (Selma) '''\n",
        "\n",
        "FE_tpfn_ = TPFN_data.drop(SHAP_COL, axis = 1) #[BASIC_COL + META_COL + DUMMY_RACE + DUMMY_GENDER]\n",
        "FE_tnfp_ = TNFP_data.drop(SHAP_COL, axis = 1) #[BASIC_COL + META_COL + DUMMY_RACE + DUMMY_GENDER]\n",
        "\n",
        "FE_tpfn = initialize_dataset(FE_tpfn_)\n",
        "FE_tnfp = initialize_dataset(FE_tnfp_)\n",
        "\n",
        "#FE_tpfn_.info()\n",
        "FE_tpfn.info()\n",
        "#FE_tnfp.info()\n",
        "#FE_tnfp_.info()"
      ],
      "metadata": {
        "id": "AsgNawceyLEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}