{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvZbK5PewHtf/KgeN5B+eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCLy0OtAU6-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import DBSCAN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SHAP_ERROR_DATA"
      ],
      "metadata": {
        "id": "95UadjU1VNpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shap_error_data = pd.read_csv('/content/Shap_error_data.csv')\n",
        "#Shap_error_data.info()\n",
        "features = Shap_error_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)"
      ],
      "metadata": {
        "id": "pIwbBWIeVLEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Shap_error_data.isna().sum())\n",
        "len(Shap_error_data.index)"
      ],
      "metadata": {
        "id": "4kLbqu6yYSYW",
        "outputId": "0d3fb1ea-9ffe-4773-d484-571e6706ee7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                              0\n",
            "priors_count                     0\n",
            "sex_Female                       0\n",
            "sex_Male                         0\n",
            "race_African-American            0\n",
            "race_Asian                       0\n",
            "race_Caucasian                   0\n",
            "race_Hispanic                    0\n",
            "race_Native American             0\n",
            "race_Other                       0\n",
            "Shap_age                         0\n",
            "Shap_priors_count                0\n",
            "Shap_sex_Female                  0\n",
            "Shap_sex_Male                    0\n",
            "Shap_race_African-American       0\n",
            "Shap_race_Asian                  0\n",
            "Shap_race_Caucasian              0\n",
            "Shap_race_Hispanic               0\n",
            "Shap_race_Native American        0\n",
            "Shap_race_Other                  0\n",
            "predicted_class               1508\n",
            "true_class                    1508\n",
            "errors                        1508\n",
            "TP                            1508\n",
            "TN                            1508\n",
            "FN                            1508\n",
            "FP                            1508\n",
            "Error_Type                    1508\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing: Initialize / scaling dataset"
      ],
      "metadata": {
        "id": "idrXNeKQDvkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_dataset (raw_data, with_errors = True, just_features = True, scale_features = True, with_classes = True):\n",
        "  #input is the original dataset, whether errors are included, only features should be used, to scale the features, class labels are included\n",
        "  #Initialisation of the dataset. Scales the features and errors, which can be included or exluded for clustering\n",
        "  #it returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC\n",
        "\n",
        "  new_data = raw_data.copy(deep=True)\n",
        "\n",
        "  if with_errors:\n",
        "    scaling_factor = 0.8  # default scaling factor - there is a trade-off between scaling of weighing the errors to guide biased clusters while preventing too large and uninformative clusters\n",
        "    new_data['scaled_TP'] = new_data['TP'] * scaling_factor\n",
        "    new_data['scaled_TN'] = new_data['TN'] * scaling_factor\n",
        "    new_data['scaled_FN'] = new_data['FN'] * scaling_factor\n",
        "    new_data['scaled_FP'] = new_data['FP'] * scaling_factor\n",
        "\n",
        "  if just_features:\n",
        "    new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)\n",
        "\n",
        "  if scale_features:\n",
        "    to_scale = raw_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1).columns\n",
        "    new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "\n",
        "  if with_classes:\n",
        "    for col in ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP']:\n",
        "      new_data[col] = raw_data[col]\n",
        "\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "dW-4moGZDy8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "needs to be separated still for TN/FP and TP/FN"
      ],
      "metadata": {
        "id": "lXLzp-j9Vtcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "\n",
        "    #extract features for PCA and drop the other columns in other_features df\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type']]\n",
        "\n",
        "    # Scale the PCA features before using PCA\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(pca_features)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca_result = PCA(n_components=2).fit_transform(scaled_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index)\n",
        "\n",
        "    #create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "    temp_dataset.rename(columns={0: 'Principal Component 1', 1: 'Principal Component 2'}, inplace=True)\n",
        "\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset,x ='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"Error_Type\", palette=\"tab10\", style=\"Error_Type\")\n",
        "\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = PCA(n_components=2).fit(scaled_features).explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RMvUUByXVtBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Functions"
      ],
      "metadata": {
        "id": "1_3QOzE2Cgw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate error metric\n",
        "\n",
        "#Calculate bias\n",
        "\n",
        "#Calculate absolute difference\n",
        "\n",
        "#Calculate variance\n",
        "\n",
        "#Get min splittable cluster size\n",
        "\n",
        "##########################################\n",
        "\n",
        "#Select a new cluster\n",
        "#Returns cluster with highest absolute difference for the False positives (FP) and TN\n",
        "#Cluster with highest absolute difference with 0.5 is selected as splitting cluster.\n",
        "#Function requires a df.columns named 'clusters' and 'FP'\n",
        "def select_new_cluster(data, FP, TN):\n",
        "  highest_variance = -1\n",
        "  cluster_number = 0\n",
        "\n",
        "  for cluster_number in data['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    cluster_data = data[data['clusters'] == cluster_number]\n",
        "    #FP and TN instead of error column\n",
        "    mean_fp = cluster_data(['FP']).mean()\n",
        "    mean_tn = cluster_data(['TN']).mean()\n",
        "    mean_diff = abs(0.5 - mean_fp)\n",
        "\n",
        "    if mean_diff > highest_variance:\n",
        "      highest_variance = mean_diff\n",
        "      selected_cluster = cluster_number\n",
        "\n",
        "    return selected_cluster\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Select a random cluster\n",
        "#returns a random cluster from provided list of clusters that is not -1\n",
        "def get_random_cluster(clusters):\n",
        "  result = -1\n",
        "  while (result == -1):\n",
        "    result = random.randint(0, len(clusters.unique()))\n",
        "  print('This is the random cluster we picked:', result)\n",
        "  return result\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Plot cluster\n",
        "def plot_clusters(data):\n",
        "  scatterplot = sns.scatterplot(data=data, x='1st', y='2nd', hue=\"clusters\", size = 'errors', sizes=(100, 20), palette = \"tab10\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "yKMXs1SpCgTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bias_with_error_rate(full_data, input_columns, error_column, max_iter=10, min_splittable_cluster_size=10, dbscan_max_iter=100):\n",
        "#Cluster the data based on the FP or FN for error column and selects the best cluster iteratively.\n",
        "  for i in range(1, max_iter):\n",
        ""
      ],
      "metadata": {
        "id": "jdlKRBSXF5-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}