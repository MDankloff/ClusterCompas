{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lXLzp-j9Vtcs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rCLy0OtAU6-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "#from sklearn import DBSCAN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SHAP_ERROR_DATA"
      ],
      "metadata": {
        "id": "95UadjU1VNpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shap_error_data = pd.read_csv('/content/Shap_error_data.csv')\n",
        "#Shap_error_data.info()\n",
        "features = Shap_error_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)"
      ],
      "metadata": {
        "id": "pIwbBWIeVLEM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Shap_error_data.loc[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgwWGJUvOs8O",
        "outputId": "727e71b0-2af0-442a-b276-083a1865ecf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                          -0.746845\n",
            "priors_count                  0.727079\n",
            "sex_Female                   -0.494734\n",
            "sex_Male                      0.494734\n",
            "race_African-American        -1.019194\n",
            "race_Asian                   -0.067641\n",
            "race_Caucasian                 1.39634\n",
            "race_Hispanic                -0.313915\n",
            "race_Native American         -0.052726\n",
            "race_Other                   -0.239983\n",
            "Shap_age                       0.08955\n",
            "Shap_priors_count            -0.666958\n",
            "Shap_sex_Female               0.090624\n",
            "Shap_sex_Male                  0.19573\n",
            "Shap_race_African-American    1.053784\n",
            "Shap_race_Asian               0.069726\n",
            "Shap_race_Caucasian           1.444355\n",
            "Shap_race_Hispanic             0.20207\n",
            "Shap_race_Native American     0.035609\n",
            "Shap_race_Other               0.192882\n",
            "predicted_class                    1.0\n",
            "true_class                         0.0\n",
            "errors                             1.0\n",
            "TP                                 0.0\n",
            "TN                                 0.0\n",
            "FN                                 0.0\n",
            "FP                                 1.0\n",
            "Error_Type                          FP\n",
            "Name: 3, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(Shap_error_data.isna().sum())\n",
        "len(Shap_error_data.index)"
      ],
      "metadata": {
        "id": "4kLbqu6yYSYW",
        "outputId": "d1e986c8-c641-47c3-9386-56694f6301f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing: Initialize / scaling dataset"
      ],
      "metadata": {
        "id": "idrXNeKQDvkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_dataset (raw_data, with_errors = True, just_features = True, scale_features = True, with_classes = True):\n",
        "  #input is the original dataset, whether errors are included, only features should be used, to scale the features, class labels are included\n",
        "  #Initialisation of the dataset. Scales the features and errors, which can be included or exluded for clustering\n",
        "  #it returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC\n",
        "\n",
        "  new_data = raw_data.copy(deep=True)\n",
        "\n",
        "  if with_errors:\n",
        "    scaling_factor = 0.8  # default scaling factor - there is a trade-off between scaling of weighing the errors to guide biased clusters while preventing too large and uninformative clusters\n",
        "    new_data['scaled_TP'] = new_data['TP'] * scaling_factor\n",
        "    new_data['scaled_TN'] = new_data['TN'] * scaling_factor\n",
        "    new_data['scaled_FN'] = new_data['FN'] * scaling_factor\n",
        "    new_data['scaled_FP'] = new_data['FP'] * scaling_factor\n",
        "\n",
        "  if just_features:\n",
        "    new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)\n",
        "\n",
        "  if scale_features:\n",
        "    to_scale = raw_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1).columns\n",
        "    new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "\n",
        "  if with_classes:\n",
        "    for col in ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP']:\n",
        "      new_data[col] = raw_data[col]\n",
        "\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "dW-4moGZDy8P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering on either TP/FN or TN/FP --> drop 0,0 columns"
      ],
      "metadata": {
        "id": "NADVrTXqIG_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Drop rows where both TP and FN are zero\n",
        "def drop_zero_TP_FN(data):\n",
        "  return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "TP_FN_data = drop_zero_TP_FN(Shap_error_data)\n",
        "\n",
        "################################################\n",
        "\n",
        "#Drop rows where both TN and FP are zero\n",
        "def drop_zero_TN_FP(data):\n",
        "  return data.loc[(data['TN']  == 1 ) | (data['FP'] == 1)]\n",
        "\n",
        "TN_FP_data = drop_zero_TN_FP(Shap_error_data)\n",
        "\n",
        "################################################\n",
        "if ((TP_FN_data['TP'] == 0) & (TP_FN_data['FN'] == 0)).any():\n",
        "    print(\"There's at least one occurrence of TP 0 and FN 0 in the dataset.\")\n",
        "else:\n",
        "    print(\"There's no occurrence of 0,0 in both columns TP and FN.\")\n",
        "\n",
        "if ((TN_FP_data['TN'] == 0) & (TN_FP_data['FP'] == 0) ).any():\n",
        "    print(\"There's at least one occurrence of TP 0 and FN 0 in the dataset.\")\n",
        "else:\n",
        "    print(\"There's no occurrence of 0,0 in both columns TN and FP.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rll1BfIztumu",
        "outputId": "b388110a-345b-4f73-a55d-95b9c313340b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's no occurrence of 0,0 in both columns TP and FN.\n",
            "There's no occurrence of 0,0 in both columns TN and FP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(TP_FN_data)\n",
        "#len(TN_FP_data)\n",
        "#len(Shap_error_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUuUKoIg6WYL",
        "outputId": "dc95605a-9fdc-487d-a085-aff241bc5163"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1579"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "needs to be separated still for TN/FP and TP/FN"
      ],
      "metadata": {
        "id": "lXLzp-j9Vtcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "\n",
        "    #extract features for PCA and drop the other columns in other_features df\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type']]\n",
        "\n",
        "    # Scale the PCA features before using PCA\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(pca_features)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca_result = PCA(n_components=2).fit_transform(scaled_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index)\n",
        "\n",
        "    #create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "    temp_dataset.rename(columns={0: 'Principal Component 1', 1: 'Principal Component 2'}, inplace=True)\n",
        "\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset,x ='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"Error_Type\", palette=\"tab10\", style=\"Error_Type\")\n",
        "\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = PCA(n_components=2).fit(scaled_features).explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RMvUUByXVtBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering Functions"
      ],
      "metadata": {
        "id": "1_3QOzE2Cgw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate accuracy on error\n",
        "def accuracy_error (results):\n",
        "  if len(results) == 0:\n",
        "    print (\"you are calculating the accuracy on an empty cluster\")\n",
        "  correct = results.loc[results['error'] == 0]\n",
        "  acc = len(correct)/ len(results)\n",
        "  return acc\n",
        "\n",
        "#################################\n",
        "\n",
        "#Calculate bias based on accuracy_error function\n",
        "\n",
        "'''Calculate bias: negative bias as the accuracy of a selected cluster - accruacy of the remaining clusters\n",
        " Colster col: name of the DF column where the cluster assignments are'''\n",
        "\n",
        "def bias_w_error (data, cluster_id, cluster_col):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  if len(cluster_x) ==0:\n",
        "    print(\"this is an empty cluster\", cluster_id)\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "  if len(remaining_clusters) ==0:\n",
        "    print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "  return accuracy_error(cluster_x) - accuracy_error(remaining_clusters)\n",
        "\n",
        "#################################\n",
        "\n",
        "#Combined function for get_max_neg_bias and get_max_pos_bias\n",
        "\n",
        "def get_max_bias(fulldata, bias_type = 'negative', function = bias_w_error):\n",
        "  max_bias = float('inf') if bias_type == 'negative' else -float('inf') #initializes max_bias with either positive of negative infinity (special floating point value) based on bias_type param\n",
        "  for cluster_number in fulldata['new_clusters'].unique():\n",
        "    if cluster_number == -1:#outliers in dbscan\n",
        "      continue\n",
        "    current_bias = function(fulldata, cluster_number, 'new_clusters') #for each cluster the bias_w_error function is calculated\n",
        "    if (bias_type == 'negative' and current_bias < max_bias) or (bias_type == 'positive' and current_bias > max_bias):\n",
        "      max_bias = current_bias\n",
        "  print(f'Maximum {bias_type} bias is:', max_bias)\n",
        "  return max_bias\n",
        "\n",
        "\n",
        "###################################\n",
        "#get max bias cluster --> returns a cluster with neg bias (for newly added clusters)\n",
        "\n",
        "def get_cluster_max_bias(fulldata, function = bias_w_error):\n",
        "  max_pos_bias = 100 #max_abs bias selma code\n",
        "  max_bias_cluster = -2\n",
        "  for cluster_number in fulldata['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    current_bias = (function(fulldata, cluster_number, 'clusters')) #pos function to find the highest bias\n",
        "    print(f\"{cluster_number} has bias {current_bias}\")\n",
        "    if current_bias < max_pos_bias:\n",
        "      max_pos_bias = current_bias\n",
        "      max_bias_cluster = cluster_number\n",
        "  print ('cluster with the highest discriminating bias:', max_bias_cluster)\n",
        "  return max_bias_cluster\n",
        "\n",
        "#################################\n",
        "\n",
        "#Select a new cluster to split on based on the smallest absolute difference from the overall error rate of 0.5\n",
        "#Function requires a df.columns named 'clusters' and 'FP'\n",
        "\n",
        "def select_new_cluster(data, error_column='FP', overall_error_rate=0.5):\n",
        "  smallest_diff = 1\n",
        "  cluster_number = 0\n",
        "\n",
        "  for cluster_number in data['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    cluster_data = data[data['clusters'] == cluster_number]\n",
        "    cluster_error_rate = cluster_data([error_column]).mean()\n",
        "    abs_diff = abs(overall_error_rate - cluster_error_rate)\n",
        "\n",
        "    if abs_diff < smallest_diff:\n",
        "      smallest_diff = abs_diff\n",
        "      selected_cluster = cluster_number\n",
        "\n",
        "  return selected_cluster\n",
        "\n",
        "#############################################\n",
        "#Calculate variance\n",
        "\n",
        "def calculate_variance(data):\n",
        "  variance_list_local = []\n",
        "  for j in data['clusters'].unique():\n",
        "    average_acc = accuracy_error(data)\n",
        "    bias_clus = bias_w_error(data, j, 'clusters')\n",
        "    variance_list_local.append(bias_clus)\n",
        "  variance = np.variance(variance_list_local)\n",
        "  return variance\n",
        "\n",
        "#calculate bias_acc_global\n",
        "\n",
        "def calculate_bias_global_average(data, cluster_id, cluster_col, ave_acc):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  return accuracy_error(cluster_x) - ave_acc\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Get min splittable cluster size - returns size of smallest new cluster\n",
        "def min_split_cluster_size(data):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1:\n",
        "      continue\n",
        "    size = len(data.loc[data['new_clusters']==i])\n",
        "    if size < min_cluster_size:\n",
        "      min_cluster_size = size\n",
        "  return min_cluster_size\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "#Select a random cluster from provided list of clusters that is not -1\n",
        "def get_random_cluster(clusters):\n",
        "  result = -1\n",
        "  while (result == -1):\n",
        "    result = random.randint(0, len(clusters.unique()))\n",
        "  print('This is the random cluster we picked:', result)\n",
        "  return result\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Plot cluster\n",
        "def plot_clusters(data):\n",
        "  scatterplot = sns.scatterplot(data=data, x='1st', y='2nd', hue=\"clusters\", size = 'errors', sizes=(100, 20), palette = \"tab10\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "yKMXs1SpCgTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bias_with_error_rate(full_data, input_columns, error_column = 'FP', max_iter=10, min_splittable_cluster_size=10, dbscan_max_iter=100):\n",
        "#Cluster the data based on the FP or FN for error column and selects the best cluster iteratively.\n",
        "  for i in range(1, max_iter):\n"
      ],
      "metadata": {
        "id": "jdlKRBSXF5-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}