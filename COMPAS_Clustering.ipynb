{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7BdHfVWvqsKJV/cWXIaJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rCLy0OtAU6-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SHAP_ERROR_DATA"
      ],
      "metadata": {
        "id": "95UadjU1VNpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shap_error_data = pd.read_csv('/content/Shap_error_data.csv')\n",
        "Shap_error_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIwbBWIeVLEM",
        "outputId": "76bcc25d-adf4-4561-8d83-26bafb17dd1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5050 entries, 0 to 5049\n",
            "Data columns (total 28 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         5050 non-null   float64\n",
            " 1   priors_count                5050 non-null   float64\n",
            " 2   sex_Female                  5050 non-null   float64\n",
            " 3   sex_Male                    5050 non-null   float64\n",
            " 4   race_African-American       5050 non-null   float64\n",
            " 5   race_Asian                  5050 non-null   float64\n",
            " 6   race_Caucasian              5050 non-null   float64\n",
            " 7   race_Hispanic               5050 non-null   float64\n",
            " 8   race_Native American        5050 non-null   float64\n",
            " 9   race_Other                  5050 non-null   float64\n",
            " 10  Shap_age                    5050 non-null   float64\n",
            " 11  Shap_priors_count           5050 non-null   float64\n",
            " 12  Shap_sex_Female             5050 non-null   float64\n",
            " 13  Shap_sex_Male               5050 non-null   float64\n",
            " 14  Shap_race_African-American  5050 non-null   float64\n",
            " 15  Shap_race_Asian             5050 non-null   float64\n",
            " 16  Shap_race_Caucasian         5050 non-null   float64\n",
            " 17  Shap_race_Hispanic          5050 non-null   float64\n",
            " 18  Shap_race_Native American   5050 non-null   float64\n",
            " 19  Shap_race_Other             5050 non-null   float64\n",
            " 20  predicted_class             3542 non-null   float64\n",
            " 21  true_class                  3542 non-null   float64\n",
            " 22  errors                      3542 non-null   float64\n",
            " 23  TP                          3542 non-null   float64\n",
            " 24  TN                          3542 non-null   float64\n",
            " 25  FN                          3542 non-null   float64\n",
            " 26  FP                          3542 non-null   float64\n",
            " 27  Error_Type                  3542 non-null   object \n",
            "dtypes: float64(27), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(Shap_error_data.isna().sum())\n",
        "len(Shap_error_data.index)"
      ],
      "metadata": {
        "id": "4kLbqu6yYSYW",
        "outputId": "7268adfd-cbc5-4d8f-85bb-5af27558fcf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize / scaling dataset"
      ],
      "metadata": {
        "id": "idrXNeKQDvkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_dataset (raw_data, with_errors = True, just_features = True, scale_features = True, with_classes = True):\n",
        "  #input is the original dataset, whether errors are included, only features should be used, to scale the features, class labels are included\n",
        "\n",
        "  new_data = raw_data.copy(deep=True)\n",
        "\n",
        "  if with_errors:\n",
        "    scaling_factor = 0.8  # default scaling factor - there is a trade-off between scaling of weighing the errors to guide biased clusters while preventing too large and uninformative clusters\n",
        "    new_data['scaled_TP'] = new_data['TP'] * scaling_factor\n",
        "    new_data['scaled_TN'] = new_data['TN'] * scaling_factor\n",
        "    new_data['scaled_FN'] = new_data['FN'] * scaling_factor\n",
        "    new_data['scaled_FP'] = new_data['FP'] * scaling_factor\n",
        "\n",
        "  if just_features:\n",
        "    new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)\n",
        "\n",
        "\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "dW-4moGZDy8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "lXLzp-j9Vtcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def pca_plot(data, title, alpha):\n",
        "\n",
        "    #extract features for PCA and drop the other columns in other_features df\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type']]\n",
        "\n",
        "    # Scale the PCA features before using PCA\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(pca_features)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca_result = PCA(n_components=2).fit_transform(scaled_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index)\n",
        "\n",
        "    #create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "    temp_dataset.rename(columns={0: 'Principal Component 1', 1: 'Principal Component 2'}, inplace=True)\n",
        "\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset,x ='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"Error_Type\", palette=\"tab10\", style=\"Error_Type\")\n",
        "\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = PCA(n_components=2).fit(scaled_features).explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RMvUUByXVtBp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering Functions"
      ],
      "metadata": {
        "id": "1_3QOzE2Cgw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Density based clustering\n"
      ],
      "metadata": {
        "id": "yKMXs1SpCgTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}