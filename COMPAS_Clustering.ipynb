{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rCLy0OtAU6-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "08904c5d-53a1-49f3-8095-447fe75dd391"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'DBSCAN' from 'sklearn' (/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c8ebd98ef34d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DBSCAN' from 'sklearn' (/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "#from sklearn import DBSCAN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SHAP_ERROR_DATA"
      ],
      "metadata": {
        "id": "95UadjU1VNpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shap_error_data = pd.read_csv('/content/Shap_error_data.csv')\n",
        "#Shap_error_data.info()\n",
        "features = Shap_error_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)"
      ],
      "metadata": {
        "id": "pIwbBWIeVLEM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Shap_error_data.isna().sum())\n",
        "len(Shap_error_data.index)"
      ],
      "metadata": {
        "id": "4kLbqu6yYSYW",
        "outputId": "d4ec1ae8-fa0c-4a66-edf2-3c24fc7b1bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                              0\n",
            "priors_count                     0\n",
            "sex_Female                       0\n",
            "sex_Male                         0\n",
            "race_African-American            0\n",
            "race_Asian                       0\n",
            "race_Caucasian                   0\n",
            "race_Hispanic                    0\n",
            "race_Native American             0\n",
            "race_Other                       0\n",
            "Shap_age                         0\n",
            "Shap_priors_count                0\n",
            "Shap_sex_Female                  0\n",
            "Shap_sex_Male                    0\n",
            "Shap_race_African-American       0\n",
            "Shap_race_Asian                  0\n",
            "Shap_race_Caucasian              0\n",
            "Shap_race_Hispanic               0\n",
            "Shap_race_Native American        0\n",
            "Shap_race_Other                  0\n",
            "predicted_class               1507\n",
            "true_class                    1507\n",
            "errors                        1507\n",
            "TP                            1507\n",
            "TN                            1507\n",
            "FN                            1507\n",
            "FP                            1507\n",
            "Error_Type                    1507\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Shap_error_data.iloc[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbenGMRp2Zz6",
        "outputId": "1f9cd838-f78c-4fee-be1b-8ffd109e0b89"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                           1.557897\n",
            "priors_count                  1.966547\n",
            "sex_Female                   -0.491321\n",
            "sex_Male                      0.491321\n",
            "race_African-American         0.978838\n",
            "race_Asian                    -0.06462\n",
            "race_Caucasian               -0.711101\n",
            "race_Hispanic                -0.314295\n",
            "race_Native American         -0.054582\n",
            "race_Other                   -0.244563\n",
            "Shap_age                      1.950683\n",
            "Shap_priors_count            -1.257692\n",
            "Shap_sex_Female                0.14411\n",
            "Shap_sex_Male                -0.151978\n",
            "Shap_race_African-American    1.787196\n",
            "Shap_race_Asian              -0.018007\n",
            "Shap_race_Caucasian           0.707991\n",
            "Shap_race_Hispanic            0.264025\n",
            "Shap_race_Native American     0.046934\n",
            "Shap_race_Other               0.250397\n",
            "predicted_class                    NaN\n",
            "true_class                         NaN\n",
            "errors                             NaN\n",
            "TP                                 NaN\n",
            "TN                                 NaN\n",
            "FN                                 NaN\n",
            "FP                                 NaN\n",
            "Error_Type                         NaN\n",
            "Name: 3, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing: Initialize / scaling dataset"
      ],
      "metadata": {
        "id": "idrXNeKQDvkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_dataset (raw_data, with_errors = True, just_features = True, scale_features = True, with_classes = True):\n",
        "  #input is the original dataset, whether errors are included, only features should be used, to scale the features, class labels are included\n",
        "  #Initialisation of the dataset. Scales the features and errors, which can be included or exluded for clustering\n",
        "  #it returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC\n",
        "\n",
        "  new_data = raw_data.copy(deep=True)\n",
        "\n",
        "  if with_errors:\n",
        "    scaling_factor = 0.8  # default scaling factor - there is a trade-off between scaling of weighing the errors to guide biased clusters while preventing too large and uninformative clusters\n",
        "    new_data['scaled_TP'] = new_data['TP'] * scaling_factor\n",
        "    new_data['scaled_TN'] = new_data['TN'] * scaling_factor\n",
        "    new_data['scaled_FN'] = new_data['FN'] * scaling_factor\n",
        "    new_data['scaled_FP'] = new_data['FP'] * scaling_factor\n",
        "\n",
        "  if just_features:\n",
        "    new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1)\n",
        "\n",
        "  if scale_features:\n",
        "    to_scale = raw_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP'], axis=1).columns\n",
        "    new_data[to_scale] = StandardScaler().fit_transform(features[to_scale])\n",
        "\n",
        "  if with_classes:\n",
        "    for col in ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP']:\n",
        "      new_data[col] = raw_data[col]\n",
        "\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "dW-4moGZDy8P"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Drop rows where both TP and FN are zero\n",
        "def drop_zero_TP_FN(data):\n",
        "  return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "TP_FN_data = drop_zero_TP_FN(Shap_error_data)\n",
        "\n",
        "#Drop rows where both TN and FP are zero\n",
        "def drop_zero_TN_FP(data):\n",
        "  return data.loc[(data['TN']  == 1 ) & (data['FP'] == 1)]\n",
        "\n",
        "TN_FP_data = drop_zero_TN_FP(Shap_error_data)\n"
      ],
      "metadata": {
        "id": "Rll1BfIztumu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (TP_FN_data['TP'] == 0 & (TP_FN_data['FN'] == 0) ).any():\n",
        "    print(\"There's at least one occurrence of TP 0 and FN 0 in the dataset.\")\n",
        "else:\n",
        "    print(\"There's no occurrence of 1 in the column.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKJCVUiuDpqM",
        "outputId": "b8b67e3a-9a58-4c82-8765-d4c3ffe260e9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's at least one occurrence of TP 0 and FN 0 in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TP_FN_data[['TP', 'FN']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOkPZba61RWz",
        "outputId": "baa88e74-909b-4c0e-db20-bb4ae204c6ed"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TP   FN\n",
            "1     1.0  0.0\n",
            "6     1.0  0.0\n",
            "9     0.0  1.0\n",
            "11    1.0  0.0\n",
            "14    0.0  1.0\n",
            "...   ...  ...\n",
            "5039  0.0  1.0\n",
            "5040  1.0  0.0\n",
            "5041  0.0  1.0\n",
            "5043  1.0  0.0\n",
            "5048  1.0  0.0\n",
            "\n",
            "[1584 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TP_FN_data.iloc[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wgwtwu616Kw",
        "outputId": "7b287fc1-3c3a-4cf1-e9ea-1153355e8e0f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age                           -1.08491\n",
            "priors_count                 -0.505199\n",
            "sex_Female                     2.03533\n",
            "sex_Male                      -2.03533\n",
            "race_African-American         -1.02162\n",
            "race_Asian                    -0.06462\n",
            "race_Caucasian                 1.40627\n",
            "race_Hispanic                -0.314295\n",
            "race_Native American         -0.054582\n",
            "race_Other                   -0.244563\n",
            "Shap_age                     -0.256999\n",
            "Shap_priors_count             1.045115\n",
            "Shap_sex_Female               4.433496\n",
            "Shap_sex_Male                 4.233697\n",
            "Shap_race_African-American    0.804825\n",
            "Shap_race_Asian              -0.052872\n",
            "Shap_race_Caucasian           1.299981\n",
            "Shap_race_Hispanic            0.284534\n",
            "Shap_race_Native American     0.046934\n",
            "Shap_race_Other               0.040714\n",
            "predicted_class                    1.0\n",
            "true_class                         1.0\n",
            "errors                             0.0\n",
            "TP                                 1.0\n",
            "TN                                 0.0\n",
            "FN                                 0.0\n",
            "FP                                 0.0\n",
            "Error_Type                          TP\n",
            "Name: 11, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "needs to be separated still for TN/FP and TP/FN"
      ],
      "metadata": {
        "id": "lXLzp-j9Vtcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "\n",
        "    #extract features for PCA and drop the other columns in other_features df\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type'], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type']]\n",
        "\n",
        "    # Scale the PCA features before using PCA\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(pca_features)\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca_result = PCA(n_components=2).fit_transform(scaled_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index)\n",
        "\n",
        "    #create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "    temp_dataset.rename(columns={0: 'Principal Component 1', 1: 'Principal Component 2'}, inplace=True)\n",
        "\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset,x ='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"Error_Type\", palette=\"tab10\", style=\"Error_Type\")\n",
        "\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = PCA(n_components=2).fit(scaled_features).explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RMvUUByXVtBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Functions"
      ],
      "metadata": {
        "id": "1_3QOzE2Cgw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate error metric\n",
        "\n",
        "#Calculate bias\n",
        "\n",
        "#Calculate absolute difference\n",
        "\n",
        "#Calculate variance\n",
        "\n",
        "#Get min splittable cluster size\n",
        "\n",
        "##########################################\n",
        "\n",
        "#Select a new cluster to split on based on the smallest absolute difference from the overall error rate of 0.5\n",
        "#Function requires a df.columns named 'clusters' and 'FP'\n",
        "\n",
        "def select_new_cluster(data, error_column='FP', overall_error_rate=0.5):\n",
        "  smallest_diff = 1\n",
        "  cluster_number = 0\n",
        "\n",
        "  for cluster_number in data['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    cluster_data = data[data['clusters'] == cluster_number]\n",
        "    cluster_error_rate = cluster_data([error_column]).mean()\n",
        "    abs_diff = abs(overall_error_rate - cluster_error_rate)\n",
        "\n",
        "    if abs_diff < smallest_diff:\n",
        "      smallest_diff = abs_diff\n",
        "      selected_cluster = cluster_number\n",
        "\n",
        "  return selected_cluster\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Select a random cluster\n",
        "#returns a random cluster from provided list of clusters that is not -1\n",
        "def get_random_cluster(clusters):\n",
        "  result = -1\n",
        "  while (result == -1):\n",
        "    result = random.randint(0, len(clusters.unique()))\n",
        "  print('This is the random cluster we picked:', result)\n",
        "  return result\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Plot cluster\n",
        "def plot_clusters(data):\n",
        "  scatterplot = sns.scatterplot(data=data, x='1st', y='2nd', hue=\"clusters\", size = 'errors', sizes=(100, 20), palette = \"tab10\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "yKMXs1SpCgTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bias_with_error_rate(full_data, input_columns, error_column, max_iter=10, min_splittable_cluster_size=10, dbscan_max_iter=100):\n",
        "#Cluster the data based on the FP or FN for error column and selects the best cluster iteratively.\n",
        "  for i in range(1, max_iter):\n"
      ],
      "metadata": {
        "id": "jdlKRBSXF5-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}