{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KWNoIYnwRkW_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/ClusterCompas/blob/main/COMPAS_Clustering1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "rCLy0OtAU6-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SHAP_ERROR_DATA"
      ],
      "metadata": {
        "id": "95UadjU1VNpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Shap_error_data = pd.read_csv('/content/Shap_error_data.csv')\n",
        "Shap_error_data.info()\n",
        "features = Shap_error_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP',\n",
        "                                 'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'], axis=1)\n",
        "\n",
        "#print(Shap_error_data.loc[3])\n",
        "#print(Shap_error_data.isna().sum())\n",
        "len(Shap_error_data.index)"
      ],
      "metadata": {
        "id": "pIwbBWIeVLEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a74943-40c9-459b-fd49-d68a7e0959f1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5050 entries, 0 to 5049\n",
            "Data columns (total 28 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         5050 non-null   float64\n",
            " 1   priors_count                5050 non-null   float64\n",
            " 2   sex_Female                  5050 non-null   float64\n",
            " 3   sex_Male                    5050 non-null   float64\n",
            " 4   race_African-American       5050 non-null   float64\n",
            " 5   race_Asian                  5050 non-null   float64\n",
            " 6   race_Caucasian              5050 non-null   float64\n",
            " 7   race_Hispanic               5050 non-null   float64\n",
            " 8   race_Native American        5050 non-null   float64\n",
            " 9   race_Other                  5050 non-null   float64\n",
            " 10  Shap_age                    5050 non-null   float64\n",
            " 11  Shap_priors_count           5050 non-null   float64\n",
            " 12  Shap_sex_Female             5050 non-null   float64\n",
            " 13  Shap_sex_Male               5050 non-null   float64\n",
            " 14  Shap_race_African-American  5050 non-null   float64\n",
            " 15  Shap_race_Asian             5050 non-null   float64\n",
            " 16  Shap_race_Caucasian         5050 non-null   float64\n",
            " 17  Shap_race_Hispanic          5050 non-null   float64\n",
            " 18  Shap_race_Native American   5050 non-null   float64\n",
            " 19  Shap_race_Other             5050 non-null   float64\n",
            " 20  predicted_class             3534 non-null   float64\n",
            " 21  true_class                  3534 non-null   float64\n",
            " 22  errors                      3534 non-null   float64\n",
            " 23  TP                          3534 non-null   float64\n",
            " 24  TN                          3534 non-null   float64\n",
            " 25  FN                          3534 non-null   float64\n",
            " 26  FP                          3534 non-null   float64\n",
            " 27  Error_Type                  3534 non-null   object \n",
            "dtypes: float64(27), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(len(Shap_error_data.loc[Shap_error_data['FP']==1]))\n",
        "display(len(Shap_error_data.loc[Shap_error_data['FN']==1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "St3FgMUUVnWh",
        "outputId": "ccba85c2-5f57-413b-b532-b9c25c66d264"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "750"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All functions"
      ],
      "metadata": {
        "id": "n9iXBWEEJWth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_plot(data, title, alpha):\n",
        "    # Extract features for PCA and drop certain columns\n",
        "    pca_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                              'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other' ], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                           'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other']]\n",
        "\n",
        "    # Apply PCA with 2 components to scaled features and create a df for the resulting principal components\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(pca_features)\n",
        "    pca_df = pd.DataFrame(pca_result, index=pca_features.index, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "\n",
        "    # Create temporary dataset that contains both principal components and other features\n",
        "    temp_dataset = pca_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='Principal Component 1', y='Principal Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    print(f\"Explained Variance Ratio: PC1 = {explained_variance_ratio[0]:.2f}, PC2 = {explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "###############################################\n",
        "\n",
        "'''\n",
        "input is the original dataset, whether errors are included, only features should be used, to scale the features, class labels are included\n",
        "Scales the features and errors, which can be included or exluded for clustering\n",
        "it returns a scaled dataset with new columns \"clusters\" = 0 and \"new_clusters\" = -1, which is required for HBAC\n",
        "'''\n",
        "\n",
        "def initialize_dataset(data, with_errors=True, just_features=True, scale_features=True, with_classes=True):\n",
        "    new_data = data.copy(deep=True)\n",
        "\n",
        "    if with_errors:\n",
        "        scaling_factor = 0.8\n",
        "        error_columns = ['TP', 'TN', 'FN', 'FP']\n",
        "        new_data[error_columns] = new_data[error_columns] * scaling_factor\n",
        "\n",
        "    if just_features:\n",
        "        new_data = new_data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type',\n",
        "                                  'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American',\n",
        "                                  'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other'], axis=1)\n",
        "\n",
        "    if scale_features:\n",
        "        numeric_columns = new_data.select_dtypes(include=['number']).columns\n",
        "        scaler = StandardScaler()\n",
        "        new_data[numeric_columns] = scaler.fit_transform(new_data[numeric_columns])\n",
        "\n",
        "    if with_classes:\n",
        "      for col in ['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type',\n",
        "                  'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American',\n",
        "                  'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other']:\n",
        "            new_data[col] = data[col]\n",
        "\n",
        "    new_data['clusters'] = 0\n",
        "    new_data['new_clusters'] = -1\n",
        "\n",
        "    return new_data\n",
        "\n",
        "###############################################\n",
        "\n",
        "##Drop rows where both TP and FN are 0\n",
        "def drop_zero_TP_FN(data):\n",
        "    return data.loc[(data['TP'] == 1) | (data['FN'] == 1)]\n",
        "\n",
        "TPFN_data = drop_zero_TP_FN(Shap_error_data)\n",
        "\n",
        "#Drop rows where both TN and FP are 0\n",
        "\n",
        "def drop_zero_TN_FP(data):\n",
        "    return data.loc[(data['TN'] == 1) | (data['FP'] == 1)]\n",
        "\n",
        "TNFP_data = drop_zero_TN_FP(Shap_error_data)\n",
        "\n",
        "################################################\n",
        "\n",
        "\n",
        "#Calculate accuracy on error\n",
        "def accuracy_error (results, error =None):\n",
        "  if len(results) == 0:\n",
        "    print (\"you are calculating the accuracy on an empty cluster\")\n",
        "  correct = results.loc[results[error] == 0]\n",
        "  acc = len(correct)/ len(results)\n",
        "  return acc\n",
        "\n",
        "#################################\n",
        "\n",
        "#Calculate bias based on accuracy_error function\n",
        "\n",
        "'''Calculate bias: negative bias as the accuracy of a selected cluster - accruacy of the remaining clusters\n",
        " Colster col: name of the DF column where the cluster assignments are'''\n",
        "\n",
        "def bias_w_error (data, cluster_id, cluster_col):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  if len(cluster_x) ==0:\n",
        "    print(\"this is an empty cluster\", cluster_id)\n",
        "  remaining_clusters = data.loc[data[cluster_col] != cluster_id]\n",
        "  if len(remaining_clusters) ==0:\n",
        "    print (\"This cluster is the entire dataset. Cluster:\", cluster_id)\n",
        "  return accuracy_error(cluster_x) - accuracy_error(remaining_clusters)\n",
        "\n",
        "#################################\n",
        "\n",
        "#Combined function for get_max_neg_bias and get_max_pos_bias\n",
        "\n",
        "def get_max_bias(data, bias_type = 'negative', function = bias_w_error):\n",
        "  max_bias = float('inf') if bias_type == 'negative' else -float('inf') #initializes max_bias with either positive of negative infinity (special floating point value) based on bias_type param\n",
        "  for cluster_number in data['new_clusters'].unique():\n",
        "    if cluster_number == -1:#outliers in dbscan\n",
        "      continue\n",
        "    current_bias = function(data, cluster_number, 'new_clusters') #for each cluster the bias_w_error function is calculated\n",
        "    if (bias_type == 'negative' and current_bias < max_bias) or (bias_type == 'positive' and current_bias > max_bias):\n",
        "      max_bias = current_bias\n",
        "  print(f'Maximum {bias_type} bias is:', max_bias)\n",
        "  return max_bias\n",
        "\n",
        "\n",
        "###################################\n",
        "#get max bias cluster --> returns a cluster with neg bias (for newly added clusters)\n",
        "\n",
        "def get_cluster_max_bias(data, function = bias_w_error):\n",
        "  max_pos_bias = 100 #max_abs bias selma code\n",
        "  max_bias_cluster = -2\n",
        "  for cluster_number in data['clusters'].unique():\n",
        "    if cluster_number == -1:\n",
        "      continue\n",
        "    current_bias = (function(data, cluster_number, 'clusters')) #pos function to find the highest bias\n",
        "    print(f\"{cluster_number} has bias {current_bias}\")\n",
        "    if current_bias < max_pos_bias:\n",
        "      max_pos_bias = current_bias\n",
        "      max_bias_cluster = cluster_number\n",
        "  print ('cluster with the highest discriminating bias:', max_bias_cluster)\n",
        "  return max_bias_cluster\n",
        "\n",
        "#################################\n",
        "\n",
        "#Select a new cluster to split on based on the smallest absolute difference from the overall error rate of 0.5\n",
        "#Function requires a df.columns named 'clusters' and 'FP'\n",
        "\n",
        "def select_new_cluster(data, error_column=None, overall_error_rate=0.5):\n",
        "    smallest_diff = 1\n",
        "    selected_cluster = None\n",
        "\n",
        "    if error_column is None:\n",
        "        error_column = 'FP'  # Default to 'FP' if error_column is not specified\n",
        "\n",
        "    for cluster_number in data['clusters'].unique():\n",
        "        if cluster_number == -1:\n",
        "            continue\n",
        "        cluster_data = data[data['clusters'] == cluster_number]\n",
        "        cluster_error_rate = cluster_data[error_column].mean()  # Use specified error column\n",
        "        abs_diff = abs(overall_error_rate - cluster_error_rate)\n",
        "\n",
        "        if abs_diff < smallest_diff:\n",
        "            smallest_diff = abs_diff\n",
        "            selected_cluster = cluster_number\n",
        "\n",
        "    return selected_cluster\n",
        "\n",
        "#############################################\n",
        "#Calculate variance\n",
        "\n",
        "def calculate_variance(data):\n",
        "  variance_list_local = []\n",
        "  for j in data['clusters'].unique():\n",
        "    average_acc = accuracy_error(data)\n",
        "    bias_clus = bias_w_error(data, j, 'clusters')\n",
        "    variance_list_local.append(bias_clus)\n",
        "  variance = np.variance(variance_list_local)\n",
        "  return variance\n",
        "\n",
        "#calculate bias_acc_global\n",
        "\n",
        "def calculate_bias_global_average(data, cluster_id, cluster_col, ave_acc):\n",
        "  cluster_x = data.loc[data[cluster_col] == cluster_id]\n",
        "  return accuracy_error(cluster_x) - ave_acc\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Get min splittable cluster size - returns size of smallest new cluster\n",
        "def min_split_cluster_size(data):\n",
        "  min_cluster_size = len(data)\n",
        "  for i in data['new_clusters'].unique():\n",
        "    if i == -1:\n",
        "      continue\n",
        "    size = len(data.loc[data['new_clusters']==i])\n",
        "    if size < min_cluster_size:\n",
        "      min_cluster_size = size\n",
        "  return min_cluster_size\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "#Select a random cluster from provided list of clusters that is not -1\n",
        "def get_random_cluster(clusters):\n",
        "  result = -1\n",
        "  while (result == -1):\n",
        "    result = random.randint(0, len(clusters.unique()))\n",
        "  print('This is the random cluster we picked:', result)\n",
        "  return result\n",
        "\n",
        "#############################################\n",
        "\n",
        "#Plot cluster\n",
        "def plot_clusters(data):\n",
        "  scatterplot = sns.scatterplot(data=data, x='1st', y='2nd', hue=\"clusters\", size = 'errors', sizes=(100, 20), palette = \"tab10\")\n",
        "  plt.show()\n",
        "\n",
        "'''\n",
        "\n",
        "def tsne_plot(data, title, perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5):\n",
        "    tsne_features = data.drop(['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                              'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other' ], axis=1)\n",
        "    other_features = data[['predicted_class', 'true_class', 'errors', 'TP', 'TN', 'FN', 'FP', 'Error_Type', 'clusters', 'new_clusters',\n",
        "                           'Shap_age', 'Shap_priors_count' , 'Shap_sex_Female', 'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian', 'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American', 'Shap_race_Other']]\n",
        "\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\n",
        "    tsne_result = tsne.fit_transform(tsne_features)\n",
        "    tsne_df = pd.DataFrame(tsne_result, index = features.index, columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
        "\n",
        "    temp_dataset = tsne_df.join(other_features, how='left')\n",
        "\n",
        "    # Create scatterplot using seaborn\n",
        "    scatterplot = sns.scatterplot(data=temp_dataset, x='t-SNE Component 1', y='t-SNE Component 2', alpha=alpha, hue=\"clusters\", palette='tab10', style='Error_Type')\n",
        "    scatterplot.set_title(title)\n",
        "    scatterplot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=1)\n",
        "\n",
        "    plt.show()\n",
        "'''\n"
      ],
      "metadata": {
        "id": "yKMXs1SpCgTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bbf8236c-9ec6-468d-d9e8-89a74b268f9b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ndef tsne_plot(data, title, perplexity = 30, learning_rate = 200, n_iter = 1000, alpha = 0.5):\\n    tsne_features = data.drop([\\'predicted_class\\', \\'true_class\\', \\'errors\\', \\'TP\\', \\'TN\\', \\'FN\\', \\'FP\\', \\'Error_Type\\', \\'clusters\\', \\'new_clusters\\', \\n                              \\'Shap_age\\', \\'Shap_priors_count\\' , \\'Shap_sex_Female\\', \\'Shap_sex_Male\\', \\'Shap_race_African-American\\', \\'Shap_race_Asian\\', \\'Shap_race_Caucasian\\', \\'Shap_race_Hispanic\\', \\'Shap_race_Native American\\', \\'Shap_race_Other\\' ], axis=1)\\n    other_features = data[[\\'predicted_class\\', \\'true_class\\', \\'errors\\', \\'TP\\', \\'TN\\', \\'FN\\', \\'FP\\', \\'Error_Type\\', \\'clusters\\', \\'new_clusters\\',\\n                           \\'Shap_age\\', \\'Shap_priors_count\\' , \\'Shap_sex_Female\\', \\'Shap_sex_Male\\', \\'Shap_race_African-American\\', \\'Shap_race_Asian\\', \\'Shap_race_Caucasian\\', \\'Shap_race_Hispanic\\', \\'Shap_race_Native American\\', \\'Shap_race_Other\\']]\\n\\n\\n    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\\n    tsne_result = tsne.fit_transform(tsne_features)\\n    tsne_df = pd.DataFrame(tsne_result, index = features.index, columns=[\\'t-SNE Component 1\\', \\'t-SNE Component 2\\'])\\n\\n    temp_dataset = tsne_df.join(other_features, how=\\'left\\')\\n\\n    # Create scatterplot using seaborn\\n    scatterplot = sns.scatterplot(data=temp_dataset, x=\\'t-SNE Component 1\\', y=\\'t-SNE Component 2\\', alpha=alpha, hue=\"clusters\", palette=\\'tab10\\', style=\\'Error_Type\\')\\n    scatterplot.set_title(title)\\n    scatterplot.legend(loc=\\'center left\\', bbox_to_anchor=(1.0, 0.5), ncol=1)\\n\\n    plt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TPFN_data = drop_zero_TP_FN(Shap_error_data)\n",
        "TNFP_data = drop_zero_TN_FP(Shap_error_data)\n",
        "\n",
        "if ((TPFN_data['TP'] == 0) & (TPFN_data['FN'] == 0)).any():\n",
        "    print(\"There's at least one occurrence of TP 0 and FN 0 in the dataset.\")\n",
        "else:\n",
        "    print(\"There's no occurrence of 0,0 in both columns TP and FN.\")\n",
        "\n",
        "if ((TNFP_data['TN'] == 0) & (TNFP_data['FP'] == 0) ).any():\n",
        "    print(\"There's at least one occurrence of TP 0 and FN 0 in the dataset.\")\n",
        "else:\n",
        "    print(\"There's no occurrence of 0,0 in both columns TN and FP.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgc662egKHlh",
        "outputId": "93e11c9a-e1d4-4a08-b6c9-1b655d55736d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's no occurrence of 0,0 in both columns TP and FN.\n",
            "There's no occurrence of 0,0 in both columns TN and FP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TPFN_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZPppH-8N-a5",
        "outputId": "c55d1713-2143-4f87-bf38-4054a21c279f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1579 entries, 1 to 5048\n",
            "Data columns (total 28 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         1579 non-null   float64\n",
            " 1   priors_count                1579 non-null   float64\n",
            " 2   sex_Female                  1579 non-null   float64\n",
            " 3   sex_Male                    1579 non-null   float64\n",
            " 4   race_African-American       1579 non-null   float64\n",
            " 5   race_Asian                  1579 non-null   float64\n",
            " 6   race_Caucasian              1579 non-null   float64\n",
            " 7   race_Hispanic               1579 non-null   float64\n",
            " 8   race_Native American        1579 non-null   float64\n",
            " 9   race_Other                  1579 non-null   float64\n",
            " 10  Shap_age                    1579 non-null   float64\n",
            " 11  Shap_priors_count           1579 non-null   float64\n",
            " 12  Shap_sex_Female             1579 non-null   float64\n",
            " 13  Shap_sex_Male               1579 non-null   float64\n",
            " 14  Shap_race_African-American  1579 non-null   float64\n",
            " 15  Shap_race_Asian             1579 non-null   float64\n",
            " 16  Shap_race_Caucasian         1579 non-null   float64\n",
            " 17  Shap_race_Hispanic          1579 non-null   float64\n",
            " 18  Shap_race_Native American   1579 non-null   float64\n",
            " 19  Shap_race_Other             1579 non-null   float64\n",
            " 20  predicted_class             1579 non-null   float64\n",
            " 21  true_class                  1579 non-null   float64\n",
            " 22  errors                      1579 non-null   float64\n",
            " 23  TP                          1579 non-null   float64\n",
            " 24  TN                          1579 non-null   float64\n",
            " 25  FN                          1579 non-null   float64\n",
            " 26  FP                          1579 non-null   float64\n",
            " 27  Error_Type                  1579 non-null   object \n",
            "dtypes: float64(27), object(1)\n",
            "memory usage: 422.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP_FN_data = initialize_dataset(TPFN_data)\n",
        "TN_FP_data = initialize_dataset(TNFP_data)\n",
        "TP_FN_data.info()\n",
        "#len(TP_FN_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwy2MRyjde1L",
        "outputId": "964e796b-8094-4a8d-9f92-b5b4640bb82f"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1579 entries, 1 to 5048\n",
            "Data columns (total 30 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   age                         1579 non-null   float64\n",
            " 1   priors_count                1579 non-null   float64\n",
            " 2   sex_Female                  1579 non-null   float64\n",
            " 3   sex_Male                    1579 non-null   float64\n",
            " 4   race_African-American       1579 non-null   float64\n",
            " 5   race_Asian                  1579 non-null   float64\n",
            " 6   race_Caucasian              1579 non-null   float64\n",
            " 7   race_Hispanic               1579 non-null   float64\n",
            " 8   race_Native American        1579 non-null   float64\n",
            " 9   race_Other                  1579 non-null   float64\n",
            " 10  predicted_class             1579 non-null   float64\n",
            " 11  true_class                  1579 non-null   float64\n",
            " 12  errors                      1579 non-null   float64\n",
            " 13  TP                          1579 non-null   float64\n",
            " 14  TN                          1579 non-null   float64\n",
            " 15  FN                          1579 non-null   float64\n",
            " 16  FP                          1579 non-null   float64\n",
            " 17  Error_Type                  1579 non-null   object \n",
            " 18  Shap_age                    1579 non-null   float64\n",
            " 19  Shap_priors_count           1579 non-null   float64\n",
            " 20  Shap_sex_Female             1579 non-null   float64\n",
            " 21  Shap_sex_Male               1579 non-null   float64\n",
            " 22  Shap_race_African-American  1579 non-null   float64\n",
            " 23  Shap_race_Asian             1579 non-null   float64\n",
            " 24  Shap_race_Caucasian         1579 non-null   float64\n",
            " 25  Shap_race_Hispanic          1579 non-null   float64\n",
            " 26  Shap_race_Native American   1579 non-null   float64\n",
            " 27  Shap_race_Other             1579 non-null   float64\n",
            " 28  clusters                    1579 non-null   int64  \n",
            " 29  new_clusters                1579 non-null   int64  \n",
            "dtypes: float64(27), int64(2), object(1)\n",
            "memory usage: 447.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determining the epsilon parameter using the K-Distance plot\n",
        "\n",
        "Using the k-distance plot to determine epsilon by identifying the knee point (where there is a significant increese"
      ],
      "metadata": {
        "id": "KWNoIYnwRkW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_features = TP_FN_data.iloc[:, : 11]\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors = 3) # compute distance to 2nd nearest neighbor (k=3) for each data point\n",
        "nbrs = neigh.fit(nn_features)\n",
        "distances, indices = nbrs.kneighbors(nn_features)\n",
        "\n",
        "distances = np.sort(distances, axis= 0)\n",
        "distances = distances [:,1]\n",
        "plt.plot(distances)\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('K-distance')\n",
        "plt.title(\"Optimal Value of Epsilon for COMPAS\")\n",
        "\n",
        "round(0.02 * len(TP_FN_data)) #decide on min nr points for dbscan?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ifPwnQZpRikK",
        "outputId": "6028c702-7bdc-4cc7-e3b7-02f427113bf6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAHHCAYAAACyWSKnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPVUlEQVR4nO3dd1hTZ/sH8G9YARQiIFsQFBUnbou7Soujji4Vrautr/Oto9Vq+6tWbYttX63VWu0UtVirdbXWUcStuLd1YUUcgAyZyszz+wNzNDJkBE44fj/XlcvknCcn9xNDuHmmSgghQERERKQgJnIHQERERGRoTHCIiIhIcZjgEBERkeIwwSEiIiLFYYJDREREisMEh4iIiBSHCQ4REREpDhMcIiIiUhwmOERERKQ4THDomRESEgKVSoWoqKhn5rX37NkDlUqFPXv2VOrrltb27dvRvHlzWFpaQqVSITk5We6Q9HTt2hVdu3aVHkdFRUGlUiEkJESWeFatWgVfX1+Ym5ujRo0assRAZOyY4JBsLly4gDfeeAPu7u5Qq9Vwc3PDkCFDcOHChXJd97PPPsOmTZsME2QlatasGTw9PVHc7ikdOnSAs7MzcnNzKzGyipWYmIgBAwbAysoKS5YswapVq1CtWrVCy+oSxaJuhw8fruToK9+lS5cwYsQI1K1bFz/88AO+//77Snnd06dP44033oCHhwfUajXs7e0REBCA5cuXIy8vT69sRkYG5s6di2bNmsHa2hoajQadOnXCypUrC/186/7/3n777UJf+8MPP5TKJCQkSMdHjBih9/9va2sLPz8/zJ8/H1lZWQWuM23aNKhUKgwcOLDIesbHx2PixInw9fWFlZUVnJyc0LZtW7z//vtIT08v6dtFRsBM7gDo2bRhwwYEBQXB3t4eb731Fry9vREVFYWffvoJv//+O9asWYOXX365TNf+7LPP8Nprr6F///56x4cOHYpBgwZBrVYboAaGN2TIEEyfPh379+9H586dC5yPiopCREQEJkyYADMz5fzoHjt2DGlpaZg7dy4CAgJK9Jw5c+bA29u7wHEfHx9DhwcA+PvvvyvkumWxZ88eaLVafP311xVW3yf9+OOPGDNmDJydnTF06FDUq1cPaWlpCA8Px1tvvYWYmBh88MEHAIC4uDh0794dFy9exKBBgzBhwgRkZmZi/fr1GD58OLZu3YrQ0FCYmprqvYalpSXWr1+Pb7/9FhYWFnrnfv31V1haWiIzM7NAbGq1Gj/++CMAIDk5GevXr8d7772HY8eOYc2aNVI5IQR+/fVXeHl54c8//0RaWhpsbGz0rpWUlITWrVsjNTUVb775Jnx9fZGYmIizZ89i6dKlGDt2LKpXr26Q95QqgSCqZJGRkcLa2lr4+vqKu3fv6p2Lj48Xvr6+olq1auLatWtlun61atXE8OHDDRCp4SxfvlwAENevXy+yTHR0tFCpVGL06NGFnv/ss88EAHH48OESv+7u3bsFALF79+5SRlx5VqxYIQCIY8eOPbWs7n0sSdmKdP36dQFALF++vNJfe/bs2QKAiI+PN9g1MzIyijwXEREhTE1NRceOHUVqamqB88eOHdN7HwIDA4WJiYnYvHlzgbLvvfeeACDmzZundxyA6N+/vzAxMRGbNm3SO3fw4EEBQLz66qsF6j18+HBRrVo1vfJ5eXmidevWAoC4ffu2dHzXrl0CgNi1a5cwNzcXISEhBeL74osvBABx8ODBAudSUlLEgwcPChwn48UEhyrd6NGjBQCxb9++Qs/v3btXAND7RT9r1iwBQFy8eFG8/vrrwsbGRtjb24t33nlH70sHQIGbLtkpLMmoXbu26N27t9i9e7do1aqVsLS0FE2aNJESgvXr14smTZoItVotWrZsKU6ePKkX65kzZ8Tw4cOFt7e3UKvVwtnZWYwcOVIkJCTolStJgiOEEF26dBEODg4iOzu7wLkmTZqIunXrCiGEiIqKEmPHjhX169cXlpaWwt7eXrz22msFrl9YglO7du1CE8AuXbqILl266B3LzMwUM2fOFHXr1hUWFhaiVq1aYurUqSIzM7PYeuisXbtWtGzZUlhaWgoHBwcxZMgQcevWLb3XLOr/qzAlTXB0CciXX34pFixYIDw9PYWlpaXo3LmzOHfunF7ZmJgYMWLECOHu7i4sLCyEi4uL6Nu3r957+eR7U1SCEx4eLjp27Cisra2FRqMRffv2Ff/8849eGd1n+erVq2L48OFCo9EIW1tbMWLEiGITDSHy/++efL9mzZolnV+yZIlo1KiRsLCwEK6urmLcuHHi3r17etfo0qWLaNy4sTh+/Ljo1KmTsLKyEhMnTizyNXv06CHMzMzEjRs3io1NiPxkCIB48803Cz2fk5Mj6tWrJ+zs7MT9+/el4wDE+PHjRdeuXcWAAQP0njNu3DjRtGlT6X17WoIjxKNE6vFE5a233hKNGjUSQgjRs2dP8cILLxR43ujRo4WpqanIy8t7al3J+HEMDlW6P//8E15eXujUqVOh5zt37gwvLy/89ddfBc4NGDAAmZmZCA4ORq9evbBo0SL85z//kc6vWrUKarUanTp1wqpVq7Bq1SqMHj262HgiIyMxePBg9OnTB8HBwbh37x769OmD0NBQTJ48GW+88QZmz56Na9euYcCAAdBqtdJzw8LC8O+//2LkyJFYvHgxBg0ahDVr1qBXr17FjqUpypAhQ5CYmIgdO3boHT937hzOnz+PIUOGAMjv1jl06BAGDRqERYsWYcyYMQgPD0fXrl1x//79Ur9uYbRaLfr27Yv//e9/6NOnDxYvXoz+/fvjq6++KnYMg05ISAgGDBgAU1NTBAcHY9SoUdiwYQM6duwoDSL+8MMPpf+/OXPmlOj/CwBSUlKQkJCgd0tMTCxQbuXKlVi0aBHGjx+PGTNm4Pz58+jWrRvi4uKkMq+++io2btyIkSNH4ttvv8U777yDtLQ0REdHl/Cdyrdz504EBgbi7t27+PjjjzFlyhQcOnQIHTp0KHRw+YABA5CWlobg4GAMGDAAISEhmD17drGvsXDhQqnrdunSpVi1ahVeeeUVAMDHH3+M8ePHw83NDfPnz8err76K7777Di+++CJycnL0rpOYmIiePXuiefPmWLhwIZ5//vlCX+/+/fsIDw9H586d4enp+dT34M8//wQADBs2rNDzZmZmGDx4MO7du4eDBw8WOD948GD8+eef0liX3NxcrFu3DoMHD37qaz/u2rVrAAAHBwcAQFZWFtavX4+goCAAQFBQEHbt2oXY2Fi959WuXRt5eXlYtWpVqV6PjJTcGRY9W5KTkwUA0a9fv2LL9e3bVwCQmsR1f7317dtXr9y4ceMEAHHmzBnpWFFdVEW14AAQhw4dko7t2LFDABBWVlZ6f7V+9913BVpDHv8rVOfXX38t0EJV0hacpKQkoVarRVBQkN7x6dOnCwDi8uXLRb6u7q/nlStXSsfK04KzatUqYWJiIvbv369XbtmyZUU24+tkZ2cLJycn0aRJE70Wti1btggAYubMmdKx0nQ76coWdlOr1VI5XQuLlZWVXovRkSNHBAAxefJkIYQQ9+7dk1p6ilOSFpzmzZsLJycnkZiYKB07c+aMMDExEcOGDZOO6T7LT7ZyvPzyy8LBweGp70FhLRl3794VFhYW4sUXX9Rrffjmm28EAPHzzz/r1QWAWLZs2VNf68yZMwJAsS08j+vfv78AUKDV6HEbNmwQAMSiRYukY3jYgpOUlCQsLCzEqlWrhBBC/PXXX0KlUomoqKhiW3Di4+NFfHy8iIyMFJ999plQqVSiWbNmUrnff/9dajUTQojU1FRhaWkpvvrqK73YYmNjhaOjowAgfH19xZgxY8Tq1atFcnJyiepPxoUtOFSp0tLSAKDA4L4n6c6npqbqHR8/frze4//+978AgK1bt5Y5pkaNGsHf31963K5dOwBAt27d9P5q1R3/999/pWNWVlbS/czMTCQkJOC5554DAJw8ebLUsdjZ2aFXr174448/kJGRASB/cOSaNWvQunVr1K9fv8Dr5uTkIDExET4+PqhRo0aZXrcw69atQ8OGDeHr66vXUtKtWzcAwO7du4t87vHjx3H37l2MGzcOlpaW0vHevXvD19e30Na50liyZAnCwsL0btu2bStQrn///nB3d5cet23bFu3atZM+L1ZWVrCwsMCePXtw7969MscTExOD06dPY8SIEbC3t5eON2vWDC+88EKhn88xY8boPe7UqRMSExMLfOZLYufOncjOzsakSZNgYvLoa33UqFGwtbUt8H6r1WqMHDnyqdfVxfK0n1edkvx8F/WzDeR//nv06IFff/0VALB69Wq0b98etWvXLvJ6GRkZcHR0hKOjI3x8fPDBBx/A398fGzdulMqEhoaidevW0qBsGxsb9O7dG6GhoXrXcnZ2xpkzZzBmzBjcu3cPy5Ytw+DBg+Hk5IS5c+eWqVWW5MMEhyqV7stN90VYlKK+KOvVq6f3uG7dujAxMSnX+jJPNr1rNBoAgIeHR6HHH/9FmJSUhIkTJ8LZ2RlWVlZwdHSUZvekpKSUKZ4hQ4YgIyMDmzdvBgAcOnQIUVFRUvcUADx48AAzZ86UpuzWrFkTjo6OSE5OLvPrPunq1au4cOGC9MtDd9MlWXfv3i3yuTdu3AAANGjQoMA5X19f6XxZtW3bFgEBAXq3wrpZnvy8AED9+vWlz4tarcbnn3+Obdu2wdnZGZ07d8YXX3xRoOviaYqrb8OGDZGQkCAlrDpPfu7s7OwAoEyJVlGvb2FhgTp16hR4v93d3QvMVCqMra0tgKf/vOqU5Of7aUnQ4MGDERYWhujoaGzatOmp3VOWlpZSkrtv3z7cvHkTBw8eRJ06dQDkz6zaunUrunTpgsjISOnWoUMHHD9+HFeuXNG7nqurK5YuXYqYmBhcvnwZixYtgqOjI2bOnImffvqpRO8DGQflzDWlKkGj0cDV1RVnz54tttzZs2fh7u4ufcEWRaVSlTumJ6erPu3443/FDRgwAIcOHcLUqVPRvHlzVK9eHVqtFj169NAbq1MaL730EjQaDVavXo3Bgwdj9erVMDU1xaBBg6Qy//3vf7F8+XJMmjQJ/v7+0Gg0UKlUGDRo0FNft6j3LC8vT6/OWq0WTZs2xYIFCwot/2QCWFVNmjQJffr0waZNm7Bjxw589NFHCA4Oxq5du9CiRYsKe92SfL4qyuMtgMXx8fGBmZkZzp07V6LyDRs2xKZNm3D27NlClzoAIP3sN2rUqNDzffv2hVqtxvDhw5GVlYUBAwYU+5qmpqbFLi+wbt06ZGVlYf78+Zg/f36B86GhoYWOfVKpVKhfvz7q16+P3r17o169eggNDS1yrR4yPkxwqNK99NJL+OGHH3DgwAF07NixwPn9+/cjKiqq0MGmV69e1Vv/JDIyElqtFl5eXtIxQyQ9JXHv3j2Eh4dj9uzZmDlzpl6M5aFWq/Haa69h5cqViIuLw7p169CtWze4uLhIZX7//XcMHz5c7ws7MzOzRCsA29nZFVruxo0b0l+9QH7r2JkzZ9C9e/dSv6e6LoXLly9LXVo6ly9fLrbLwZAK+7+4cuWK3ucFyK/ru+++i3fffRdXr15F8+bNMX/+fPzyyy8lep3H6/ukS5cuoWbNmkUuXmgIj7/+4/+H2dnZuH79eonXF3qStbU1unXrhl27duHmzZtPTWpfeuklBAcHY+XKlYUmOHl5eVi9ejXs7OzQoUOHQq9hZWWF/v3745dffkHPnj1Rs2bNMsWuExoaiiZNmmDWrFkFzn333XdYvXr1Uwd316lTB3Z2doiJiSlXLFS52EVFlW7q1KmwsrLC6NGjC8x8SUpKwpgxY2BtbY2pU6cWeO6SJUv0Hi9evBgA0LNnT+lYtWrVKmWpf91f4E/+xb1w4cJyX3vIkCHIycnB6NGjER8fr9c9pXvtJ1938eLFBVaULUzdunVx+PBhZGdnS8e2bNmCmzdv6pUbMGAAbt++jR9++KHANR48eFCgy+VxrVu3hpOTE5YtW6a3ouy2bdtw8eJF9O7d+6lxGsKmTZtw+/Zt6fHRo0dx5MgR6fNy//79AovH1a1bFzY2NoWuhFsUV1dXNG/eHCtWrND77J0/fx5///03evXqVb6KPEVAQAAsLCywaNEivc/FTz/9hJSUlHK937NmzYIQAkOHDi10Jd8TJ05gxYoVAID27dtLqxtv2bKlQNkPP/wQV65cwbRp04ptRXrvvfcwa9YsfPTRR2WOGwBu3ryJffv2YcCAAXjttdcK3EaOHInIyEgcOXIEAHDkyJFCP9dHjx5FYmJioV2QZLzYgkOVrl69elixYgWGDBmCpk2bFljJOCEhAb/++ivq1q1b4LnXr19H37590aNHD0REROCXX37B4MGD4efnJ5Vp1aoVdu7ciQULFsDNzQ3e3t7SAGFDsrW1lcZs5OTkwN3dHX///TeuX79e7mt36dIFtWrVwubNm2FlZSVNBdZ56aWXsGrVKmg0GjRq1AgRERHYuXOnNC22OG+//TZ+//139OjRAwMGDMC1a9fwyy+/FHi/hw4dirVr12LMmDHYvXs3OnTogLy8PFy6dAlr167Fjh070Lp160Jfw9zcHJ9//jlGjhyJLl26ICgoCHFxcfj666/h5eWFyZMnl/3NQX6idOnSpQLH27dvr9eC4ePjg44dO2Ls2LHIysrCwoUL4eDggGnTpgHIb83p3r07BgwYgEaNGsHMzAwbN25EXFycXpdgSXz55Zfo2bMn/P398dZbb+HBgwdYvHgxNBoNPv7443LV92kcHR0xY8YMzJ49Gz169EDfvn1x+fJlfPvtt2jTpg3eeOONMl+7ffv2WLJkCcaNGwdfX1+9lYz37NmDP/74A5988olUfuXKlejevTv69euHwYMHo1OnTsjKysKGDRuwZ88eDBw4sNA/Xh7n5+en9zNdVqtXr4YQAn379i30fK9evWBmZobQ0FC0a9cOq1atQmhoKF5++WW0atUKFhYWuHjxIn7++WdYWlpKqzVTFSHb/C165p09e1YEBQUJV1dXYW5uLlxcXERQUFCBhdiEeDQ19p9//hGvvfaasLGxEXZ2dmLChAkFVhe9dOmS6Ny5s7CysirxQn9PwsNpq497fPE4nVu3bomXX35Z1KhRQ2g0GvH666+LO3fuFFiAraTTxB83depUAaDAwmdC5E9vHjlypKhZs6aoXr26CAwMFJcuXSowBbyolYznz58v3N3dhVqtFh06dBDHjx8vdKG/7Oxs8fnnn4vGjRsLtVot7OzsRKtWrcTs2bNFSkrKU+vw22+/iRYtWgi1Wi3s7e0LLPQnhOGmieOxaduP/1/Nnz9feHh4CLVaLTp16qS3pEBCQoIYP368tHq2RqMR7dq1E2vXrtV73ZIu9Ldz507RoUMHYWVlJWxtbUWfPn2KXOjvyZWIS/oZKer5QuRPC/f19RXm5ubC2dlZjB07tsiF/krrxIkTYvDgwcLNzU2Ym5sLOzs70b17d7FixYoCC+OlpaWJjz/+WDRu3FhYWVkJGxsb0aFDBxESEiK0Wm2Baxf281aSehe10J9O06ZNhaenZ7HX7dq1q3BychI5OTni7NmzYurUqaJly5bC3t5emJmZCVdXV/H6668XWOSTjJ9KCM57I+P38ccfY/bs2YiPjy93nzwpX1RUFLy9vfHll1/ivffekzscIpIBx+AQERGR4jDBISIiIsVhgkNERESKwzE4REREpDhswSEiIiLFYYJDREREivPMLfSn1Wpx584d2NjYVNqS/kRERFQ+QgikpaXBzc0NJiZPb5955hKcO3fuKGaTQCIiomfNzZs3UatWraeWe+YSHBsbGwD5b9DTdqomIiIi45CamgoPDw/p9/jTPHMJjq5bytbWlgkOERFRFVPS4SUcZExERESKwwSHiIiIFIcJDhERESmOrAlOcHAw2rRpAxsbGzg5OaF///64fPlysc8JCQmBSqXSu1laWlZSxERERFQVyJrg7N27F+PHj8fhw4cRFhaGnJwcvPjii8jIyCj2eba2toiJiZFuN27cqKSIiYiIqCqQdRbV9u3b9R6HhITAyckJJ06cQOfOnYt8nkqlgouLS0WHR0RERFWUUY3BSUlJAQDY29sXWy49PR21a9eGh4cH+vXrhwsXLhRZNisrC6mpqXo3IiIiUjajSXC0Wi0mTZqEDh06oEmTJkWWa9CgAX7++Wds3rwZv/zyC7RaLdq3b49bt24VWj44OBgajUa6cRVjIiIi5VMJIYTcQQDA2LFjsW3bNhw4cKBESzDr5OTkoGHDhggKCsLcuXMLnM/KykJWVpb0WLcSYkpKChf6IyIiqiJSU1Oh0WhK/PvbKFYynjBhArZs2YJ9+/aVKrkBAHNzc7Ro0QKRkZGFnler1VCr1YYIk4iIiKoIWbuohBCYMGECNm7ciF27dsHb27vU18jLy8O5c+fg6upaARESERFRVSRrC8748eOxevVqbN68GTY2NoiNjQUAaDQaWFlZAQCGDRsGd3d3BAcHAwDmzJmD5557Dj4+PkhOTsaXX36JGzdu4O2335atHkRERGRcZE1wli5dCgDo2rWr3vHly5djxIgRAIDo6GiYmDxqaLp37x5GjRqF2NhY2NnZoVWrVjh06BAaNWpUWWETERFRERLSs6AVAk428i7CazSDjCtLaQcpERERUcmsORqN6RvOoYVnDWwc18Gg1y7t72+jmSZOREREVduZW8kAgFPRybLGATDBISIiIgPR9Qm992J9eQMBExwiIiIyEF2Co1Kp5A0ETHCIiIjIQLQPMxwjyG+Y4BAREZFh6GYtqSB/hsMEh4iIiAxC10VlIn9+wwSHiIiIDEOwi4qIiIiUhl1UREREpDgcZExERESKw2niREREpDiPuqjkxwSHiIiIDELXRcVZVERERKQc7KIiIiIipRFgCw4REREpjFb78A5bcIiIiEgpdC048qc3THCIiIjIQB5t1SB/isMEh4iIiAxCKw0yljcOgAkOERERGQy7qIiIiEhh2EVFREREiqOV9mqQNw6ACQ4REREZCLdqICIiIsXRsouKiIiIlEY87KIygvyGCQ4REREZFhMcIiIiUoxHu4nLn+EwwSEiIiKD0E2iMgZMcIiIiMggpFnibMEhIiIipXjURSVzIGCCQ0RERAbyaB0c+TMcJjhERERkGNI6OPKGATDBISIiIgPRch0cIiIiUppHk6jkz3CY4BAREZFBcJAxERERKQ6niRMREZHicDdxIiIiUhzdZpsmRpBdGEEIREREpARSF5URtOEwwSEiIiKDEJAyHNkxwSEiIiKD0Grz/+Vu4kRERKQYHGRMREREiiO4kjEREREpjZD2opI/w2GCQ0RERAahG2Qsf3rDBIeIiIgMRGtEg3CY4BAREZFBSAv9sYuKiIiIlMKIGnCY4BAREZFhcLNNIiIiUpxHXVQyBwImOERERGQgUhcVExwiIiJSCq200J/8GQ4THCIiIjIIYTx7bTLBISIiIsPgIGMiIiJSHA4yJiIiIsV5tA6O/BkOExwiIiIyiEddVPLGATDBISIiIgN5NItK5kAgc4ITHByMNm3awMbGBk5OTujfvz8uX7781OetW7cOvr6+sLS0RNOmTbF169ZKiJaIiIiKwy6qh/bu3Yvx48fj8OHDCAsLQ05ODl588UVkZGQU+ZxDhw4hKCgIb731Fk6dOoX+/fujf//+OH/+fCVGTkRERE8SRtSCoxK6aIxAfHw8nJycsHfvXnTu3LnQMgMHDkRGRga2bNkiHXvuuefQvHlzLFu27KmvkZqaCo1Gg5SUFNja2hosdiIiomddq7lhSMzIxo5JndHAxcag1y7t72+jGoOTkpICALC3ty+yTEREBAICAvSOBQYGIiIiokJjIyIiouIZ01YNZnIHoKPVajFp0iR06NABTZo0KbJcbGwsnJ2d9Y45OzsjNja20PJZWVnIysqSHqemphomYCIiItIjDTKWOQ7AiFpwxo8fj/Pnz2PNmjUGvW5wcDA0Go108/DwMOj1iYiIKB9XMn7ChAkTsGXLFuzevRu1atUqtqyLiwvi4uL0jsXFxcHFxaXQ8jNmzEBKSop0u3nzpsHiJiIiokeMaZCxrAmOEAITJkzAxo0bsWvXLnh7ez/1Of7+/ggPD9c7FhYWBn9//0LLq9Vq2Nra6t2IiIjI8Ixps01Zx+CMHz8eq1evxubNm2FjYyONo9FoNLCysgIADBs2DO7u7ggODgYATJw4EV26dMH8+fPRu3dvrFmzBsePH8f3338vWz2IiIjo0SBjEyNowpG1BWfp0qVISUlB165d4erqKt1+++03qUx0dDRiYmKkx+3bt8fq1avx/fffw8/PD7///js2bdpU7MBkIiIiqnjG1EUlawtOSZbg2bNnT4Fjr7/+Ol5//fUKiIiIiIjKSvvw1/oz34JDREREyiFgNGsHM8EhIiIiw9ByN3EiIiJSHHZRERERkdLouqiMIL9hgkNERESGIXVRGcFKOExwiIiIyCB0s6NN5M9vmOAQERGRYUhzqJjgEBERkVIIdlERERGRkjy+eC+7qIiIiEgRHt+cQGUE06iY4BAREVG5aR/LcORPb5jgEBERkQE8vkkDF/ojIiIiRdDq9VHJF4cOExwiIiIqN/0xOPLFocMEh4iIiAyKXVRERESkCBxkTERERIrzeBcVW3CIiIhIER6fRWUE+Q0THCIiIio/vVlURoAJDhEREZUbu6iIiIhIeThNnIiIiJSGs6iIiIhIcbhVAxERESmOeLwFR/78hgkOERERlZ9WbwyO/BkOExwiIiIqN/Gwk8oIchsATHCIiIjIAHQ9VEaS3zDBISIiovKTEhwjacJhgkNERETlpuuiMjGO/IYJDhEREZWfVuqiMo4MhwkOERERlZswskE4THCIiIio3HT5DbuoiIiISDEEu6iIiIhIabgODhERESnOoy4q48hwmOAQERFRuel2EzeO9IYJDhERERmAbisqI2nAYYJDRERE5aebJs6VjImIiEgxHm3VIG8cOkxwiIiIqNykLipZo3iECQ4RERGVW97DvRpMjWSlPyY4REREVG66BIfTxImIiEgxdNPE2YJDREREisEuKiIiIlIctuAQERGR4uTmPUxwOAaHiIiIlCLvYQuOCVtwiIiISCm02vx/2YJDREREipHHMThERESkNFrOoiIiIiKlydVyDA4REREpjLQOjnHkN0xwiIiIqPy4Dg4REREpDlcyJiIiIsVhCw4REREpDncTJyIiIsXJZRcVERERKY20Dg5bcIB9+/ahT58+cHNzg0qlwqZNm4otv2fPHqhUqgK32NjYygmYiIiICsWVjB+TkZEBPz8/LFmypFTPu3z5MmJiYqSbk5NTBUVIREREJWFsKxmbyfniPXv2RM+ePUv9PCcnJ9SoUcPwAREREVGZ5HEl4/Jr3rw5XF1d8cILL+DgwYPFls3KykJqaqrejYiIiAwrLz+/4RicsnB1dcWyZcuwfv16rF+/Hh4eHujatStOnjxZ5HOCg4Oh0Wikm4eHRyVGTERE9GzI02oBsIuqTBo0aIAGDRpIj9u3b49r167hq6++wqpVqwp9zowZMzBlyhTpcWpqKpMcIiIiA8vOzU9w1GbG0XZSpRKcwrRt2xYHDhwo8rxarYZara7EiIiIiJ4997PzAABWFqYyR5LPONKscjh9+jRcXV3lDoOIiOiZpktwrI0kwZG1BSc9PR2RkZHS4+vXr+P06dOwt7eHp6cnZsyYgdu3b2PlypUAgIULF8Lb2xuNGzdGZmYmfvzxR+zatQt///23XFUgIiIiAPezcwEA1hbG0TkkaxTHjx/H888/Lz3WjZUZPnw4QkJCEBMTg+joaOl8dnY23n33Xdy+fRvW1tZo1qwZdu7cqXcNIiIiqnxSF5W5cbTgqIR4uPRgKSUnJ+P333/HtWvXMHXqVNjb2+PkyZNwdnaGu7u7oeM0mNTUVGg0GqSkpMDW1lbucIiIiBSh4Ufb8SAnD/NeaYpBbT0Nfv3S/v4uUwvO2bNnERAQAI1Gg6ioKIwaNQr29vbYsGEDoqOjpS4lIiIiUr6cPC0e5OS34NhamcscTb4yDTKeMmUKRowYgatXr8LS0lI63qtXL+zbt89gwREREZHxy8nTSvc71qspYySPlCnBOXbsGEaPHl3guLu7Oze+JCIiesbk5D0a7WJpZhxjcMqU4KjV6kK3PLhy5QocHR3LHRQRERFVHbp9qADAzEhWMi5TgtO3b1/MmTMHOTk5AACVSoXo6Gi8//77ePXVVw0aIBERERm33IddVCaqKr7Z5vz585Geng4nJyc8ePAAXbp0gY+PD2xsbPDpp58aOkYiIiIyYjkPW3DMTI1n/eAyzaLSaDQICwvDwYMHcebMGaSnp6Nly5YICAgwdHxERERk5PIejsExlu4poJwL/XXo0AEdOnQwVCxERERUBeU83EncmBKcMrUlvfPOO1i0aFGB49988w0mTZpU3piIiIioCskzwi6qMkWyfv36Qltu2rdvj99//73cQREREVHVoVsHp8q34CQmJkKj0RQ4bmtri4SEhHIHRURERFWH1IJT1RMcHx8fbN++vcDxbdu2oU6dOuUOioiIiKoO3UJ/xtRFVaZBxlOmTMGECRMQHx+Pbt26AQDCw8Mxf/58LFy40JDxERERkZF7NAbHeFpwypTgvPnmm8jKysKnn36KuXPnAgC8vLywdOlSDBs2zKABEhERkXHLNcIxOGWeJj527FiMHTsW8fHxsLKyQvXq1Q0ZFxEREVURudIYnCreRfU47j1FRET0bMvVrYNjRF1UZUq14uLiMHToULi5ucHMzAympqZ6NyIiInp2ZOcqpItqxIgRiI6OxkcffQRXV1eoVMZTISIiIqpc8WlZAICa1dUyR/JImRKcAwcOYP/+/WjevLmBwyEiIqKqJiYlEwDgqrGUOZJHytRF5eHhASGEoWMhIiKiKujsrRQAgKdDNZkjeaRMCc7ChQsxffp0REVFGTgcIiIiqmquxKUBAFrXtpM5kkfK1EU1cOBA3L9/H3Xr1oW1tTXMzc31ziclJRkkOCIiIjJ+D7LzAAA2luWenG0wZYqEqxUTERERAAghcD8nP8GxtqjiCc7w4cMNHQcRERFVQTl5QtqqwcrCeJaKKXeqlZmZiezsbL1jtra25b0sERERVQG67ikAsDI3ngSnTIOMMzIyMGHCBDg5OaFatWqws7PTuxEREdGz4X5OLoD8Rf4szIxnq4YyRTJt2jTs2rULS5cuhVqtxo8//ojZs2fDzc0NK1euNHSMREREZKR0LTjG1D0FlLGL6s8//8TKlSvRtWtXjBw5Ep06dYKPjw9q166N0NBQDBkyxNBxEhERkRG6ee8BAMDRiFYxBsrYgpOUlIQ6deoAyB9vo5sW3rFjR+zbt89w0REREZFRO387f5G/xu4amSPRV6YEp06dOrh+/ToAwNfXF2vXrgWQ37JTo0YNgwVHRERExk2X4DR1N64JRmVKcEaOHIkzZ84AAKZPn44lS5bA0tISkydPxtSpUw0aIBERERmnrNw8bDsfCwBoYmQtOGUagzN58mTpfkBAAC5duoQTJ07Ax8cHzZo1M1hwREREZLwORiZI9xu7GVeCU6YWnJUrVyIrK0t6XLt2bbzyyivw9fXlLCoiIqJnRPL9HAD5WzRorMyfUrpylbmLKiUlpcDxtLQ0jBw5stxBERERkfG7/3CKuH8dB5kjKahMCY4QAiqVqsDxW7duQaMxriYqIiIiqhi6NXCsjWwNHKCUY3BatGgBlUoFlUqF7t27w8zs0dPz8vJw/fp19OjRw+BBEhERkfG5Ly3yZzybbOqUKqL+/fsDAE6fPo3AwEBUr15dOmdhYQEvLy+8+uqrBg2QiIiIjJNum4Yq34Iza9YsAICXlxcGDRoEtdq4Vi0kIiKiyrP8YBQA40xwyjQGp1u3boiPj5ceHz16FJMmTcL3339vsMCIiIjIuOlmTrnVsJI5koLKlOAMHjwYu3fvBgDExsYiICAAR48exYcffog5c+YYNEAiIiIyTg+UNovq/PnzaNu2LQBg7dq1aNq0KQ4dOoTQ0FCEhIQYMj4iIiIyQkIIPMgxzp3EgTImODk5OdL4m507d6Jv374A8veliomJMVx0REREZJRy8gTytAIAYGmukASncePGWLZsGfbv34+wsDBpavidO3fg4GB8zVRERERkWLruKUBBg4w///xzfPfdd+jatSuCgoLg5+cHAPjjjz+krisiIiJSLl33lJmJCuamZUonKlSZVubp2rUrEhISkJqaCjs7O+n4f/7zH1hbWxssOCIiIjJO0vgbI+yeAsqY4ACAqampXnID5K+PQ0RERMqX8uDRRpvGqMRRtWzZEuHh4bCzs5O2bCjKyZMnDRIcERERGae7qZkAAEdbS5kjKVyJE5x+/fpJM6d0WzYQERHRsyk+PQsA4FjdOHc1KHGCo9um4cn7RERE9Oy5de8BAMBVY5wtOMY37JmIiIiM3pF/EwEAdRyryRxJ4UrcgmNnZ1fsuJvHJSUllTkgIiIiMm73s3NxMjoZANC+bk15gylCiROchQsXSvcTExPxySefIDAwEP7+/gCAiIgI7NixAx999JHBgyQiIiLjkZSRDQCwMDNBfefqMkdTuBInOMOHD5fuv/rqq5gzZw4mTJggHXvnnXfwzTffYOfOnZg8ebJhoyQiIiKjcS8jf4q4nbV5iXt3KluZxuDs2LFD2p7hcT169MDOnTvLHRQREREZryPX88ff2FlbyBxJ0cqU4Dg4OGDz5s0Fjm/evJl7URERESncxZg0AIBDdeNNcMq0/ODs2bPx9ttvY8+ePWjXrh0A4MiRI9i+fTt++OEHgwZIRERExiU2NX+KeD8/d5kjKVqZEpwRI0agYcOGWLRoETZs2AAAaNiwIQ4cOCAlPERERKQ8Wq3AtbsZAIBadlYyR1O0Mq+D065dO4SGhuLkyZM4efIkQkNDS53c7Nu3D3369IGbmxtUKhU2bdr01Ofs2bMHLVu2hFqtho+PD0JCQspWASIiIiq1q3fTEZuaCTMTFRq62sodTpHKvdBf7969ERMTU6bnZmRkwM/PD0uWLClR+evXr6N37954/vnncfr0aUyaNAlvv/02duzYUabXJyIiotK5k5LfPVXP2QZ21RQ2Budx+/btw4MHD6TH6enpqF69ZHPie/bsiZ49e5b4tZYtWwZvb2/Mnz8fwKNusa+++gqBgYGlC5yIiIhKLT41fw8qJxvj3INKp1QtOF999VWx59PS0io00YiIiEBAQIDescDAQERERFTYaxIREdEjS/deAwC4G/H4G6CULTgffPABHBwcMGzYMOlY7dq1YW5ujvT0dPTo0QOJiYkGD1InNjYWzs7OesecnZ2RmpqKBw8ewMqq4JudlZWFrKws6XFqamqFxUdERKRkD7LzcD0hf4Bx1/qOMkdTvFK14KxatQqjR4/GH3/8IR07f/487Ozs0KNHD8THx2P37t0GD7I8goODodFopJuHh4fcIREREVVJaVk50v0XGjkXU1J+pUpwXnvtNSxevBhBQUHYs2cPAOD+/fvo1asX4uLisGfPHri6ulZEnAAAFxcXxMXF6R2Li4uDra1toa03ADBjxgykpKRIt5s3b1ZYfEREREqWkZUHAKiuNjPaLRp0Sj3I+O2330ZSUhL69euHzZs3Y+bMmbh9+zb27t0LNze3iohR4u/vj61bt+odCwsLkzb8LIxarYZabdwDoYiIiKqCjKxcAEA1tanMkTxdmWZRTZs2DUlJSejevTu8vLywZ88e1KpVq9TXSU9PR2RkpPT4+vXrOH36NOzt7eHp6YkZM2bg9u3bWLlyJQBgzJgx+OabbzBt2jS8+eab2LVrF9auXYu//vqrLNUgIiKiUkjLzE9wqqvLPQm7wpUqwldeeUXvsbm5OWrWrImJEyfqHdetbvw0x48fx/PPPy89njJlCoD8nctDQkIQExOD6Oho6by3tzf++usvTJ48GV9//TVq1aqFH3/8kVPEiYiIKkFaZv4YHMUlOBqNRu9xUFBQuV68a9euEEIUeb6wVYq7du2KU6dOlet1iYiIqPSu3k0HANR2qCZzJE9XqgRn+fLlFRUHERERGblb9+4DAOo6lmxBXzmVe6sGIiIiejZk5WoBAFYWxp8+GH+EREREZBSyHyY45qbGnz4Yf4RERERkFHLymOAQERGRwuhacCzMjD99MP4IiYiIyCjk5OXPfLZgCw4REREpBVtwiIiISHGyOQaHiIiIlIYtOERERKQ4j1pwjHsncYAJDhEREZVQ5MOtGjjImIiIiBTh9M1k6b6tlbl8gZQQExwiIiJ6qh0XYgEADtUsUM+Je1ERERFRFZebp8XSPdcAAOOe94FKxTE4REREVMUt3HlVuv9iI2cZIyk5JjhERERUpOsJGfhmdyQAoJuvEzzsrWWOqGSY4BAREVGR1p+4Jd2f3bexjJGUDhMcIiIiKpJuavgw/9pVpvUGYIJDRERExbiblgkAaF/XQeZISocJDhERERUpNTMXQNVY++ZxTHCIiIioSGmZOQAAW0smOERERKQAWq1AXGoWAMDG0kzmaEqHCQ4REREVaunea9J9tuAQERGRIlyMSQUAWJqbwK6ahczRlA4THCIiIirUvfvZAIDgV5rKHEnpMcEhIiKiQh2MTAQA2FlXrdYbgAkOERERFUK3wB8AuGqsZIykbJjgEBERUQFHridK9xu42MgYSdkwwSEiIqIC1h67CQB4oYrsHv4kJjhERERUQHaeAAC08bKTOZKyYYJDREREBSRl5C/w175uTZkjKRsmOERERKRHCIGkjPwp4lVt/RsdJjhERESkJytXi5yHXVS2VWyLBh0mOERERKQnO08r3bcwq5qpQtWMmoiIiCpM7sPWGwAwN6maqULVjJqIiIgqTM7DFhxTExVMTFQyR1M2THCIiIhIT3ZufoJjblo1kxuACQ4RERE9QdeCU1W7pwAmOERERPSEXG3+GBzzKjrAGGCCQ0RERE9gFxUREREpjtRFZVp104SqGzkRERFVCN0if0xwiIiISDFy89hFRURERAqTmZsHgC04REREpBBpmTl4M+Q4AMCMCQ4REREpwe8nbkn3u9Z3lDGS8mGCQ0RERADyZ0/N/vMfAEA3XydMfqG+zBGVHRMcIiIiAgAkZWRL9yd2rydjJOXHBIeIiIgAAInp+QmOQzUL+HnUkDeYcmKCQ0RERACAsH/iAAD21SxkjqT8mOAQERERAODWvfsAgPSsXJkjKT8mOERERAQAyMjOT2ze7lRH5kjKjwkOERERAQDSs/IX+NNYmcscSfkxwSEiIiIAQMbDrqnqalOZIyk/JjhEREQEALiT/AAAUE1tJnMk5ccEh4iIiHAvIxsxKZkAmOAQERGRQly4kyrdb+xmK2MkhlH1UzQiIiIql9l/XsCqiBsAgK4NHKE24xgcg1iyZAm8vLxgaWmJdu3a4ejRo0WWDQkJgUql0rtZWlpWYrRERETKsuHkbeRqBQCgc72qu8Hm42Rvwfntt98wZcoULFu2DO3atcPChQsRGBiIy5cvw8nJqdDn2Nra4vLly9JjlUpVWeESEREpSnauFikPcgAAu9/rCu+a1WSOyDBkb8FZsGABRo0ahZEjR6JRo0ZYtmwZrK2t8fPPPxf5HJVKBRcXF+nm7OxciRETEREpxz8x+WNvTE1UqG1vLXM0hiNrgpOdnY0TJ04gICBAOmZiYoKAgABEREQU+bz09HTUrl0bHh4e6NevHy5cuFBk2aysLKSmpurdiIiIKN/ByAQAQJ5WwMREOT0isiY4CQkJyMvLK9AC4+zsjNjY2EKf06BBA/z888/YvHkzfvnlF2i1WrRv3x63bt0qtHxwcDA0Go108/DwMHg9iIiIqird4n49GrvIHIlhyd5FVVr+/v4YNmwYmjdvji5dumDDhg1wdHTEd999V2j5GTNmICUlRbrdvHmzkiMmIiIyXrqNNes7V5c5EsOSdZBxzZo1YWpqiri4OL3jcXFxcHEpWSZpbm6OFi1aIDIystDzarUaarW63LESEREpUXrmw+0ZLGWfd2RQsrbgWFhYoFWrVggPD5eOabVahIeHw9/fv0TXyMvLw7lz5+Dq6lpRYRIRESlSbp4WG07dBgDYWFb9DTYfJ3u6NmXKFAwfPhytW7dG27ZtsXDhQmRkZGDkyJEAgGHDhsHd3R3BwcEAgDlz5uC5556Dj48PkpOT8eWXX+LGjRt4++235awGERFRlbP/aoJ038lGWb0dsic4AwcORHx8PGbOnInY2Fg0b94c27dvlwYeR0dHw8TkUUPTvXv3MGrUKMTGxsLOzg6tWrXCoUOH0KhRI7mqQEREVCXp9p4CgM71lbHAn45KCCHkDqIypaamQqPRICUlBba2VX+vDSIiorJasjsSX+64jNdb1cKXr/vJHU6xSvv7W/YWHCIiIqpc+6/GY+bmC4hPywIA2FezkDkiw2OCQ0RE9Iz5/cQtXE/IkB43cdfIGE3FYIJDRET0jMjIykVSRjZuJt0HAEwNbICXmrmitoMy9p96HBMcIiIihfnnTir2XomHwKNhtndTsxByKEqvXKvadopMbgAmOERERIpyPzsXvRbtL7aMtYUpvGtWQ7Nayuua0mGCQ0REpCBTfjsj3e/r5wZL80dLrZibmmBkBy/4ONnIEVqlYoJDRERUhUXeTcfZW8nS45PR9wAAPZu4YFFQC5mikh8THCIioioqN0+LV5ceQsqDnALn5vRrIkNExoMJDhERURWVkZ0nJTed6tWUjj9XxwGOCtt6obSY4BAREVVRWTl5AACVClj5ZluoVCqZIzIesu4mTkRERGX34GGCY2VuyuTmCUxwiIiIqqjMHC0AwNLcVOZIjA8THCIioioq87EWHNLHBIeIiKiK0nVRqc356/xJHGRMRERUBe2+dBfrT94CAFiasQXnSUxwiIiIqpiMrFyMXnUC2Xn5Y3Dsq1nIHJHxYYJDRERkhP48cwff7IpErlZb4FxOnkB2nhY2lmYIauuJl1u4yxChcWOCQ0REZIR+OXwDl+PSii3Tpb4jPujVsJIiqlqY4BARERkh3QDi93v4oqVnjQLnTU1UaKrg3cDLiwkOERGREcp6uMZNs1oatKvjIHM0VQ/nlRERERmhzNz8FhxLTgEvE75rRERERki3iJ+aU8DLhAkOERGREXq0DQN/VZcF3zUiIiIjxBac8uEgYyIiokr094VYHItKemq5rFxupFkeTHCIiIgqSVpmDsaFnkSuVpSovLmpCtXV/FVdFnzXiIiIDCj0yA18FXal0CQmTyuQqxXQWJljUFuPp16rdW17WFmwBacsmOAQEREZ0PoTt5CQnl1smR6NXTCjJ1cgrkhMcIiIiAwo5UEOAGD+637w86hR4LypiQq17a0rOapnDxMcIiIiA0rNzAUA+LrawMepuszRPLuY4BAR0TPr7K1khF+8i5IN+S2Z5Pv53VMaK3MDXpVKiwkOERE9syasPoXopPsGv66ZiQp21hYGvy6VHBMcIiIyWkIIRN5Nl1b1NaRcrVZKboLaesDMxHBr37bxtkc1Tu+WFd99IiIyWt/t+xfztl2q0NewMjfFZy83hUqlqtDXocrFBIeIiIzWhTupAABbS7MKW/DutdYeTG4UiAkOEREZrbTM/CnX/9e7EQa0efrCeEQ63GyTiIiMVtrDKde2Vvx7nEqHnxgiIpIIIXAyOhl3UzPlDgUAEJuSH4eNJadcU+kwwSEiIsnJ6Ht4dWmE3GEUwDVlqLSY4BARkeTWvQcAABtLM/i62MgcTT4fp+po5GordxhUxTDBISIiSU5e/pq+rWrbIWRkW5mjISo7DjImIiJJTl7+gnrmpvz1QFUbP8FERCTJzs1PcCyY4FAVx08wERFJdC04Fmb89UBVGz/BREQkyZa6qLiyL1VtTHCIiEii66LiGByq6vgJJiIiCQcZk1LwE0xERBLdNHE1x+BQFcdPMBERSdhFRUrBhf6ISBbrT9zCptO35Q6DnhB5Nx0AExyq+pjgEJEsPt9+CXfTsuQOg4rgqrGUOwSicmGCQ0SyeJCdBwD4oJcvalZXyxwNPU5jZY7O9R3lDoOoXJjgEJEsdOut9G7mBvcaVjJHQ0RKw05WIqp0QgguKEdEFYoJDhFVujytgMifjcw9j4ioQvCbhYgqnW6tFYB7HhFRxeA3CxFVOl33FMDpyERUMfjNQkSVTreYHACYmXAMDhEZnlEkOEuWLIGXlxcsLS3Rrl07HD16tNjy69atg6+vLywtLdG0aVNs3bq1kiIlIkPQ7XdkYWYClYoJDhEZnuwJzm+//YYpU6Zg1qxZOHnyJPz8/BAYGIi7d+8WWv7QoUMICgrCW2+9hVOnTqF///7o378/zp8/X8mRE1FZSQkOu6eIqIKohBDi6cUqTrt27dCmTRt88803AACtVgsPDw/897//xfTp0wuUHzhwIDIyMrBlyxbp2HPPPYfmzZtj2bJlT3291NRUaDQapKSkwNbW1mD1yMrNQzxXZSUqkRuJ9zHkxyOwszbHqZkvyh0OEVUBpf39LetCf9nZ2Thx4gRmzJghHTMxMUFAQAAiIiIKfU5ERASmTJmidywwMBCbNm0qtHxWVhaysh4lHqmpqeUPvBAX7qTilW8PVci1iZSKA4yJqKLImuAkJCQgLy8Pzs7OesednZ1x6dKlQp8TGxtbaPnY2NhCywcHB2P27NmGCbgYKgBqTnclKjGVCujr5yZ3GESkUIrfqmHGjBl6LT6pqanw8PAw+Ou08LTD5U96Gvy6REREVHqyJjg1a9aEqakp4uLi9I7HxcXBxcWl0Oe4uLiUqrxarYZazY38iIiIniWy9qlYWFigVatWCA8Pl45ptVqEh4fD39+/0Of4+/vrlQeAsLCwIssTERHRs0f2LqopU6Zg+PDhaN26Ndq2bYuFCxciIyMDI0eOBAAMGzYM7u7uCA4OBgBMnDgRXbp0wfz589G7d2+sWbMGx48fx/fffy9nNYiIiMiIyJ7gDBw4EPHx8Zg5cyZiY2PRvHlzbN++XRpIHB0dDROTRw1N7du3x+rVq/F///d/+OCDD1CvXj1s2rQJTZo0kasKREREZGRkXwenslXUOjhERERUcUr7+5vzmomIiEhxmOAQERGR4jDBISIiIsVhgkNERESKwwSHiIiIFIcJDhERESkOExwiIiJSHCY4REREpDhMcIiIiEhxZN+qobLpFm5OTU2VORIiIiIqKd3v7ZJuwPDMJThpaWkAAA8PD5kjISIiotJKS0uDRqN5arlnbi8qrVaLO3fuwMbGBiqVyqDXTk1NhYeHB27evKnYfa5YR2VgHZXjWagn66gM5a2jEAJpaWlwc3PT24S7KM9cC46JiQlq1apVoa9ha2ur2A+oDuuoDKyjcjwL9WQdlaE8dSxJy40OBxkTERGR4jDBISIiIsVhgmNAarUas2bNglqtljuUCsM6KgPrqBzPQj1ZR2Wo7Do+c4OMiYiISPnYgkNERESKwwSHiIiIFIcJDhERESkOExwiIiJSHCY4BrJkyRJ4eXnB0tIS7dq1w9GjR+UOqcSCg4PRpk0b2NjYwMnJCf3798fly5f1ymRmZmL8+PFwcHBA9erV8eqrryIuLk6vTHR0NHr37g1ra2s4OTlh6tSpyM3NrcyqlNi8efOgUqkwadIk6ZgS6nj79m288cYbcHBwgJWVFZo2bYrjx49L54UQmDlzJlxdXWFlZYWAgABcvXpV7xpJSUkYMmQIbG1tUaNGDbz11ltIT0+v7KoUKi8vDx999BG8vb1hZWWFunXrYu7cuXp701TFOu7btw99+vSBm5sbVCoVNm3apHfeUHU6e/YsOnXqBEtLS3h4eOCLL76o6KpJiqtjTk4O3n//fTRt2hTVqlWDm5sbhg0bhjt37uhdoyrX8UljxoyBSqXCwoUL9Y4roY4XL15E3759odFoUK1aNbRp0wbR0dHS+Ur7rhVUbmvWrBEWFhbi559/FhcuXBCjRo0SNWrUEHFxcXKHViKBgYFi+fLl4vz58+L06dOiV69ewtPTU6Snp0tlxowZIzw8PER4eLg4fvy4eO6550T79u2l87m5uaJJkyYiICBAnDp1SmzdulXUrFlTzJgxQ44qFevo0aPCy8tLNGvWTEycOFE6XtXrmJSUJGrXri1GjBghjhw5Iv7991+xY8cOERkZKZWZN2+e0Gg0YtOmTeLMmTOib9++wtvbWzx48EAq06NHD+Hn5ycOHz4s9u/fL3x8fERQUJAcVSrg008/FQ4ODmLLli3i+vXrYt26daJ69eri66+/lspUxTpu3bpVfPjhh2LDhg0CgNi4caPeeUPUKSUlRTg7O4shQ4aI8+fPi19//VVYWVmJ7777TvY6Jicni4CAAPHbb7+JS5cuiYiICNG2bVvRqlUrvWtU5To+bsOGDcLPz0+4ubmJr776Su9cVa9jZGSksLe3F1OnThUnT54UkZGRYvPmzXq/Dyvru5YJjgG0bdtWjB8/Xnqcl5cn3NzcRHBwsIxRld3du3cFALF3714hRP6Xj7m5uVi3bp1U5uLFiwKAiIiIEELkf+hNTExEbGysVGbp0qXC1tZWZGVlVW4FipGWlibq1asnwsLCRJcuXaQERwl1fP/990XHjh2LPK/VaoWLi4v48ssvpWPJyclCrVaLX3/9VQghxD///CMAiGPHjklltm3bJlQqlbh9+3bFBV9CvXv3Fm+++abesVdeeUUMGTJECKGMOj75S8NQdfr222+FnZ2d3mf1/fffFw0aNKjgGhVU3C9/naNHjwoA4saNG0II5dTx1q1bwt3dXZw/f17Url1bL8FRQh0HDhwo3njjjSKfU5nfteyiKqfs7GycOHECAQEB0jETExMEBAQgIiJCxsjKLiUlBQBgb28PADhx4gRycnL06ujr6wtPT0+pjhEREWjatCmcnZ2lMoGBgUhNTcWFCxcqMfrijR8/Hr1799arC6CMOv7xxx9o3bo1Xn/9dTg5OaFFixb44YcfpPPXr19HbGysXh01Gg3atWunV8caNWqgdevWUpmAgACYmJjgyJEjlVeZIrRv3x7h4eG4cuUKAODMmTM4cOAAevbsCUAZdXySoeoUERGBzp07w8LCQioTGBiIy5cv4969e5VUm5JLSUmBSqVCjRo1ACijjlqtFkOHDsXUqVPRuHHjAuereh21Wi3++usv1K9fH4GBgXByckK7du30urEq87uWCU45JSQkIC8vT+8/AgCcnZ0RGxsrU1Rlp9VqMWnSJHTo0AFNmjQBAMTGxsLCwkL6otF5vI6xsbGFvge6c8ZgzZo1OHnyJIKDgwucU0Id//33XyxduhT16tXDjh07MHbsWLzzzjtYsWIFgEcxFvdZjY2NhZOTk955MzMz2NvbG0Udp0+fjkGDBsHX1xfm5uZo0aIFJk2ahCFDhgBQRh2fZKg6Gfvn93GZmZl4//33ERQUJG3KqIQ6fv755zAzM8M777xT6PmqXse7d+8iPT0d8+bNQ48ePfD333/j5ZdfxiuvvIK9e/dKMVbWd+0zt5s4FW/8+PE4f/48Dhw4IHcoBnXz5k1MnDgRYWFhsLS0lDucCqHVatG6dWt89tlnAIAWLVrg/PnzWLZsGYYPHy5zdIaxdu1ahIaGYvXq1WjcuDFOnz6NSZMmwc3NTTF1fNbl5ORgwIABEEJg6dKlcodjMCdOnMDXX3+NkydPQqVSyR1OhdBqtQCAfv36YfLkyQCA5s2b49ChQ1i2bBm6dOlSqfGwBaecatasCVNT0wIjwOPi4uDi4iJTVGUzYcIEbNmyBbt370atWrWk4y4uLsjOzkZycrJe+cfr6OLiUuh7oDsntxMnTuDu3bto2bIlzMzMYGZmhr1792LRokUwMzODs7Nzla+jq6srGjVqpHesYcOG0uwFXYzFfVZdXFxw9+5dvfO5ublISkoyijpOnTpVasVp2rQphg4dismTJ0utckqo45MMVSdj//wCj5KbGzduICwsTGq9Aap+Hffv34+7d+/C09NT+g66ceMG3n33XXh5eUkxVuU61qxZE2ZmZk/9Hqqs71omOOVkYWGBVq1aITw8XDqm1WoRHh4Of39/GSMrOSEEJkyYgI0bN2LXrl3w9vbWO9+qVSuYm5vr1fHy5cuIjo6W6ujv749z587p/XDqvqCe/LDLoXv37jh37hxOnz4t3Vq3bo0hQ4ZI96t6HTt06FBgev+VK1dQu3ZtAIC3tzdcXFz06piamoojR47o1TE5ORknTpyQyuzatQtarRbt2rWrhFoU7/79+zAx0f/aMjU1lf5yVEIdn2SoOvn7+2Pfvn3IycmRyoSFhaFBgwaws7OrpNoUTZfcXL16FTt37oSDg4Pe+apex6FDh+Ls2bN630Fubm6YOnUqduzYAaDq19HCwgJt2rQp9nuoUn+flHg4MhVpzZo1Qq1Wi5CQEPHPP/+I//znP6JGjRp6I8CN2dixY4VGoxF79uwRMTEx0u3+/ftSmTFjxghPT0+xa9cucfz4ceHv7y/8/f2l87ppfS+++KI4ffq02L59u3B0dDSaKdSFeXwWlRBVv45Hjx4VZmZm4tNPPxVXr14VoaGhwtraWvzyyy9SmXnz5okaNWqIzZs3i7Nnz4p+/foVOt24RYsW4siRI+LAgQOiXr16RjNNfPjw4cLd3V2aJr5hwwZRs2ZNMW3aNKlMVaxjWlqaOHXqlDh16pQAIBYsWCBOnTolzSAyRJ2Sk5OFs7OzGDp0qDh//rxYs2aNsLa2rrTpxcXVMTs7W/Tt21fUqlVLnD59Wu976PFZM1W5joV5chaVEFW/jhs2bBDm5ubi+++/F1evXhWLFy8WpqamYv/+/dI1Kuu7lgmOgSxevFh4enoKCwsL0bZtW3H48GG5QyoxAIXeli9fLpV58OCBGDdunLCzsxPW1tbi5ZdfFjExMXrXiYqKEj179hRWVlaiZs2a4t133xU5OTmVXJuSezLBUUId//zzT9GkSROhVquFr6+v+P777/XOa7Va8dFHHwlnZ2ehVqtF9+7dxeXLl/XKJCYmiqCgIFG9enVha2srRo4cKdLS0iqzGkVKTU0VEydOFJ6ensLS0lLUqVNHfPjhh3q/BKtiHXfv3l3oz+Dw4cOFEIar05kzZ0THjh2FWq0W7u7uYt68eZVVxWLreP369SK/h3bv3q2IOhamsARHCXX86aefhI+Pj7C0tBR+fn5i06ZNeteorO9alRCPLQFKREREpAAcg0NERESKwwSHiIiIFIcJDhERESkOExwiIiJSHCY4REREpDhMcIiIiEhxmOAQERGR4jDBISIqxscff4zmzZvLHQYRlRITHCIqtxEjRkClUkGlUsHc3BzOzs544YUX8PPPP0v7RJVUSEgIatSoUTGBlsF7772nt29OSXh5eWHhwoUVExARlQgTHCIyiB49eiAmJgZRUVHYtm0bnn/+eUycOBEvvfQScnNz5Q6vzKpXr15g40ciMn5McIjIINRqNVxcXODu7o6WLVvigw8+wObNm7Ft2zaEhIRI5RYsWICmTZuiWrVq8PDwwLhx45Ceng4A2LNnD0aOHImUlBSpRejjjz8GAKxatQqtW7eGjY0NXFxcMHjwYL3dhgvj5eWFuXPnIigoCNWqVYO7uzuWLFmiVyY6Ohr9+vVD9erVYWtriwEDBiAuLk46/2QX1YgRI9C/f3/873//g6urKxwcHDB+/Hhpd+euXbvixo0bmDx5slQHIqp8THCIqMJ069YNfn5+2LBhg3TMxMQEixYtwoULF7BixQrs2rUL06ZNAwC0b98eCxcuhK2tLWJiYhATE4P33nsPAJCTk4O5c+fizJkz2LRpE6KiojBixIinxvDll1/Cz88Pp06dwvTp0zFx4kSEhYUBALRaLfr164ekpCTs3bsXYWFh+PfffzFw4MBir7l7925cu3YNu3fvxooVKxASEiIlcRs2bECtWrUwZ84cqQ5EVPnM5A6AiJTN19cXZ8+elR5PmjRJuu/l5YVPPvkEY8aMwbfffgsLCwtoNBqoVCq4uLjoXefNN9+U7tepUweLFi1CmzZtkJ6ejurVqxf5+h06dMD06dMBAPXr18fBgwfx1Vdf4YUXXkB4eDjOnTuH69evw8PDAwCwcuVKNG7cGMeOHUObNm0KvaadnR2++eYbmJqawtfXF71790Z4eDhGjRoFe3t7mJqaSi1NRCQPtuAQUYUSQuh10+zcuRPdu3eHu7s7bGxsMHToUCQmJuL+/fvFXufEiRPo06cPPD09YWNjgy5dugDI72Iqjr+/f4HHFy9eBABcvHgRHh4eUnIDAI0aNUKNGjWkMoVp3LgxTE1Npceurq5P7S4josrFBIeIKtTFixfh7e0NAIiKisJLL72EZs2aYf369Thx4oQ0JiY7O7vIa2RkZCAwMBC2trYIDQ3FsWPHsHHjxqc+r6KYm5vrPVapVKWeLUZEFYtdVERUYXbt2oVz585h8uTJAPJbYbRaLebPnw8Tk/y/r9auXav3HAsLC+Tl5ekdu3TpEhITEzFv3jypteX48eMliuHw4cMFHjds2BAA0LBhQ9y8eRM3b96UrvvPP/8gOTkZjRo1KmVti68DEVUutuAQkUFkZWUhNjYWt2/fxsmTJ/HZZ5+hX79+eOmllzBs2DAAgI+PD3JycrB48WL8+++/WLVqFZYtW6Z3HS8vL6SnpyM8PBwJCQm4f/8+PD09YWFhIT3vjz/+wNy5c0sU18GDB/HFF1/gypUrWLJkCdatW4eJEycCAAICAtC0aVMMGTIEJ0+exNGjRzFs2DB06dIFrVu3LvN74eXlhX379uH27dtISEgo83WIqOyY4BCRQWzfvh2urq7w8vJCjx49sHv3bixatAibN2+Wxqv4+flhwYIF+Pzzz9GkSROEhoYiODhY7zrt27fHmDFjMHDgQDg6OuKLL76Ao6MjQkJCsG7dOjRq1Ajz5s3D//73vxLF9e677+L48eNo0aIFPvnkEyxYsACBgYEA8ruWNm/eDDs7O3Tu3BkBAQGoU6cOfvvtt3K9F3PmzEFUVBTq1q0LR0fHcl2LiMpGJYQQcgdBRFQRvLy8MGnSJL2ZW0T0bGALDhERESkOExwiIiJSHHZRERERkeKwBYeIiIgUhwkOERERKQ4THCIiIlIcJjhERESkOExwiIiISHGY4BAREZHiMMEhIiIixWGCQ0RERIrDBIeIiIgU5/8B9TNcBXtQQ3QAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DBSCAN clustering"
      ],
      "metadata": {
        "id": "6VN31LFxHESk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''False Negatives Only\n",
        "\n",
        "variance_list = []\n",
        "acc = accuracy_error(TP_FN_data, error = 'FN')\n",
        "pca_plot(TP_FN_data,'HBAC-DBSCAN on COMPAS False Negatives', 0.6) #change this for TN FP data\n",
        "#tsne_plot(TP_FN_data, \"t-SNE Plot for False Negatives\", perplexity=30, learning_rate=200, n_iter=1000, alpha=0.5)\n",
        "\n",
        "\n",
        "def bias_with_error_rate_FN(data = TP_FN_data, input_columns = features, error_column = 'FN', max_iter=30, min_splittable_cluster_size = 5, min_acceptable_cluster_size = 5, #dbscan_max_iter=100,\n",
        "                         eps = 0.1, min_samples = 3, x = 0, initial_bias = 0):\n",
        "  for i in range(1, max_iter):\n",
        "    print('This is the current cluster: ', x)\n",
        "    eps = eps + 0.001\n",
        "    if len(data['clusters'].unique()) != 1:\n",
        "      variance_list.append(calculate_variance(data))\n",
        "\n",
        "    data['new_clusters'] = -2\n",
        "    candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "    if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "      x = get_random_cluster(data['clusters'])\n",
        "      continue\n",
        "\n",
        "  #apply DBSCAN\n",
        "    candidate_cluster['new_cluster'] = pd.DataFrame(\n",
        "            DBSCAN(eps=0.1, min_samples=3).fit_predict(\n",
        "                candidate_cluster.drop(['clusters', 'new_clusters', 'predicted_class', 'true_class', 'errors', 'TP',\n",
        "                                        'TN', 'FN', 'FP', 'Shap_age', 'Shap_priors_count', 'Shap_sex_Female',\n",
        "                                        'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian',\n",
        "                                        'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American',\n",
        "                                        'Shap_race_Other'], axis=1)), index=candidate_cluster.index)\n",
        "\n",
        "    data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "    if (len(data['new_clusters'].unique()) <= 2): #check if a new cluster is found\n",
        "      continue\n",
        "\n",
        "    max_bias = get_max_bias(data, bias_type = 'negative')\n",
        "    min_cluster_size = min_split_cluster_size(data)\n",
        "    print('smallest cluster: ', min_cluster_size)\n",
        "\n",
        "    if (max_bias <= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "    #add new cluster\n",
        "        n_cluster = max(data['clusters'])\n",
        "    #print(data['new_clusters'])\n",
        "\n",
        "        first = True\n",
        "        dif = 1\n",
        "        for clstr in data['new_clusters'].unique():\n",
        "          if (clstr == -1):\n",
        "            data['clusters'][data['new_clusters'] == clstr] = -1\n",
        "          elif (clstr == -2):\n",
        "            continue\n",
        "          elif first:\n",
        "            data['clusters'][data['new_clusters'] == clstr] = x\n",
        "            first = False\n",
        "          else:\n",
        "            data['clusters'][data['new_clusters'] == clstr] =  n_cluster + dif\n",
        "            dif += 1\n",
        "\n",
        "        pca_plot(TP_FN_data,'HBAC-DBSCAN on COMPAS False Negatives', 0.6) #change this for TN FP data\n",
        "        #tsne_plot(TP_FN_data, \"t-SNE Plot for False Negatives\", perplexity=30, learning_rate=200, n_iter=1000, alpha=0.5)\n",
        "        x = select_new_cluster(data, error_column = 'FN')\n",
        "        initial_bias = max_bias\n",
        "    else:\n",
        "        x = get_random_cluster(data['clusters'])\n",
        "\n",
        "print('MAX_ITER')\n",
        "print(acc)\n",
        "print(variance_list)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def bias_with_error_rate_FN(data = TP_FN_data, input_columns = features, error_column = 'FN', max_iter=100, min_splittable_cluster_size = 3, min_acceptable_cluster_size = 3, #dbscan_max_iter=100,\n",
        "                         eps = 0.1, min_samples = 3, x = 0, initial_bias = 0, pca_plot = 0.6):\n",
        "\n",
        "    # Initialize the dataset\n",
        "    data = initialize_dataset(data)\n",
        "    variance_list = []\n",
        "    acc = accuracy_error(data, error = 'FN') #Calculating accuracy based on FN error\n",
        "\n",
        "    #Loop for clustering iterations\n",
        "    for i in range(1, max_iter):\n",
        "        print('This is the current cluster: ', x)\n",
        "        eps = eps + 0.001\n",
        "        if len(data['clusters'].unique()) != 1:\n",
        "            variance_list.append(calculate_variance(data))\n",
        "\n",
        "        data['new_clusters'] = -2\n",
        "        candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "        if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "            x = get_random_cluster(data['clusters'])\n",
        "            continue\n",
        "\n",
        "       #apply DBSCAN\n",
        "        candidate_cluster['new_cluster'] = pd.DataFrame(\n",
        "            DBSCAN(eps=eps, min_samples=min_samples).fit_predict(\n",
        "                candidate_cluster.drop(['clusters', 'new_clusters', 'predicted_class', 'true_class', 'errors', 'TP',\n",
        "                                        'TN', 'FN', 'FP', 'Error_Type', 'Shap_age', 'Shap_priors_count', 'Shap_sex_Female',\n",
        "                                        'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian',\n",
        "                                        'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American',\n",
        "                                        'Shap_race_Other'], axis=1)), index=candidate_cluster.index)\n",
        "\n",
        "        data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "        if (len(data['new_clusters'].unique()) <= 2): # Check if a new cluster is found\n",
        "            continue\n",
        "\n",
        "        max_bias = get_max_bias(data, bias_type = 'negative')\n",
        "        min_cluster_size = min_split_cluster_size(data)\n",
        "        print('smallest cluster: ', min_cluster_size)\n",
        "\n",
        "        if (max_bias <= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "            # Add new cluster\n",
        "            n_cluster = max(data['clusters'])\n",
        "            first = True\n",
        "            dif = 1\n",
        "            for clstr in data['new_clusters'].unique():\n",
        "                if (clstr == -1):\n",
        "                    data['clusters'][data['new_clusters'] == clstr] = -1\n",
        "                elif (clstr == -2):\n",
        "                    continue\n",
        "                elif first:\n",
        "                    data['clusters'][data['new_clusters'] == clstr] = x\n",
        "                    first = False\n",
        "                else:\n",
        "                    data['clusters'][data['new_clusters'] == clstr] =  n_cluster + dif\n",
        "                    dif += 1\n",
        "\n",
        "            pca_plot(data,'HBAC-DBSCAN on COMPAS False Negatives', 0.6) # Change this for TN FP data\n",
        "            # tsne_plot(data, \"t-SNE Plot for False Negatives\", perplexity=30, learning_rate=200, n_iter=1000, alpha=0.5)\n",
        "            x = select_new_cluster(data, error_column = 'FN')\n",
        "            initial_bias = max_bias\n",
        "        else:\n",
        "            x = get_random_cluster(data['clusters'])\n",
        "\n",
        "    print('MAX_ITER')\n",
        "    print(acc)\n",
        "    print(variance_list)\n",
        "\n",
        "bias_with_error_rate_FN()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jdlKRBSXF5-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d76d8d9-152f-4e5b-ad59-65f2f95fcfe5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "MAX_ITER\n",
            "0.5250158328055732\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MqDTYxH89tm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_features = TN_FP_data.iloc[:, : 11]\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors = 3) # compute distance to 2nd nearest neighbor (k=3) for each data point\n",
        "nbrs = neigh.fit(nn_features)\n",
        "distances, indices = nbrs.kneighbors(nn_features)\n",
        "\n",
        "distances = np.sort(distances, axis= 0)\n",
        "distances = distances [:,1]\n",
        "plt.plot(distances)\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('K-distance')\n",
        "plt.title(\"Optimal Value of Epsilon for COMPAS\")\n",
        "\n",
        "round(0.02 * len(TN_FP_data)) #decide on min nr points for dbscan?"
      ],
      "metadata": {
        "id": "heFDOlVgFCCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''False Positives Only  '''\n",
        "\n",
        "\n",
        "def bias_with_error_rate_FP(data = TN_FP_data,input_columns = features, error_column = 'FP', max_iter=100, min_splittable_cluster_size = 3, min_acceptable_cluster_size = 3, #bscan_max_iter=100,\n",
        "                         eps = 0.5, min_samples = 20, x = 0, initial_bias = 0, pca_plot = 0.6):\n",
        "\n",
        "    data = initialize_dataset(data)\n",
        "    variance_list = []\n",
        "    acc = accuracy_error(TN_FP_data, error = 'FP')\n",
        "\n",
        "    for i in range(1, max_iter):\n",
        "        #print('Iteration:', i)\n",
        "        print('This is the current cluster: ', x)\n",
        "        eps = eps + 0.001\n",
        "\n",
        "        if len(data['clusters'].unique()) != 1:\n",
        "            variance_list.append(calculate_variance(data))\n",
        "\n",
        "        data['new_clusters'] = -2\n",
        "        candidate_cluster = data.loc[data['clusters'] == x]\n",
        "\n",
        "        if len(candidate_cluster) < min_splittable_cluster_size:\n",
        "            x = get_random_cluster(data['clusters'])\n",
        "            print('Selected a random cluster:', x)\n",
        "            continue\n",
        "\n",
        "        candidate_cluster['new_cluster'] = pd.DataFrame(\n",
        "            DBSCAN(eps=eps, min_samples=min_samples).fit_predict(\n",
        "                candidate_cluster.drop(['clusters', 'new_clusters', 'predicted_class', 'true_class', 'errors', 'TP',\n",
        "                                        'TN', 'FN', 'FP', 'Error_Type', 'Shap_age', 'Shap_priors_count', 'Shap_sex_Female',\n",
        "                                        'Shap_sex_Male', 'Shap_race_African-American', 'Shap_race_Asian',\n",
        "                                        'Shap_race_Caucasian', 'Shap_race_Hispanic', 'Shap_race_Native American',\n",
        "                                        'Shap_race_Other'], axis=1)), index=candidate_cluster.index)\n",
        "        data['new_clusters'] = candidate_cluster['new_clusters'].combine_first(data['new_clusters'])\n",
        "\n",
        "        if (len(data['new_clusters'].unique()) <= 2):\n",
        "            continue\n",
        "\n",
        "        max_bias = get_max_bias(data, bias_type = 'negative')\n",
        "        min_cluster_size = min_split_cluster_size(data)\n",
        "        print('Smallest cluster size:', min_cluster_size)\n",
        "\n",
        "        if (max_bias <= initial_bias) & (min_cluster_size > min_acceptable_cluster_size):\n",
        "            n_cluster = max(data['clusters'])\n",
        "            first = True\n",
        "            dif = 1\n",
        "            for clstr in data['new_clusters'].unique():\n",
        "                if (clstr == -1):\n",
        "                    data['clusters'][data['new_clusters'] == clstr] = -1\n",
        "                elif (clstr == -2):\n",
        "                    continue\n",
        "                elif first:\n",
        "                    data['clusters'][data['new_clusters'] == clstr] = x\n",
        "                    first = False\n",
        "                else:\n",
        "                    data['clusters'][data['new_clusters'] == clstr] =  n_cluster + dif\n",
        "                    dif += 1\n",
        "\n",
        "            print('Adding new clusters...')\n",
        "            pca_plot(data,'HBAC-DBSCAN on COMPAS False Positives', 0.6)\n",
        "            x = select_new_cluster(data, error_column = 'FP')\n",
        "            initial_bias = max_bias\n",
        "        else:\n",
        "            x = get_random_cluster(data['clusters'])\n",
        "\n",
        "    print('MAX_ITER')\n",
        "    print(acc)\n",
        "    print(variance_list)\n",
        "\n",
        "bias_with_error_rate_FP()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEggOpB89KsE",
        "outputId": "707ea498-064a-46f7-fd84-cf63aa072ff7"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "This is the current cluster:  0\n",
            "MAX_ITER\n",
            "0.668542199488491\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTICS"
      ],
      "metadata": {
        "id": "o8HHQgiLO3oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TN_FP_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "pQzci5f-Ojev",
        "outputId": "af5ec3b8-2f10-4cef-dac3-b633301b742f"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        age  priors_count  sex_Female  sex_Male  race_African-American  \\\n",
              "0 -0.778253     -0.108001   -0.491994  0.491994               1.003587   \n",
              "3 -0.778253      0.705649   -0.491994  0.491994              -0.996426   \n",
              "4 -0.033198     -0.718239    2.032544 -2.032544              -0.996426   \n",
              "7  1.374127     -0.718239    2.032544 -2.032544              -0.996426   \n",
              "8 -1.274956     -0.718239   -0.491994  0.491994               1.003587   \n",
              "\n",
              "   race_Asian  race_Caucasian  race_Hispanic  race_Native American  \\\n",
              "0   -0.068006       -0.721261      -0.325222             -0.045279   \n",
              "3   -0.068006        1.386460      -0.325222             -0.045279   \n",
              "4   -0.068006       -0.721261       3.074824             -0.045279   \n",
              "7   -0.068006       -0.721261      -0.325222             -0.045279   \n",
              "8   -0.068006       -0.721261      -0.325222             -0.045279   \n",
              "\n",
              "   race_Other  ...  Shap_sex_Female  Shap_sex_Male  \\\n",
              "0   -0.246517  ...        -0.213978      -0.280610   \n",
              "3   -0.246517  ...         0.090624       0.195730   \n",
              "4   -0.246517  ...         1.246349       1.561017   \n",
              "7    4.056520  ...         1.122310       2.060375   \n",
              "8   -0.246517  ...        -0.120988      -0.118925   \n",
              "\n",
              "   Shap_race_African-American  Shap_race_Asian  Shap_race_Caucasian  \\\n",
              "0                    0.442027        -0.049024             0.795685   \n",
              "3                    1.053784         0.069726             1.444355   \n",
              "4                    0.303078         0.001755             1.373713   \n",
              "7                    0.612419        -0.065523             0.792188   \n",
              "8                    1.131335        -0.000168             0.101570   \n",
              "\n",
              "   Shap_race_Hispanic  Shap_race_Native American Shap_race_Other  clusters  \\\n",
              "0            0.159796                   0.035609       -0.144069         0   \n",
              "3            0.202070                   0.035609        0.192882         0   \n",
              "4            2.834102                   0.035609       -0.221036         0   \n",
              "7           -0.225114                   0.035609        1.217333         0   \n",
              "8            0.189651                   0.035609        0.270869         0   \n",
              "\n",
              "   new_clusters  \n",
              "0            -1  \n",
              "3            -1  \n",
              "4            -1  \n",
              "7            -1  \n",
              "8            -1  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23f69286-b276-4afc-baba-ed039d154e05\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>priors_count</th>\n",
              "      <th>sex_Female</th>\n",
              "      <th>sex_Male</th>\n",
              "      <th>race_African-American</th>\n",
              "      <th>race_Asian</th>\n",
              "      <th>race_Caucasian</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_Native American</th>\n",
              "      <th>race_Other</th>\n",
              "      <th>...</th>\n",
              "      <th>Shap_sex_Female</th>\n",
              "      <th>Shap_sex_Male</th>\n",
              "      <th>Shap_race_African-American</th>\n",
              "      <th>Shap_race_Asian</th>\n",
              "      <th>Shap_race_Caucasian</th>\n",
              "      <th>Shap_race_Hispanic</th>\n",
              "      <th>Shap_race_Native American</th>\n",
              "      <th>Shap_race_Other</th>\n",
              "      <th>clusters</th>\n",
              "      <th>new_clusters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.778253</td>\n",
              "      <td>-0.108001</td>\n",
              "      <td>-0.491994</td>\n",
              "      <td>0.491994</td>\n",
              "      <td>1.003587</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>-0.721261</td>\n",
              "      <td>-0.325222</td>\n",
              "      <td>-0.045279</td>\n",
              "      <td>-0.246517</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.213978</td>\n",
              "      <td>-0.280610</td>\n",
              "      <td>0.442027</td>\n",
              "      <td>-0.049024</td>\n",
              "      <td>0.795685</td>\n",
              "      <td>0.159796</td>\n",
              "      <td>0.035609</td>\n",
              "      <td>-0.144069</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.778253</td>\n",
              "      <td>0.705649</td>\n",
              "      <td>-0.491994</td>\n",
              "      <td>0.491994</td>\n",
              "      <td>-0.996426</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>1.386460</td>\n",
              "      <td>-0.325222</td>\n",
              "      <td>-0.045279</td>\n",
              "      <td>-0.246517</td>\n",
              "      <td>...</td>\n",
              "      <td>0.090624</td>\n",
              "      <td>0.195730</td>\n",
              "      <td>1.053784</td>\n",
              "      <td>0.069726</td>\n",
              "      <td>1.444355</td>\n",
              "      <td>0.202070</td>\n",
              "      <td>0.035609</td>\n",
              "      <td>0.192882</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.033198</td>\n",
              "      <td>-0.718239</td>\n",
              "      <td>2.032544</td>\n",
              "      <td>-2.032544</td>\n",
              "      <td>-0.996426</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>-0.721261</td>\n",
              "      <td>3.074824</td>\n",
              "      <td>-0.045279</td>\n",
              "      <td>-0.246517</td>\n",
              "      <td>...</td>\n",
              "      <td>1.246349</td>\n",
              "      <td>1.561017</td>\n",
              "      <td>0.303078</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>1.373713</td>\n",
              "      <td>2.834102</td>\n",
              "      <td>0.035609</td>\n",
              "      <td>-0.221036</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.374127</td>\n",
              "      <td>-0.718239</td>\n",
              "      <td>2.032544</td>\n",
              "      <td>-2.032544</td>\n",
              "      <td>-0.996426</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>-0.721261</td>\n",
              "      <td>-0.325222</td>\n",
              "      <td>-0.045279</td>\n",
              "      <td>4.056520</td>\n",
              "      <td>...</td>\n",
              "      <td>1.122310</td>\n",
              "      <td>2.060375</td>\n",
              "      <td>0.612419</td>\n",
              "      <td>-0.065523</td>\n",
              "      <td>0.792188</td>\n",
              "      <td>-0.225114</td>\n",
              "      <td>0.035609</td>\n",
              "      <td>1.217333</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1.274956</td>\n",
              "      <td>-0.718239</td>\n",
              "      <td>-0.491994</td>\n",
              "      <td>0.491994</td>\n",
              "      <td>1.003587</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>-0.721261</td>\n",
              "      <td>-0.325222</td>\n",
              "      <td>-0.045279</td>\n",
              "      <td>-0.246517</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120988</td>\n",
              "      <td>-0.118925</td>\n",
              "      <td>1.131335</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>0.101570</td>\n",
              "      <td>0.189651</td>\n",
              "      <td>0.035609</td>\n",
              "      <td>0.270869</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23f69286-b276-4afc-baba-ed039d154e05')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23f69286-b276-4afc-baba-ed039d154e05 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23f69286-b276-4afc-baba-ed039d154e05');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e712d7d-003c-49d3-8d3a-321e9c4a0fde\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e712d7d-003c-49d3-8d3a-321e9c4a0fde')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e712d7d-003c-49d3-8d3a-321e9c4a0fde button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "TN_FP_data"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n"
      ],
      "metadata": {
        "id": "emZh5WLDOZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identifying highest biased cluster"
      ],
      "metadata": {
        "id": "jBU-QQ1gi6tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = get_cluster_max_bias(TN_FP_data)\n",
        "highest_biased_cluster = TN_FP_data[TN_FP_data['clusters'] == c]\n",
        "len(highest_biased_cluster)\n",
        "\n",
        "accuracy_tnfp = accuracy_error(TN_FP_data)\n",
        "most_bias_cluster_kmeans_aware = bias_w_error(TN_FP_data, c, 'clusters')\n",
        "TN_FP_data[TN_FP_data['clusters']==c]\n",
        "\n",
        "\n",
        "print(f\"Cluster {c} General accuracy of classifier on this dataset:\", accuracy_tnfp)\n",
        "print(f\"Cluster {c} has the highest discrimination bias \")\n",
        "print(most_bias_cluster_kmeans_aware)"
      ],
      "metadata": {
        "id": "Hqx_ZJSZg3NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = get_cluster_max_bias(TP_FN_data)\n",
        "highest_biased_cluster = TP_FN_data[TP_FN_data['clusters'] == c]\n",
        "len(highest_biased_cluster)\n",
        "\n",
        "accuracy_tnfp = accuracy_error(TP_FN_data)\n",
        "most_bias_cluster_kmeans_aware = bias_w_error(TP_FN_data, c, 'clusters')\n",
        "TP_FN_data[TP_FN_data['clusters']==c]\n",
        "\n",
        "\n",
        "print(f\"Cluster {c} General accuracy of classifier on this dataset:\", accuracy_tnfp)\n",
        "print(f\"Cluster {c} has the highest discrimination bias \")\n",
        "print(most_bias_cluster_kmeans_aware)"
      ],
      "metadata": {
        "id": "236E7YM6Ifrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Function to decide on min_samples for clustering. If three rows have the same values then min_samples ...\n",
        "\n",
        "def calculate_min_samples(data, columns):\n",
        "  min_samples = len(columns)\n",
        "\n",
        "  for idx, row in data.iterrows():\n",
        "    similar_rows = data[(data[columns] == row[columns]).all(axis=1)]\n",
        "    if data[i.values] equals data[j.values] equals data[k.values]\n",
        "    min_samp = 3\n",
        "    else if\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "uYYSKrGAj17d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}